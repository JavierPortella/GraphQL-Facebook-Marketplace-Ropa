{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import (\n",
    "    basicConfig,\n",
    "    CRITICAL,\n",
    "    ERROR,\n",
    "    FileHandler,\n",
    "    getLogger,\n",
    "    INFO,\n",
    "    log,\n",
    "    shutdown,\n",
    "    StreamHandler,\n",
    ")\n",
    "from os import environ, getenv, makedirs, path\n",
    "from re import sub\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire.webdriver import Chrome, ChromeOptions\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    "    TimeoutException,\n",
    ")\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "CURRENT_DATE = datetime.now().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    \"\"\"Representa a los errores ocurridos durante la ejecución de un scraper\n",
    "\n",
    "    Attributes:\n",
    "        errores (dict): Conjunto de datos que contiene toda información de los errores ocurridos durante la ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Errores\"\"\"\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del atributo errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def agregar_error(self, error, enlace=None):\n",
    "        \"\"\"Agrega la información de un nuevo error al conjunto de datos errores\n",
    "\n",
    "        Args:\n",
    "            error (Exception): Error ocurrido durante la ejecución del scraper\n",
    "            enlace (str, optional): Enlace de la publicación de la página facebook marketplace. Defaults to None.\n",
    "        \"\"\"\n",
    "        log(ERROR, f\"Error:\\n{error}\")\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(traceback_error._str)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Representa al conjunto de datos generado por el scraper\n",
    "\n",
    "    Attributes:\n",
    "        dataset (dict): Conjunto de datos que contiene toda información extraída de las publicaciones de la página de facebook marketplace\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Dataset\"\"\"\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos dataset\"\"\"\n",
    "        return self._dataset\n",
    "\n",
    "    def agregar_data(self, item, fecha_extraccion, enlace):\n",
    "        \"\"\"Agrega la información de una publicación al conjunto de datos dataset\n",
    "\n",
    "        Args:\n",
    "            item (dict): Conjunto de datos que contiene toda la información de una publicación\n",
    "            fecha_extraccion (str): Fecha correspondiente a la extracción de todas las publicaciones\n",
    "            enlace (str): Enlace de la publicación\n",
    "        \"\"\"\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"titulo_marketplace\"].append(\n",
    "            item.get(\"marketplace_listing_title\")\n",
    "        )\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get(\"creation_time\"))\n",
    "        try:\n",
    "            self._dataset[\"tipo_delivery\"].append(item.get(\"delivery_types\")[0])\n",
    "        except:\n",
    "            self._dataset[\"tipo_delivery\"].append(None)\n",
    "        try:\n",
    "            self._dataset[\"descripcion\"].append(\n",
    "                item.get(\"redacted_description\").get(\"text\")\n",
    "            )\n",
    "        except:\n",
    "            self._dataset[\"descripcion\"].append(None)\n",
    "        self._dataset[\"disponible\"].append(item.get(\"is_live\"))\n",
    "        self._dataset[\"vendido\"].append(item.get(\"is_sold\"))\n",
    "        try:\n",
    "            self._dataset[\"fecha_union_vendedor\"].append(\n",
    "                item.get(\"marketplace_listing_seller\").get(\"join_time\")\n",
    "            )\n",
    "        except:\n",
    "            self._dataset[\"fecha_union_vendedor\"].append(None)\n",
    "        self._dataset[\"cantidad\"].append(item.get(\"listing_inventory_type\"))\n",
    "        try:\n",
    "            listing_price = item.get(\"listing_price\")\n",
    "            self._dataset[\"precio\"].append(listing_price.get(\"amount\"))\n",
    "            self._dataset[\"tipo_moneda\"].append(listing_price.get(\"currency\"))\n",
    "            self._dataset[\"amount_with_concurrency\"].append(\n",
    "                listing_price.get(\"amount_with_offset_in_currency\")\n",
    "            )\n",
    "        except:\n",
    "            self._dataset[\"precio\"].append(None)\n",
    "            self._dataset[\"tipo_moneda\"].append(None)\n",
    "            self._dataset[\"amount_with_concurrency\"].append(None)\n",
    "        try:\n",
    "            location = item.get(\"location\")\n",
    "            self._dataset[\"latitud\"].append(location.get(\"latitude\"))\n",
    "            self._dataset[\"longitud\"].append(location.get(\"longitude\"))\n",
    "        except:\n",
    "            self._dataset[\"latitud\"].append(None)\n",
    "            self._dataset[\"longitud\"].append(None)\n",
    "        try:\n",
    "            self._dataset[\"locacion\"].append(item.get(\"location_text\").get(\"text\"))\n",
    "        except:\n",
    "            self._dataset[\"locacion\"].append(None)\n",
    "\n",
    "        self._dataset[\"locacion_id\"].append(item.get(\"location_vanity_or_id\"))\n",
    "        try:\n",
    "            user = item.get(\"story\").get(\"actors\")[0]\n",
    "            self._dataset[\"name_vendedor\"].append(user.get(\"name\"))\n",
    "            self._dataset[\"tipo_vendedor\"].append(user.get(\"__typename\"))\n",
    "            self._dataset[\"id_vendedor\"].append(user.get(\"id\"))\n",
    "        except:\n",
    "            self._dataset[\"name_vendedor\"].append(None)\n",
    "            self._dataset[\"tipo_vendedor\"].append(None)\n",
    "            self._dataset[\"id_vendedor\"].append(None)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    \"\"\"Representa al tiempo de ejecución del scraper\n",
    "\n",
    "    Attributes\n",
    "        start (float): Hora de inicio de la ejecución del scraper en segundos\n",
    "        fecha (str): Fecha de extracción de las publicaciones en formato %d/%m/%Y\n",
    "        hora_inicio (str): Hora de inicio de la ejecución del scraper en formato %H:%M:%S\n",
    "        hora_fin (str): Hora de término de la ejecución del scraper en formato %H:%M:%S\n",
    "        cantidad (int): Cantidad de publicaciones extraídas de la página de facebook marketplace\n",
    "        cantidad_real (int): Cantidad de publicaciones analizadas de la página de facebook marketplace\n",
    "        tiempo (str): Tiempo de ejecución del scraper en formato %d days, %H:%M:%S\n",
    "        productos_por_min (float): Cantidad de publicaciones que puede extraer el scraper en un minuto\n",
    "        productos_por_min_real (float): Cantidad publicaciones que puede analizar el scraper en un minuto\n",
    "        num_error (int): Cantidad de errores ocurridos durante la ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase Tiempo\"\"\"\n",
    "        self._start = time()\n",
    "        self._fecha = CURRENT_DATE.strftime(\"%d/%m/%Y\")\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(self._start))\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = 0\n",
    "        self._cantidad_real = 0\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._productos_por_min_real = None\n",
    "        self._num_error = None\n",
    "\n",
    "    @property\n",
    "    def cantidad(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo cantidad\"\"\"\n",
    "        return self._cantidad\n",
    "\n",
    "    @property\n",
    "    def cantidad_real(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo cantidad_real\"\"\"\n",
    "        return self._cantidad_real\n",
    "\n",
    "    @property\n",
    "    def fecha(self):\n",
    "        \"\"\"Retorna el valor actual del atributo fecha\"\"\"\n",
    "        return self._fecha\n",
    "\n",
    "    @property\n",
    "    def hora_inicio(self):\n",
    "        \"\"\"Retorna el valor actual del atributo hora_inicio\"\"\"\n",
    "        return self._hora_inicio\n",
    "\n",
    "    @property\n",
    "    def num_error(self):\n",
    "        \"\"\"Retorna el valor actual o actualiza el valor del atributo num_error\"\"\"\n",
    "        return self._num_error\n",
    "\n",
    "    @cantidad.setter\n",
    "    def cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "\n",
    "    @cantidad_real.setter\n",
    "    def cantidad_real(self, cantidad_real):\n",
    "        self._cantidad_real = cantidad_real\n",
    "\n",
    "    @num_error.setter\n",
    "    def num_error(self, num_error):\n",
    "        self._num_error = num_error\n",
    "\n",
    "    def set_param_final(self):\n",
    "        \"\"\"Establece parametros finales para medir el tiempo de ejecución del scraper\"\"\"\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        total = end - self._start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = round(self._cantidad / (total / 60), 2)\n",
    "        self._productos_por_min_real = round(self._cantidad_real / (total / 60), 2)\n",
    "        log(INFO, f\"Errores encontrados: {self._num_error}\")\n",
    "        log(INFO, f\"Productos Extraídos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"Representa a un bot para hacer web scraping en fb marketplace\n",
    "\n",
    "    Attributes:\n",
    "        tiempo (Tiempo): Objeto de la clase Tiempo que maneja información del tiempo de ejecución del scraper\n",
    "        driver (webdriver.Chrome): Objeto de la clase webdriver que maneja un navegador para hacer web scraping\n",
    "        wait (WebDriverWait): Objeto de la clase WebDriverWait que maneja el Tiempo de espera durante la ejecución del scraper\n",
    "        errores (Errores): Objeto de la clase Errores que maneja información de los errores ocurridos durante la ejecución del scraper\n",
    "        data (Dataset): Objeto de la clase Dataset que maneja información de las publicaciones extraídas por el scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Genera todos los atributos para una instancia de la clase ScraperFb\"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo()\n",
    "\n",
    "        # Variable que maneja las opciones de chrome\n",
    "        chrome_options = ChromeOptions()\n",
    "\n",
    "        # Configurar nivel de notificacones de chrome\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"])\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        self._driver = Chrome(\n",
    "            chrome_options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self._driver.maximize_window()\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "        log(INFO, f\"Hora de inicio: {self._tiempo.hora_inicio}\")\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retorna el valor actual del atributo data\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del atributo errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def iniciar_sesion(self, user_name, user_password):\n",
    "        \"\"\"Inicia sesión en la página web de facebook usando un usuario y contraseña\n",
    "\n",
    "        Args:\n",
    "            user_name (str): Usuario activo de facebook\n",
    "            user_password (str): Contraseña del usuario activo de facebook\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesión\")\n",
    "        self._driver.get(\"https://www.facebook.com/\")\n",
    "\n",
    "        # Localizando los campos de usuario y contraseña\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        # Limpiando el contenido que existe en los campos de usuario y contraseña\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        # Mandando valores a los campos de usuario y contraseña\n",
    "        username.send_keys(user_name)\n",
    "        password.send_keys(user_password)\n",
    "        # Dar click en el botón de iniciar sesión\n",
    "        self._wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))\n",
    "        ).click()\n",
    "        # Esperando a que se inicie sesión correctamente\n",
    "        self._wait.until(\n",
    "            EC.presence_of_element_located(\n",
    "                (\n",
    "                    By.XPATH,\n",
    "                    \"//a[@class='x1i10hfl x1qjc9v5 xjbqb8w xjqpnuy xa49m3k xqeqjp1 x2hbi6w x13fuv20 xu3j5b3 x1q0q8m5 x26u7qi x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xdl72j9 x2lah0s xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r x2lwn1j xeuugli xexx8yu x4uap5 x18d9i69 xkhd6sd x1n2onr6 x16tdsg8 x1hl2dhg xggy1nq x1ja2u2z x1t137rt x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1q0g3np x87ps6o x1lku1pv x1rg5ohu x1a2a7pz x1hc1fzr x1k90msu x6o7n8i xbxq160']\",\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        log(INFO, \"Inicio de sesión con éxito\")\n",
    "\n",
    "    def obtener_publicaciones(self, selector, xpath):\n",
    "        \"\"\"Retornar una lista de publicaciones visibles con respecto a una categoría en facebook marketplace\n",
    "\n",
    "        Args:\n",
    "            selector (str): Selector a ser usado para localizar las publicaciones\n",
    "            xpath (str): Ruta de las publicaciones a ser usado por el selector\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de publicaciones de Facebook Marketplace\n",
    "        \"\"\"\n",
    "        return self._driver.find_elements(selector, xpath)\n",
    "\n",
    "    def mapear_datos(self, url):\n",
    "        \"\"\"Mapea y extrae los datos de las publicaciones de una categoría\n",
    "\n",
    "        Args:\n",
    "            url (str): Link de la página de una categoría en facebook marketplace\n",
    "        \"\"\"\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self._driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self._driver.switch_to.window(\"newtab\")\n",
    "        self._driver.get(url)\n",
    "\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self.obtener_publicaciones(\n",
    "            By.XPATH, '//img[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "        )\n",
    "\n",
    "        log(INFO, \"Creando variables\")\n",
    "        # Enteros que hacen referencia a la fecha en que se postea una publicación y en la que se extrae la información\n",
    "        fecha_publicacion = fecha_extraccion = int(\n",
    "            datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\").timestamp()\n",
    "        )\n",
    "        # Cuenta la cantidad de publicaciones que mapea el scraper\n",
    "        i = 0\n",
    "        # Cuenta la cantidad de errores ocurridos durante la ejecución del mapeo del scraper\n",
    "        e = 0\n",
    "        enlace = None\n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            try:\n",
    "                log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "                # Eliminar de la memoria requests innecesarios\n",
    "                del self._driver.requests\n",
    "                # Dar click a la publicación de facebook\n",
    "                ropa[i].click()\n",
    "                self._wait.until(\n",
    "                    EC.presence_of_element_located(\n",
    "                        (By.XPATH, \"//img[@class='x5yr21d xl1xv1r xh8yej3']\")\n",
    "                    )\n",
    "                )\n",
    "                # Link de la publicación de facebook\n",
    "                enlace = sub(\n",
    "                    r\"\\?.+\", \"\", self._driver.execute_script(\"return document.URL\")\n",
    "                )\n",
    "                for request in self._driver.requests:\n",
    "                    # Validar si la api es de graphql\n",
    "                    if not request.response or \"graphql\" not in request.url:\n",
    "                        continue\n",
    "\n",
    "                    # Obtener la respuesta de la api en bytes\n",
    "                    body = decode(\n",
    "                        request.response.body,\n",
    "                        request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "                    )\n",
    "                    # Decodificar la respuesta a utf-8\n",
    "                    decoded_body = body.decode(\"utf-8\")\n",
    "                    # Validar si la respuesta decodificada es la deseada\n",
    "                    if (\n",
    "                        decoded_body.find(\n",
    "                            '{\"viewer\":{\"marketplace_product_details_page\"'\n",
    "                        )\n",
    "                        == -1\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    # Convertir al formato json la respuesta decodificada anteriormente\n",
    "                    json_data = loads(decoded_body)\n",
    "\n",
    "                    # Diccionario que contiene toda la información de la publicación\n",
    "                    dato = json_data[\"data\"][\"viewer\"][\n",
    "                        \"marketplace_product_details_page\"\n",
    "                    ][\"target\"]\n",
    "\n",
    "                    # Extraer la fecha de publicación\n",
    "                    fecha_publicacion = dato[\"creation_time\"]\n",
    "\n",
    "                    log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                    self._data.agregar_data(dato, self._tiempo.fecha, enlace)\n",
    "                    log(INFO, f\"Item {i + 1} scrapeado con éxito\")\n",
    "                    break\n",
    "\n",
    "                # Regresar al inicio donde se encuentran todas las publicaciones de facebook\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except (\n",
    "                NoSuchElementException,\n",
    "                ElementNotInteractableException,\n",
    "                StaleElementReferenceException,\n",
    "            ) as error:\n",
    "                self._errores.agregar_error(error)\n",
    "                e += 1\n",
    "\n",
    "            except (\n",
    "                AttributeError,\n",
    "                KeyError,\n",
    "                JSONDecodeError,\n",
    "                TimeoutException,\n",
    "            ) as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "                e += 1\n",
    "            except Exception as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "                i += 1\n",
    "                log(CRITICAL, \"Se detuvo inesperadamente el programa\")\n",
    "                log(CRITICAL, f\"Causa:\\n{error}\")\n",
    "                break\n",
    "\n",
    "            finally:\n",
    "                i += 1\n",
    "\n",
    "                # Verificar si se ha mapeado todas las publicaciones visibles\n",
    "                if i == len(ropa):\n",
    "                    # Hacer uso del scroll para obtener más publicaciones\n",
    "                    self._driver.execute_script(\n",
    "                        \"window.scrollTo(0, document.body.scrollHeight)\"\n",
    "                    )\n",
    "                    sleep(6)\n",
    "                    # Mapear las nuevas publicaciones\n",
    "                    ropa = self.obtener_publicaciones(\n",
    "                        By.XPATH,\n",
    "                        '//img[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]',\n",
    "                    )\n",
    "                sleep(2)\n",
    "                log(\n",
    "                    INFO,\n",
    "                    \"-------------------------------------------------------------------\",\n",
    "                )\n",
    "\n",
    "        del self._driver.requests\n",
    "        # Guardar algunos datos del tiempo de ejecución del scraper\n",
    "        self._tiempo.cantidad_real = i - e\n",
    "        self._tiempo.num_error = e\n",
    "        log(INFO, \"Fin de la extraccion\")\n",
    "\n",
    "    def guardar_datos(\n",
    "        self,\n",
    "        filetype=\"Data\",\n",
    "        folder=\"Data//datos_obtenidos\",\n",
    "        filename=\"fb_data\",\n",
    "    ):\n",
    "        \"\"\"Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filetype (str, optional): Indica si la información son datos de las publicaciones o errores. Se acepta Data y Error. Defaults to \"Data\".\n",
    "            folder (str, optional): Ruta del archivo. Defaults to \"Data//datos_obtenidos\".\n",
    "            filename (str, optional): Nombre del archivo. Defaults to \"fb_data\".\n",
    "        \"\"\"\n",
    "        log(INFO, f\"Guardando {filetype}\")\n",
    "        # Comprobando si el valor ingresado para la variable filetype es correcto\n",
    "        if filetype == \"Data\":\n",
    "            # Registrando toda la información de las publicaciones extraídas por el scraper\n",
    "            dataset = self._data.dataset\n",
    "        elif filetype == \"Error\":\n",
    "            # Registrando toda la información de los errores ocurridos durante la ejecución del scraper\n",
    "            dataset = self._errores.errores\n",
    "        else:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no está admitido. Solo se aceptan los valores Data y Error\",\n",
    "            )\n",
    "            log(\n",
    "                ERROR,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no ser de tipo Data o Error\",\n",
    "            )\n",
    "            return\n",
    "        # Crear un dataframe\n",
    "        df_fb_mkp_ropa = DataFrame(dataset)\n",
    "\n",
    "        # Comprobando que el dataset contenga información\n",
    "        if len(df_fb_mkp_ropa) == 0:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no tener información\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Ejecutando diferentes acciones de acuerdo al tipo de información que se va a guardar\n",
    "        if filetype == \"Data\":\n",
    "            # Eliminando la última publicación, porque su fecha de creación es de otro día\n",
    "            df_fb_mkp_ropa.drop(len(df_fb_mkp_ropa) - 1, axis=0, inplace=True)\n",
    "            # Registrando la cantidad de información que contiene el dataset\n",
    "            cantidad = len(df_fb_mkp_ropa)\n",
    "            self._tiempo.cantidad = cantidad\n",
    "        else:\n",
    "            # Registrando la cantidad de errores ocurridos durante la ejecución del scraper\n",
    "            cantidad = self._tiempo.num_error\n",
    "\n",
    "        datetime_obj = datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\")\n",
    "        # Generando la ruta donde se va a guardar la información\n",
    "        filepath = path.join(folder, datetime_obj.strftime(\"%d-%m-%Y\"))\n",
    "        # Generando el nombre del archivo que va a contener la información\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + datetime_obj.strftime(\"%d%m%Y\")\n",
    "            + \"_\"\n",
    "            + str(cantidad)\n",
    "            + \".xlsx\"\n",
    "        )\n",
    "        # Verificando si la ruta donde se va a guardar la información existe\n",
    "        if not path.exists(filepath):\n",
    "            # Creando la ruta donde se va a guardar la información\n",
    "            makedirs(filepath)\n",
    "        # Guardando la información en un archivo de tipo excel\n",
    "        df_fb_mkp_ropa.to_excel(path.join(filepath, filename), index=False)\n",
    "        log(INFO, f\"{filetype} Guardados Correctamente\")\n",
    "\n",
    "    def guardar_tiempos(self, filename, sheet_name):\n",
    "        \"\"\"Guarda la información del tiempo de ejecución del scraper\n",
    "\n",
    "        Args:\n",
    "            filename (str): Nombre del archivo\n",
    "            sheet_name (str): Nombre de la hoja de cálculo\n",
    "        \"\"\"\n",
    "        log(INFO, \"Guardando tiempos\")\n",
    "        # Guardando los parametros finales del tiempo de ejecución del scraper\n",
    "        self._tiempo.set_param_final()\n",
    "        # Variable que indica si el encabezados existe o no en el archivo de excel\n",
    "        header_exist = True\n",
    "        # Verificando si el archivo existe o no\n",
    "        if path.isfile(filename):\n",
    "            # Leendo el archivo\n",
    "            tiempos = load_workbook(filename)\n",
    "            # Comprobando si ya existe un sheet con el nombre indicado en la variable sheet_name\n",
    "            if sheet_name not in [ws.title for ws in tiempos.worksheets]:\n",
    "                # Creando un nuevo sheet\n",
    "                tiempos.create_sheet(sheet_name)\n",
    "                # Especificar que no existen encabezados en el nuevo sheet\n",
    "                header_exist = False\n",
    "        else:\n",
    "            # Creando un archivo de tipo workbook\n",
    "            tiempos = Workbook()\n",
    "            tiempos.worksheets[0].title = sheet_name\n",
    "            header_exist = False\n",
    "\n",
    "        # Seleccionar el sheet deseado donde se va a guardar la información\n",
    "        worksheet = tiempos[sheet_name]\n",
    "\n",
    "        # Comprobando si el encabezados existe o no\n",
    "        if not header_exist:\n",
    "            # Lista que contiene los encabezados a ser insertados\n",
    "            keys = [\n",
    "                \"Fecha\",\n",
    "                \"Hora Inicio\",\n",
    "                \"Hora Fin\",\n",
    "                \"Cantidad\",\n",
    "                \"Cantidad Real\",\n",
    "                \"Tiempo Ejecucion (min)\",\n",
    "                \"Categorias / Minuto\",\n",
    "                \"Categorias / Minuto real\",\n",
    "                \"Errores\",\n",
    "            ]\n",
    "            # Otra forma de indicar los encabezados\n",
    "            # keys = list(self._tiempo.__dict__.keys())[1:]\n",
    "            # Insertando los encabezados al sheet\n",
    "            worksheet.append(keys)\n",
    "        # Lista que contiene los valores a ser insertados\n",
    "        values = list(self._tiempo.__dict__.values())[1:]\n",
    "        # Insertando la información del tiempo al sheet\n",
    "        worksheet.append(values)\n",
    "        # Guardar la información en un archivo excel\n",
    "        tiempos.save(filename)\n",
    "        # Cerrar el archivo excel\n",
    "        tiempos.close()\n",
    "        log(INFO, \"Tiempos Guardados Correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "767432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log(log_folder, log_filename, log_file_mode, log_file_encoding):\n",
    "    \"\"\"Función que configura los logs para rastrear al programa\n",
    "\n",
    "    Args:\n",
    "        log_folder (str): Carpeta donde se va a generar el archivo log\n",
    "        log_filename (str): Nombre del archivo log a ser generado\n",
    "        log_file_mode (str): Modo de guardado del archivo\n",
    "        log_file_encoding (str): Codificación usada para el archivo\n",
    "    \"\"\"\n",
    "    # Mostrar solo los errores de los registros que maneja selenium\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja urllib\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja seleniumwire\n",
    "    logger = getLogger(\"seleniumwire\")\n",
    "    logger.setLevel(ERROR)\n",
    "    environ[\"WDM_LOG\"] = \"0\"\n",
    "    # Generando la ruta donde se va a guardar los registros de ejecución\n",
    "    log_path = path.join(log_folder, CURRENT_DATE.strftime(\"%d-%m-%Y\"))\n",
    "    # Generando el nombre del archivo que va a contener los registros de ejecución\n",
    "    log_filename = log_filename + \"_\" + CURRENT_DATE.strftime(\"%d%m%Y\") + \".log\"\n",
    "    # Verificando si la ruta donde se va a guardar los registros de ejecución existe\n",
    "    if not path.exists(log_path):\n",
    "        # Creando la ruta donde se va a guardar los registros de ejecución\n",
    "        makedirs(log_path)\n",
    "    # Configuración básica de los logs que maneja este programa\n",
    "    basicConfig(\n",
    "        format=\"%(asctime)s %(message)s\",\n",
    "        level=INFO,\n",
    "        handlers=[\n",
    "            StreamHandler(),\n",
    "            FileHandler(\n",
    "                path.join(log_path, log_filename), log_file_mode, log_file_encoding\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def validar_parametros(parametros):\n",
    "    \"\"\"Función que valida si los parámetros a usar están definidos\n",
    "\n",
    "    Args:\n",
    "        parametros (list): Lista de parámetros\n",
    "\n",
    "    Returns:\n",
    "        bool: Indica si los parámetros son validos\n",
    "    \"\"\"\n",
    "    for parametro in parametros:\n",
    "        # Verifica que el parámetro haya sido definido\n",
    "        if not parametro:\n",
    "            log(ERROR, \"Parámetros incorrectos\")\n",
    "            return False\n",
    "\n",
    "    log(INFO, \"Parámetros válidos\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Formato para el debugger\n",
    "        config_log(\"Log\", \"fb_ropa_log\", \"w\", \"utf-8\")\n",
    "        log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "\n",
    "        # Cargar variables de entorno\n",
    "        log(INFO, \"Cargando Variables de entorno\")\n",
    "        load_dotenv()\n",
    "\n",
    "        # Url de la categoría a scrapear\n",
    "        url_ropa = getenv(\"URL_CATEGORY\")\n",
    "\n",
    "        # Parámetros para guardar la data extraída por el scraper\n",
    "        data_filename = getenv(\"DATA_FILENAME\")\n",
    "        data_folder = getenv(\"DATA_FOLDER\")\n",
    "\n",
    "        # Parámetros para guardar la medición de la ejecución del scraper\n",
    "        filename_tiempos = getenv(\"FILENAME_TIEMPOS\")\n",
    "        sheet_tiempos = getenv(\"SHEET_TIEMPOS\")\n",
    "\n",
    "        # Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "        error_filename = getenv(\"ERROR_FILENAME\")\n",
    "        error_folder = getenv(\"ERROR_FOLDER\")\n",
    "\n",
    "        # Parámetros de inicio de sesión\n",
    "        user = getenv(\"FB_USERNAME\")\n",
    "        password = getenv(\"FB_PASSWORD\")\n",
    "\n",
    "        # Validar parámetros\n",
    "        if not validar_parametros(\n",
    "            [\n",
    "                url_ropa,\n",
    "                data_filename,\n",
    "                data_folder,\n",
    "                filename_tiempos,\n",
    "                sheet_tiempos,\n",
    "                error_filename,\n",
    "                error_folder,\n",
    "                user,\n",
    "                password,\n",
    "            ]\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        # Inicializar scrapper\n",
    "        scraper = ScraperFb()\n",
    "\n",
    "        # Iniciar sesión\n",
    "        scraper.iniciar_sesion(user, password)\n",
    "\n",
    "        # Extracción de datos\n",
    "        scraper.mapear_datos(url_ropa)\n",
    "\n",
    "        # Guardando la data extraída por el scraper\n",
    "        scraper.guardar_datos(\"Data\", data_folder, data_filename)\n",
    "\n",
    "        # Guardando los errores extraídos por el scraper\n",
    "        scraper.guardar_datos(\"Error\", error_folder, error_filename)\n",
    "\n",
    "        # Guardando los tiempos durante la ejecución del scraper\n",
    "        scraper.guardar_tiempos(filename_tiempos, sheet_tiempos)\n",
    "        log(INFO, \"Programa finalizado\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log(ERROR, f\"Error: {error}\")\n",
    "        log(INFO, \"Programa ejecutado con fallos\")\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            del scraper\n",
    "        except:\n",
    "            pass\n",
    "        # Liberar el archivo log\n",
    "        shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a430409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
