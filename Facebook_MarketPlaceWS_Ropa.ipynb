{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import (\n",
    "    basicConfig,\n",
    "    CRITICAL,\n",
    "    ERROR,\n",
    "    FileHandler,\n",
    "    getLogger,\n",
    "    INFO,\n",
    "    log,\n",
    "    StreamHandler,\n",
    ")\n",
    "from os import getenv, makedirs, path\n",
    "from re import findall\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire import webdriver\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    \"\"\"\n",
    "    Representa a los errores ocurridos durante la ejecuci贸n de un scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    errores : dict\n",
    "        Conjunto de datos que contiene toda informaci贸n de los errores ocurridos durante la ejecuci贸n del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_error(error, enlace):\n",
    "        Agrega la informaci贸n de un error al diccionario de datos errores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Errores\n",
    "        \"\"\"\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def agregar_error(self, error, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la informaci贸n de un error al diccionario de datos errores\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        error: Exception\n",
    "            Error ocurrido durante la ejecuci贸n del scraper\n",
    "        enlace: str\n",
    "            Enlace de la publicaci贸n de la p谩gina facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(ERROR, f\"Error:\\n{error}\")\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(traceback_error._str)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Representa al conjunto de datos generado por el scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : dict\n",
    "        Conjunto de datos que contiene toda informaci贸n extra铆da de una categor铆a de la p谩gina de facebook marketplace\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_data():\n",
    "        Agrega la informaci贸n de una publicaci贸n al diccionario de datos dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Dataset\n",
    "        \"\"\"\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos dataset\"\"\"\n",
    "        return self._dataset\n",
    "\n",
    "    def agregar_data(self, item, fecha_extraccion, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la informaci贸n de una publicaci贸n al dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item: dict\n",
    "            Conjunto de datos que contiene toda la informaci贸n de una publicaci贸n\n",
    "        fecha_extraccion: str\n",
    "            Fecha correspondiente a la extracci贸n de todas las publicaciones\n",
    "        enlace: str\n",
    "            Enlace de la publicaci贸n\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._dataset[\"titulo_marketplace\"].append(\n",
    "            item.get(\"marketplace_listing_title\")\n",
    "        )\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get(\"creation_time\"))\n",
    "        self._dataset[\"disponible\"].append(item.get(\"is_live\"))\n",
    "        self._dataset[\"vendido\"].append(item.get(\"is_sold\"))\n",
    "        self._dataset[\"cantidad\"].append(item.get(\"listing_inventory_type\"))\n",
    "        self._dataset[\"name_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0].get(\"name\")\n",
    "        )\n",
    "        self._dataset[\"tipo_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0][\"__typename\"]\n",
    "        )\n",
    "        self._dataset[\"id_vendedor\"].append(item.get(\"story\").get(\"actors\")[0][\"id\"])\n",
    "        self._dataset[\"locacion_id\"].append(item.get(\"location_vanity_or_id\"))\n",
    "        self._dataset[\"latitud\"].append(item.get(\"location\", {}).get(\"latitude\"))\n",
    "        self._dataset[\"longitud\"].append(item.get(\"location\", {}).get(\"longitude\"))\n",
    "        self._dataset[\"precio\"].append(item.get(\"listing_price\", {}).get(\"amount\"))\n",
    "        self._dataset[\"tipo_moneda\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"currency\")\n",
    "        )\n",
    "        self._dataset[\"amount_with_concurrency\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"amount_with_offset_in_currency\")\n",
    "        )\n",
    "        self._dataset[\"tipo_delivery\"].append(item.get(\"delivery_types\", [None])[0])\n",
    "        self._dataset[\"descripcion\"].append(\n",
    "            item.get(\"redacted_description\", {}).get(\"text\")\n",
    "        )\n",
    "        self._dataset[\"fecha_union_vendedor\"].append(\n",
    "            item.get(\"marketplace_listing_seller\", {}).get(\"join_time\")\n",
    "        )\n",
    "        data = item.get(\"location_text\", {})\n",
    "        if data:\n",
    "            data = data.get(\"text\")\n",
    "        self._dataset[\"locacion\"].append(data)\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    \"\"\"\n",
    "    Representa al tiempo de ejecuci贸n del scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    start : float\n",
    "        Hora actual en segundos\n",
    "    hora_inicio : str\n",
    "        Hora de inicio de la ejecuci贸n del scraper en formato %H:%M:%S\n",
    "    fecha : str\n",
    "        Fecha de las publicaciones a extraer en formato %d/%m/%Y\n",
    "    hora_fin : str\n",
    "        Hora de t茅rmino de la ejecuci贸n del scraper en formato %H:%M:%S\n",
    "    cantidad : int\n",
    "        Cantidad de publicaciones extra铆das de la p谩gina de facebook marketplace\n",
    "    cantidad_real: int\n",
    "        Cantidad de publicaciones analizadas de la p谩gina de facebook marketplace\n",
    "    tiempo : str\n",
    "        Tiempo de ejecuci贸n del scraper en formato %d days, %H:%M:%S\n",
    "    productos_por_min : float\n",
    "        Cantidad de publicaciones que puede extraer el scraper en un minuto\n",
    "    productos_por_min_real : float\n",
    "        Cantidad publicaciones que puede analizar el scraper en un minuto\n",
    "    num_error : int\n",
    "        Cantidad de errores ocurridos durante la ejecuci贸n del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    set_param_final():\n",
    "        Establece los par谩metros finales cuando se termina de ejecutar el scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fecha_actual):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Tiempo\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fecha_actual: str\n",
    "            Fecha en la que se ejecuta el scraper\n",
    "        \"\"\"\n",
    "        self._start = time()\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(self._start))\n",
    "        log(INFO, f\"Hora de inicio: {self._hora_inicio}\")\n",
    "        self._fecha = fecha_actual.strftime(\"%d/%m/%Y\")\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = None\n",
    "        self._cantidad_real = None\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._productos_por_min_real = None\n",
    "        self._num_error = None\n",
    "\n",
    "    @property\n",
    "    def cantidad(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo cantidad\"\"\"\n",
    "        return self._cantidad\n",
    "\n",
    "    @property\n",
    "    def cantidad_real(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo cantidad_real\"\"\"\n",
    "        return self._cantidad_real\n",
    "\n",
    "    @property\n",
    "    def fecha(self):\n",
    "        \"\"\"Retorna el valor actual del atributo fecha\"\"\"\n",
    "        return self._fecha\n",
    "\n",
    "    @property\n",
    "    def num_error(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo num_error\"\"\"\n",
    "        return self._num_error\n",
    "\n",
    "    @cantidad.setter\n",
    "    def cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "\n",
    "    @cantidad_real.setter\n",
    "    def cantidad_real(self, cantidad_real):\n",
    "        self._cantidad_real = cantidad_real\n",
    "\n",
    "    @num_error.setter\n",
    "    def num_error(self, num_error):\n",
    "        self._num_error = num_error\n",
    "\n",
    "    def set_param_final(self):\n",
    "        \"\"\"\n",
    "        Establece parametros finales para medir el tiempo de ejecuci贸n del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        log(INFO, f\"Productos Extra铆dos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")\n",
    "        total = end - self._start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = round(self._cantidad / (total / 60), 2)\n",
    "        self._productos_por_min_real = round(self._cantidad_real / (total / 60), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"\n",
    "    Representa a un bot para hacer web scraping en fb marketplace\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tiempo : Tiempo\n",
    "        Objeto de la clase Tiempo que maneja informaci贸n del tiempo de ejecuci贸n del scraper\n",
    "    driver: webdriver.Chrome\n",
    "        Objeto de la clase webdriver que maneja un navegador para hacer web scraping\n",
    "    wait : WebDriverWait\n",
    "        Objeto de la clase WebDriverWait que maneja el Tiempo de espera durante la ejecuci贸n del scraper\n",
    "    errores : Errores\n",
    "        Objeto de la clase Errores que maneja informaci贸n de los errores ocurridos durante la ejecuci贸n del scraper\n",
    "    data : Dataset\n",
    "        Objeto de la clase Dataset que maneja informaci贸n de las publicaciones extra铆das por el scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    iniciar_sesion():\n",
    "        Inicia sesi贸n en la p谩gina web de facebook usando un usuario y contrase帽a\n",
    "    obtener_publicaciones(selector, xpath):\n",
    "        Retorna una lista de publicaciones visibles en facebook marketplace\n",
    "    mapear_datos(url):\n",
    "        Mapea y extrae los datos de las publicaciones de una categor铆a\n",
    "    guardar_datos(filetype, folder, filename):\n",
    "        Guarda los datos o errores obtenidos durante la ejecuci贸n del scraper\n",
    "    guardar_tiempos(filename, sheet_name):\n",
    "        Guarda la informaci贸n del tiempo de ejecuci贸n del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fecha_actual):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase ScraperFb\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fecha_actual: str\n",
    "            Fecha en la que se ejecuta el scraper\n",
    "        \"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo(fecha_actual)\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        self._driver = webdriver.Chrome(\n",
    "            chrome_options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retorna el valor actual del atributo data\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del atributo errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def iniciar_sesion(self):\n",
    "        \"\"\"\n",
    "        Inicia sesi贸n en la p谩gina web de facebook usando un usuario y contrase帽a\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesi贸n\")\n",
    "        # Ingresando al p谩gina de facebook\n",
    "        self._driver.get(\"https://www.facebook.com/\")\n",
    "        # Maximizando el explorador\n",
    "        self._driver.maximize_window()\n",
    "        # Localizando los campos de usuario y contrase帽a\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        # Limpiando el contenido que existe en los campos de usuario y contrase帽a\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        # Mandando valores a los campos de usuario y contrase帽a\n",
    "        username.send_keys(getenv(\"FB_USERNAME\"))\n",
    "        password.send_keys(getenv(\"FB_PASSWORD\"))\n",
    "        # Dar click en el bot贸n de iniciar sesi贸n\n",
    "        self._wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))\n",
    "        ).click()\n",
    "        sleep(10)\n",
    "        log(INFO, \"Inicio de sesi贸n con 茅xito\")\n",
    "\n",
    "    def obtener_publicaciones(self, selector, xpath):\n",
    "        \"\"\"\n",
    "        Retornar una lista de publicaciones visibles con respecto a una categor铆a en facebook marketplace\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        selector: str\n",
    "            Selector a ser usado para localizar las publicaciones\n",
    "        xpath: str\n",
    "            Ruta de las publicaciones a ser usado por el selector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "        \"\"\"\n",
    "        return self._driver.find_elements(selector, xpath)\n",
    "\n",
    "    def mapear_datos(self, url):\n",
    "        \"\"\"\n",
    "        Mapea y extrae los datos de las publicaciones de una categor铆a\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: str\n",
    "            Link de la p谩gina de una categor铆a en facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self._driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self._driver.switch_to.window(\"newtab\")\n",
    "        self._driver.get(url)\n",
    "        sleep(8)\n",
    "\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self.obtener_publicaciones(\n",
    "            By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "        )\n",
    "\n",
    "        log(INFO, \"Creando variables\")\n",
    "        # Enteros que hacen referencia a la fecha en que se postea una publicaci贸n y en la que se extrae la informaci贸n\n",
    "        fecha_publicacion = fecha_extraccion = int(\n",
    "            datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\").timestamp()\n",
    "        )\n",
    "        # Entero que hace referencia al d铆a siguiente de la fecha en la que se extrae la informaci贸n\n",
    "        fecha_flag = fecha_extraccion + 86400\n",
    "        # Cuenta la cantidad de publicaciones que mapea el scraper\n",
    "        i = 0\n",
    "        # Cuenta la cantidad de errores ocurridos durante la ejecuci贸n del mapeo del scraper\n",
    "        e = 0\n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            try:\n",
    "                log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "                # Eliminar de la memoria requests innecesarios\n",
    "                del self._driver.requests\n",
    "                # Link de la publicaci贸n de facebook\n",
    "                enlace = findall(\n",
    "                    \"(.*)\\/\\?\",\n",
    "                    ropa[i]\n",
    "                    .find_element(By.XPATH, \".//ancestor::a\")\n",
    "                    .get_attribute(\"href\"),\n",
    "                )[0]\n",
    "                # Dar click a la publicaci贸n de facebook\n",
    "                ropa[i].click()\n",
    "                sleep(5)\n",
    "\n",
    "                for request in self._driver.requests:\n",
    "                    # Validar si la api es de graphql\n",
    "                    if not request.response or \"graphql\" not in request.url:\n",
    "                        continue\n",
    "                    # Obtener la respuesta de la api en bytes\n",
    "                    body = decode(\n",
    "                        request.response.body,\n",
    "                        request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "                    )\n",
    "                    # Decodificar la respuesta a utf-8\n",
    "                    decoded_body = body.decode(\"utf-8\")\n",
    "\n",
    "                    # Validar si la respuesta decodificada es la deseada\n",
    "                    if decoded_body.find('\"extensions\":{\"prefetch_uris_v2\"') == -1:\n",
    "                        continue\n",
    "\n",
    "                    # Convertir al formato json la respuesta decodificada anteriormente\n",
    "                    json_data = loads(decoded_body)\n",
    "                    # Extraer la fecha de publicaci贸n\n",
    "                    fecha_publicacion = json_data[\"data\"][\"viewer\"][\n",
    "                        \"marketplace_product_details_page\"\n",
    "                    ][\"target\"][\"creation_time\"]\n",
    "\n",
    "                    # Validar si la fecha de publicaci贸n corresponda a la deseada\n",
    "                    if fecha_publicacion < fecha_flag:\n",
    "                        # Diccionario que contiene toda la informaci贸n de la publicaci贸n\n",
    "                        dato = json_data[\"data\"][\"viewer\"][\n",
    "                            \"marketplace_product_details_page\"\n",
    "                        ][\"target\"]\n",
    "                        log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                        self._data.agregar_data(dato, self._tiempo.fecha, enlace)\n",
    "                        log(INFO, f\"Item {i + 1} scrapeado con 茅xito\")\n",
    "\n",
    "                    break\n",
    "                # Regresar al inicio donde se encuentran todas las publicaciones de facebook\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except (\n",
    "                NoSuchElementException,\n",
    "                ElementNotInteractableException,\n",
    "                StaleElementReferenceException,\n",
    "            ) as error:\n",
    "                enlace = None\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "\n",
    "            except (KeyError, JSONDecodeError) as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except Exception as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "                i += 1\n",
    "                log(CRITICAL, \"Se detuvo inesperadamente el programa\")\n",
    "                log(CRITICAL, f\"Causa:\\n{error}\")\n",
    "                break\n",
    "\n",
    "            finally:\n",
    "                i += 1\n",
    "\n",
    "                # Verificar si se ha mapeado todas las publicaciones visibles\n",
    "                if i == len(ropa):\n",
    "                    # Hacer uso del scroll para obtener m谩s publicaciones\n",
    "                    self._driver.execute_script(\n",
    "                        \"window.scrollTo(0, document.body.scrollHeight)\"\n",
    "                    )\n",
    "                    sleep(6)\n",
    "                    # Mapear las nuevas publicaciones\n",
    "                    ropa = self.obtener_publicaciones(\n",
    "                        By.XPATH,\n",
    "                        '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]',\n",
    "                    )\n",
    "                sleep(2)\n",
    "                log(\n",
    "                    INFO,\n",
    "                    \"-------------------------------------------------------------------\",\n",
    "                )\n",
    "\n",
    "        del self._driver.requests\n",
    "        # Guardar algunos datos del tiempo de ejecuci贸n del scraper\n",
    "        self._tiempo.cantidad_real = i - e\n",
    "        self._tiempo.num_error = e\n",
    "        log(INFO, f\"Se hall贸 {e} errores\")\n",
    "        log(INFO, \"Fin de la extraccion\")\n",
    "\n",
    "    def guardar_datos(\n",
    "        self,\n",
    "        filetype=\"Data\",\n",
    "        folder=\"Data//datos_obtenidos\",\n",
    "        filename=\"fb_data\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Guarda los datos o errores obtenidos durante la ejecuci贸n del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filetype: str\n",
    "            Indica si la informaci贸n son datos de las publicaciones o errores. Se acepta Data y Error\n",
    "        folder: str\n",
    "            Ruta del archivo\n",
    "        filename: str\n",
    "            Nombre del archivo\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, f\"Guardando {filetype}\")\n",
    "        # Comprobando si el valor ingresado para la variable filetype es correcto\n",
    "        if filetype == \"Data\":\n",
    "            # Registrando toda la informaci贸n de las publicaciones extra铆das por el scraper\n",
    "            dataset = self._data.dataset\n",
    "        elif filetype == \"Error\":\n",
    "            # Registrando toda la informaci贸n de los errores ocurridos durante la ejecuci贸n del scraper\n",
    "            dataset = self._errores.errores\n",
    "        else:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no est谩 admitido. Solo se aceptan los valores Data y Error\",\n",
    "            )\n",
    "            log(\n",
    "                ERROR,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no ser de tipo Data o Error\",\n",
    "            )\n",
    "            return\n",
    "        # Crear un dataframe\n",
    "        df_fb_mkp_ropa = DataFrame(dataset)\n",
    "\n",
    "        # Comprobando que el dataset contenga informaci贸n\n",
    "        if len(df_fb_mkp_ropa) == 0:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no tener informaci贸n\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Ejecutando diferentes acciones de acuerdo al tipo de informaci贸n que se va a guardar\n",
    "        if filetype == \"Data\":\n",
    "            # Eliminando la 煤ltima publicaci贸n, porque su fecha de creaci贸n es de otro d铆a\n",
    "            df_fb_mkp_ropa.drop(len(df_fb_mkp_ropa) - 1, axis=0, inplace=True)\n",
    "            # Registrando la cantidad de informaci贸n que contiene el dataset\n",
    "            cantidad = len(df_fb_mkp_ropa)\n",
    "            self._tiempo.cantidad = cantidad\n",
    "        else:\n",
    "            # Registrando la cantidad de errores ocurridos durante la ejecuci贸n del scraper\n",
    "            cantidad = self._tiempo.num_error\n",
    "\n",
    "        datetime_obj = datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\")\n",
    "        # Generando la ruta donde se va a guardar la informaci贸n\n",
    "        filepath = path.join(folder, datetime_obj.strftime(\"%d-%m-%Y\"))\n",
    "        # Generando el nombre del archivo que va a contener la informaci贸n\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + datetime_obj.strftime(\"%d%m%Y\")\n",
    "            + \"_\"\n",
    "            + str(cantidad)\n",
    "            + \".xlsx\"\n",
    "        )\n",
    "        # Verificando si la ruta donde se va a guardar la informaci贸n existe\n",
    "        if not path.exists(filepath):\n",
    "            # Creando la ruta donde se va a guardar la informaci贸n\n",
    "            makedirs(filepath)\n",
    "        # Guardando la informaci贸n en un archivo de tipo excel\n",
    "        df_fb_mkp_ropa.to_excel(path.join(filepath, filename), index=False)\n",
    "        log(INFO, f\"{filetype} Guardados Correctamente\")\n",
    "\n",
    "    def guardar_tiempos(self, filename, sheet_name):\n",
    "        \"\"\"\n",
    "        Guarda la informaci贸n del tiempo de ejecuci贸n del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Nombre del archivo\n",
    "        sheet_name: str\n",
    "            Nombre de la hoja de c谩lculo\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Guardando tiempos\")\n",
    "        # Guardando los parametros finales del tiempo de ejecuci贸n del scraper\n",
    "        self._tiempo.set_param_final()\n",
    "        # Variable que indica si el encabezados existe o no en el archivo de excel\n",
    "        header_exist = True\n",
    "        # Verificando si el archivo existe o no\n",
    "        if path.isfile(filename):\n",
    "            # Leendo el archivo\n",
    "            tiempos = load_workbook(filename)\n",
    "        else:\n",
    "            # Creando un archivo de tipo workbook\n",
    "            tiempos = Workbook()\n",
    "\n",
    "        # Comprobando si ya existe un sheet con el nombre indicado en la variable sheet_name\n",
    "        if sheet_name not in [ws.title for ws in tiempos.worksheets]:\n",
    "            # Creando un nuevo sheet\n",
    "            tiempos.create_sheet(sheet_name)\n",
    "            # Especificar que no existen encabezados en el nuevo sheet\n",
    "            header_exist = False\n",
    "        # Seleccionar el sheet deseado donde se va a guardar la informaci贸n\n",
    "        worksheet = tiempos[sheet_name]\n",
    "\n",
    "        # Comprobando si el encabezados existe o no\n",
    "        if not header_exist:\n",
    "            # Reordenar la lista que contiene los encabezados a ser insertados\n",
    "            keys = cambiar_posiciones(list(self._tiempo.__dict__.keys())[1:], 0, 1)\n",
    "            # Insertando los encabezados al sheet\n",
    "            worksheet.append(keys)\n",
    "        # Reordenar la lista que contiene los valores a ser insertados\n",
    "        values = cambiar_posiciones(list(self._tiempo.__dict__.values())[1:], 0, 1)\n",
    "        # Insertando la informaci贸n del tiempo al sheet\n",
    "        worksheet.append(values)\n",
    "        # Guardar la informaci贸n en un archivo excel\n",
    "        tiempos.save(filename)\n",
    "        # Cerrar el archivo excel\n",
    "        tiempos.close()\n",
    "        log(INFO, \"Tiempos Guardados Correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "767432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log(\n",
    "    log_folder, log_filename, log_file_mode, log_file_encoding, fecha_actual\n",
    "):\n",
    "    \"\"\"\n",
    "    Funci贸n que configura los logs para rastrear al programa\n",
    "        Parameter:\n",
    "                log_folder (str): Carpeta donde se va a generar el archivo log\n",
    "                log_filename (str): Nombre del archivo log a ser generado\n",
    "                log_file_mode (str): Modo de guardado del archivo\n",
    "                log_file_encoding (str): Codificaci贸n usada para el archivo\n",
    "                fecha_actual (datetime): Fecha actual de la creaci贸n del archivo log\n",
    "        Returns:\n",
    "                None\n",
    "    \"\"\"\n",
    "    # Mostrar solo los errores de los registros que maneja selenium\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja urllib\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja seleniumwire\n",
    "    logger = getLogger(\"seleniumwire\")\n",
    "    logger.setLevel(ERROR)\n",
    "    # Generando la ruta donde se va a guardar los registros de ejecuci贸n\n",
    "    log_path = path.join(log_folder, fecha_actual.strftime(\"%d-%m-%Y\"))\n",
    "    # Generando el nombre del archivo que va a contener los registros de ejecuci贸n\n",
    "    log_filename = log_filename + \"_\" + fecha_actual.strftime(\"%d%m%Y\") + \".log\"\n",
    "    # Verificando si la ruta donde se va a guardar los registros de ejecuci贸n existe\n",
    "    if not path.exists(log_path):\n",
    "        # Creando la ruta donde se va a guardar los registros de ejecuci贸n\n",
    "        makedirs(log_path)\n",
    "    # Configuraci贸n b谩sica de los logs que maneja este programa\n",
    "    basicConfig(\n",
    "        format=\"%(asctime)s %(message)s\",\n",
    "        level=INFO,\n",
    "        handlers=[\n",
    "            StreamHandler(),\n",
    "            FileHandler(\n",
    "                path.join(log_path, log_filename), log_file_mode, log_file_encoding\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def validar_parametros(parametros):\n",
    "    \"\"\"\n",
    "    Funci贸n que valida si los par谩metros a usar est谩n definidos\n",
    "         Parameter:\n",
    "                 parametros (list): Lista de par谩metros\n",
    "\n",
    "        Returns:\n",
    "               None\n",
    "    \"\"\"\n",
    "    for parametro in parametros:\n",
    "        # Verifica que el par谩metro haya sido definido\n",
    "        if not parametro:\n",
    "            log(ERROR, \"Par谩metros incorrectos\")\n",
    "            # Retorna false si algunos de los par谩metros no fue definido\n",
    "            return False\n",
    "    log(INFO, \"Par谩metros v谩lidos\")\n",
    "    # Retorna verdadero si todos los par谩metros fueron definidos\n",
    "    return True\n",
    "\n",
    "\n",
    "def cambiar_posiciones(lista, index1, index2):\n",
    "    \"\"\"\n",
    "    Funci贸n que intercambia las posiciones de 2 elementos de un arreglo\n",
    "         Parameter:\n",
    "                 lista (list): Lista no vac铆a de elementos\n",
    "                 index1 (int): Posici贸n del primer elemento\n",
    "                 index2 (int): Posici贸n del segundo elemento\n",
    "\n",
    "        Returns:\n",
    "               list\n",
    "    \"\"\"\n",
    "    # Comprobar si la lista contiene valores\n",
    "    if len(lista) > 0:\n",
    "        # Intercambio de posiciones\n",
    "        aux = lista[index2]\n",
    "        lista[index2] = lista[index1]\n",
    "        lista[index1] = aux\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Formato para el debugger\n",
    "        fecha_actual = datetime.now().date() - timedelta(days=1)\n",
    "        config_log(\"Log\", \"fb_ropa_log\", \"w\", \"utf-8\", fecha_actual)\n",
    "        log(INFO, \"Configurando Formato B谩sico del Debugger\")\n",
    "\n",
    "        # Cargar variables de entorno\n",
    "        log(INFO, \"Cargando Variables de entorno\")\n",
    "        load_dotenv()\n",
    "\n",
    "        # Url de la categor铆a a scrapear\n",
    "        url_ropa = getenv(\"URL_CATEGORY\")\n",
    "\n",
    "        # Par谩metros para guardar la data extra铆da por el scraper\n",
    "        data_filename = getenv(\"DATA_FILENAME\")\n",
    "        data_folder = getenv(\"DATA_FOLDER\")\n",
    "\n",
    "        # Par谩metros para guardar la medici贸n de la ejecuci贸n del scraper\n",
    "        filename_tiempos = getenv(\"FILENAME_TIEMPOS\")\n",
    "        sheet_tiempos = getenv(\"SHEET_TIEMPOS\")\n",
    "\n",
    "        # Par谩metros para guardar los errores durante la ejecuci贸n por el scraper\n",
    "        error_filename = getenv(\"ERROR_FILENAME\")\n",
    "        error_folder = getenv(\"ERROR_FOLDER\")\n",
    "\n",
    "        # Validar par谩metros\n",
    "        if not validar_parametros(\n",
    "            [\n",
    "                url_ropa,\n",
    "                data_filename,\n",
    "                data_folder,\n",
    "                filename_tiempos,\n",
    "                sheet_tiempos,\n",
    "                error_filename,\n",
    "                error_folder,\n",
    "            ]\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        # Inicializar scrapper\n",
    "        scraper = ScraperFb(fecha_actual)\n",
    "\n",
    "        # Iniciar sesi贸n\n",
    "        scraper.iniciar_sesion()\n",
    "\n",
    "        # Extracci贸n de datos\n",
    "        scraper.mapear_datos(url_ropa)\n",
    "\n",
    "        # Guardando la data extra铆da por el scraper\n",
    "        scraper.guardar_datos(\"Data\", data_folder, data_filename)\n",
    "\n",
    "        # Guardando los errores extra铆dos por el scraper\n",
    "        scraper.guardar_datos(\"Error\", error_folder, error_filename)\n",
    "\n",
    "        # Guardando los tiempos durante la ejecuci贸n del scraper\n",
    "        scraper.guardar_tiempos(filename_tiempos, sheet_tiempos)\n",
    "        log(INFO, \"Programa finalizado\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log(ERROR, f\"Error: {error}\")\n",
    "        log(INFO, \"Programa ejecutado con fallos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a430409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:14:41,553 Configurando Formato B谩sico del Debugger\n",
      "2023-02-03 01:14:41,553 Cargando Variables de entorno\n",
      "2023-02-03 01:14:41,553 Par谩metros v谩lidos\n",
      "2023-02-03 01:14:41,568 Inicializando scraper\n",
      "2023-02-03 01:14:41,568 Hora de inicio: 01:14:41\n",
      "2023-02-03 01:14:41,568 ====== WebDriver manager ======\n",
      "2023-02-03 01:14:43,081 Get LATEST chromedriver version for google-chrome 108.0.5359\n",
      "2023-02-03 01:14:44,487 About to download new driver from https://chromedriver.storage.googleapis.com/108.0.5359.71/chromedriver_win32.zip\n",
      "[WDM] - Downloading: 100%|| 6.58M/6.58M [00:00<00:00, 8.84MB/s]\n",
      "2023-02-03 01:14:46,581 Driver has been saved in cache [C:\\Users\\param\\.wdm\\drivers\\chromedriver\\win32\\108.0.5359]\n",
      "2023-02-03 01:14:48,627 Iniciando sesi贸n\n",
      "2023-02-03 01:15:04,249 Inicio de sesi贸n con 茅xito\n",
      "2023-02-03 01:15:04,251 Accediendo a la URL\n",
      "2023-02-03 01:15:27,217 Mapeando Publicaciones\n",
      "2023-02-03 01:15:27,233 Creando variables\n",
      "2023-02-03 01:15:27,233 Scrapeando item 1\n",
      "2023-02-03 01:15:34,801 -------------------------------------------------------------------\n",
      "2023-02-03 01:15:34,801 Scrapeando item 2\n",
      "2023-02-03 01:15:42,086 -------------------------------------------------------------------\n",
      "2023-02-03 01:15:42,086 Scrapeando item 3\n",
      "2023-02-03 01:15:49,260 -------------------------------------------------------------------\n",
      "2023-02-03 01:15:49,260 Scrapeando item 4\n",
      "2023-02-03 01:15:56,468 -------------------------------------------------------------------\n",
      "2023-02-03 01:15:56,468 Scrapeando item 5\n",
      "2023-02-03 01:16:03,676 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:03,676 Scrapeando item 6\n",
      "2023-02-03 01:16:10,872 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:10,872 Scrapeando item 7\n",
      "2023-02-03 01:16:18,090 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:18,090 Scrapeando item 8\n",
      "2023-02-03 01:16:25,299 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:25,299 Scrapeando item 9\n",
      "2023-02-03 01:16:32,490 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:32,490 Scrapeando item 10\n",
      "2023-02-03 01:16:39,708 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:39,708 Scrapeando item 11\n",
      "2023-02-03 01:16:46,889 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:46,889 Scrapeando item 12\n",
      "2023-02-03 01:16:54,135 -------------------------------------------------------------------\n",
      "2023-02-03 01:16:54,135 Scrapeando item 13\n",
      "2023-02-03 01:17:01,343 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:01,343 Scrapeando item 14\n",
      "2023-02-03 01:17:08,518 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:08,518 Scrapeando item 15\n",
      "2023-02-03 01:17:15,725 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:15,725 Scrapeando item 16\n",
      "2023-02-03 01:17:22,898 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:22,898 Scrapeando item 17\n",
      "2023-02-03 01:17:30,123 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:30,123 Scrapeando item 18\n",
      "2023-02-03 01:17:37,306 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:37,306 Scrapeando item 19\n",
      "2023-02-03 01:17:44,520 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:44,520 Scrapeando item 20\n",
      "2023-02-03 01:17:57,864 -------------------------------------------------------------------\n",
      "2023-02-03 01:17:57,864 Scrapeando item 21\n",
      "2023-02-03 01:18:05,046 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:05,046 Scrapeando item 22\n",
      "2023-02-03 01:18:12,231 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:12,231 Scrapeando item 23\n",
      "2023-02-03 01:18:19,407 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:19,407 Scrapeando item 24\n",
      "2023-02-03 01:18:26,582 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:26,582 Scrapeando item 25\n",
      "2023-02-03 01:18:33,773 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:33,773 Scrapeando item 26\n",
      "2023-02-03 01:18:40,956 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:40,957 Scrapeando item 27\n",
      "2023-02-03 01:18:48,170 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:48,170 Scrapeando item 28\n",
      "2023-02-03 01:18:55,343 -------------------------------------------------------------------\n",
      "2023-02-03 01:18:55,345 Scrapeando item 29\n",
      "2023-02-03 01:19:02,524 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:02,524 Scrapeando item 30\n",
      "2023-02-03 01:19:09,733 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:09,733 Scrapeando item 31\n",
      "2023-02-03 01:19:17,015 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:17,015 Scrapeando item 32\n",
      "2023-02-03 01:19:24,334 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:24,342 Scrapeando item 33\n",
      "2023-02-03 01:19:32,050 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:32,050 Scrapeando item 34\n",
      "2023-02-03 01:19:39,377 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:39,377 Scrapeando item 35\n",
      "2023-02-03 01:19:46,594 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:46,594 Scrapeando item 36\n",
      "2023-02-03 01:19:53,857 -------------------------------------------------------------------\n",
      "2023-02-03 01:19:53,857 Scrapeando item 37\n",
      "2023-02-03 01:20:01,110 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:01,110 Scrapeando item 38\n",
      "2023-02-03 01:20:08,336 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:08,336 Scrapeando item 39\n",
      "2023-02-03 01:20:15,590 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:15,590 Scrapeando item 40\n",
      "2023-02-03 01:20:28,978 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:28,978 Scrapeando item 41\n",
      "2023-02-03 01:20:36,201 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:36,201 Scrapeando item 42\n",
      "2023-02-03 01:20:43,419 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:43,420 Scrapeando item 43\n",
      "2023-02-03 01:20:48,574 Joggers Afranelados De Moda De Var贸n\n",
      "2023-02-03 01:20:48,574 Item 43 scrapeado con 茅xito\n",
      "2023-02-03 01:20:50,616 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:50,616 Scrapeando item 44\n",
      "2023-02-03 01:20:55,795 Parkas Importadas Para Caballero\n",
      "2023-02-03 01:20:55,795 Item 44 scrapeado con 茅xito\n",
      "2023-02-03 01:20:57,831 -------------------------------------------------------------------\n",
      "2023-02-03 01:20:57,831 Scrapeando item 45\n",
      "2023-02-03 01:21:03,005 Blusitas para damas x mayor y menor asemos env铆os a nivel nacional con total seguridad\n",
      "2023-02-03 01:21:03,005 Item 45 scrapeado con 茅xito\n",
      "2023-02-03 01:21:05,039 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:05,039 Scrapeando item 46\n",
      "2023-02-03 01:21:10,190 Zapatillas de ni帽a o ni帽o talla 33.5\n",
      "2023-02-03 01:21:10,190 Item 46 scrapeado con 茅xito\n",
      "2023-02-03 01:21:12,231 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:12,231 Scrapeando item 47\n",
      "2023-02-03 01:21:17,408 Snoppy conjunto talla 4\n",
      "2023-02-03 01:21:17,408 Item 47 scrapeado con 茅xito\n",
      "2023-02-03 01:21:19,438 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:19,438 Scrapeando item 48\n",
      "2023-02-03 01:21:24,602 Manguitas Tejidas \n",
      "2023-02-03 01:21:24,602 Item 48 scrapeado con 茅xito\n",
      "2023-02-03 01:21:26,632 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:26,632 Scrapeando item 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:21:31,812 Sandalias planta taco\n",
      "2023-02-03 01:21:31,812 Item 49 scrapeado con 茅xito\n",
      "2023-02-03 01:21:33,839 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:33,840 Scrapeando item 50\n",
      "2023-02-03 01:21:39,022 Polera para Mujer Talla S Azzorti\n",
      "2023-02-03 01:21:39,022 Item 50 scrapeado con 茅xito\n",
      "2023-02-03 01:21:41,061 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:41,061 Scrapeando item 51\n",
      "2023-02-03 01:21:46,239 Tops de hiloｏ\n",
      "2023-02-03 01:21:46,239 Item 51 scrapeado con 茅xito\n",
      "2023-02-03 01:21:48,269 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:48,269 Scrapeando item 52\n",
      "2023-02-03 01:21:53,465 Vestidos Importados\n",
      "2023-02-03 01:21:53,465 Item 52 scrapeado con 茅xito\n",
      "2023-02-03 01:21:55,499 -------------------------------------------------------------------\n",
      "2023-02-03 01:21:55,499 Scrapeando item 53\n",
      "2023-02-03 01:22:00,728 Conjunto Ni帽a 2 piezas overol short y polo\n",
      "2023-02-03 01:22:00,728 Item 53 scrapeado con 茅xito\n",
      "2023-02-03 01:22:02,771 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:02,771 Scrapeando item 54\n",
      "2023-02-03 01:22:07,950 Medias para damas,caballeros,ni帽as y ni帽os. \n",
      "Tenemos taloneras y tobilleras para damas,caballeros.\n",
      "2023-02-03 01:22:07,950 Item 54 scrapeado con 茅xito\n",
      "2023-02-03 01:22:09,995 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:09,995 Scrapeando item 55\n",
      "2023-02-03 01:22:15,206 Ropa en remate\n",
      "2023-02-03 01:22:15,222 Item 55 scrapeado con 茅xito\n",
      "2023-02-03 01:22:17,244 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:17,244 Scrapeando item 56\n",
      "2023-02-03 01:22:22,470 What's [hidden information] tallas del 33 al 43 G茅nesis Quispe \n",
      "2023-02-03 01:22:22,470 Item 56 scrapeado con 茅xito\n",
      "2023-02-03 01:22:24,497 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:24,499 Scrapeando item 57\n",
      "2023-02-03 01:22:29,767 Polos san valentin \n",
      "2023-02-03 01:22:29,767 Item 57 scrapeado con 茅xito\n",
      "2023-02-03 01:22:31,804 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:31,804 Scrapeando item 58\n",
      "2023-02-03 01:22:37,009 葛POLERAS HELLO KITTY en FRANELA REACTIVA 20/1葛\n",
      "2023-02-03 01:22:37,009 Item 58 scrapeado con 茅xito\n",
      "2023-02-03 01:22:39,042 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:39,046 Scrapeando item 59\n",
      "2023-02-03 01:22:44,258 Zapatitos Ni帽o. Canguros\n",
      "2023-02-03 01:22:44,258 Item 59 scrapeado con 茅xito\n",
      "2023-02-03 01:22:46,306 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:46,306 Scrapeando item 60\n",
      "2023-02-03 01:22:51,565 Remato Chaquetas Cl铆nicas Dama\n",
      "2023-02-03 01:22:51,565 Item 60 scrapeado con 茅xito\n",
      "2023-02-03 01:22:59,687 -------------------------------------------------------------------\n",
      "2023-02-03 01:22:59,690 Scrapeando item 61\n",
      "2023-02-03 01:23:04,927 Oversize 100% algod贸n\n",
      "2023-02-03 01:23:04,927 Item 61 scrapeado con 茅xito\n",
      "2023-02-03 01:23:06,982 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:06,982 Scrapeando item 62\n",
      "2023-02-03 01:23:12,179 MOCHILAS Y LONCHERAS FROZEN DE DISNEY锔\n",
      "2023-02-03 01:23:12,179 Item 62 scrapeado con 茅xito\n",
      "2023-02-03 01:23:14,213 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:14,213 Scrapeando item 63\n",
      "2023-02-03 01:23:19,362 Conjunto verano 2022 pantal贸n gales strech y camisa modelo Slim fit cuello neru en colores variados\n",
      "2023-02-03 01:23:19,362 Item 63 scrapeado con 茅xito\n",
      "2023-02-03 01:23:21,389 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:21,390 Scrapeando item 64\n",
      "2023-02-03 01:23:26,545 Polos de hilo\n",
      "2023-02-03 01:23:26,545 Item 64 scrapeado con 茅xito\n",
      "2023-02-03 01:23:28,594 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:28,594 Scrapeando item 65\n",
      "2023-02-03 01:23:33,813 Vestidos\n",
      "2023-02-03 01:23:33,814 Item 65 scrapeado con 茅xito\n",
      "2023-02-03 01:23:35,844 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:35,844 Scrapeando item 66\n",
      "2023-02-03 01:23:41,010 Hermosos abrigos de material peluche \n",
      "2023-02-03 01:23:41,010 Item 66 scrapeado con 茅xito\n",
      "2023-02-03 01:23:43,056 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:43,056 Scrapeando item 67\n",
      "2023-02-03 01:23:48,223 Bota taco cu帽a tendencia PE\n",
      "2023-02-03 01:23:48,223 Item 67 scrapeado con 茅xito\n",
      "2023-02-03 01:23:50,268 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:50,268 Scrapeando item 68\n",
      "2023-02-03 01:23:55,416 Polos oversay\n",
      "2023-02-03 01:23:55,416 Item 68 scrapeado con 茅xito\n",
      "2023-02-03 01:23:57,448 -------------------------------------------------------------------\n",
      "2023-02-03 01:23:57,448 Scrapeando item 69\n",
      "2023-02-03 01:24:02,629 Pack 3 medias simpsons\n",
      "2023-02-03 01:24:02,629 Item 69 scrapeado con 茅xito\n",
      "2023-02-03 01:24:04,668 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:04,668 Scrapeando item 70\n",
      "2023-02-03 01:24:09,849 Conjunto jean para beb茅s y ni帽os  temporada primavera   \n",
      "2023-02-03 01:24:09,851 Item 70 scrapeado con 茅xito\n",
      "2023-02-03 01:24:11,872 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:11,873 Scrapeando item 71\n",
      "2023-02-03 01:24:17,023 Vestido Rosado Talla 6\n",
      "2023-02-03 01:24:17,023 Item 71 scrapeado con 茅xito\n",
      "2023-02-03 01:24:19,054 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:19,056 Scrapeando item 72\n",
      "2023-02-03 01:24:24,248 VESTIDOS XXL\n",
      "2023-02-03 01:24:24,248 Item 72 scrapeado con 茅xito\n",
      "2023-02-03 01:24:26,279 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:26,279 Scrapeando item 73\n",
      "2023-02-03 01:24:31,456 Converse Talla 34  Original\n",
      "2023-02-03 01:24:31,456 Item 73 scrapeado con 茅xito\n",
      "2023-02-03 01:24:33,487 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:33,487 Scrapeando item 74\n",
      "2023-02-03 01:24:38,727 Abrigo Lanilla dama talla S/M , tambi茅n env铆o provincia x Shalom\n",
      "2023-02-03 01:24:38,727 Item 74 scrapeado con 茅xito\n",
      "2023-02-03 01:24:40,760 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:40,760 Scrapeando item 75\n",
      "2023-02-03 01:24:45,934 What's [hidden information] tallas del 33 al 43 G茅nesis Quispe \n",
      "2023-02-03 01:24:45,934 Item 75 scrapeado con 茅xito\n",
      "2023-02-03 01:24:47,968 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:47,968 Scrapeando item 76\n",
      "2023-02-03 01:24:53,106 Flats con tiras\n",
      "2023-02-03 01:24:53,122 Item 76 scrapeado con 茅xito\n",
      "2023-02-03 01:24:55,147 -------------------------------------------------------------------\n",
      "2023-02-03 01:24:55,147 Scrapeando item 77\n",
      "2023-02-03 01:25:00,356 Sandalias Cris\n",
      "2023-02-03 01:25:00,356 Item 77 scrapeado con 茅xito\n",
      "2023-02-03 01:25:02,387 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:02,387 Scrapeando item 78\n",
      "2023-02-03 01:25:07,595 Zapatilla rosa metalizada\n",
      "2023-02-03 01:25:07,595 Item 78 scrapeado con 茅xito\n",
      "2023-02-03 01:25:09,637 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:09,638 Scrapeando item 79\n",
      "2023-02-03 01:25:14,782 Pijama de verano short y polo\n",
      "2023-02-03 01:25:14,782 Item 79 scrapeado con 茅xito\n",
      "2023-02-03 01:25:16,831 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:16,831 Scrapeando item 80\n",
      "2023-02-03 01:25:22,014 CAMISAS MANGA LARGA  A RAYASALGODON STRESH \n",
      "2023-02-03 01:25:22,014 Item 80 scrapeado con 茅xito\n",
      "2023-02-03 01:25:30,117 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:30,117 Scrapeando item 81\n",
      "2023-02-03 01:25:35,258 Leggins para ni帽as\n",
      "2023-02-03 01:25:35,258 Item 81 scrapeado con 茅xito\n",
      "2023-02-03 01:25:37,307 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:37,307 Scrapeando item 82\n",
      "2023-02-03 01:25:42,524 Sandalias\n",
      "2023-02-03 01:25:42,524 Item 82 scrapeado con 茅xito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:25:44,559 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:44,559 Scrapeando item 83\n",
      "2023-02-03 01:25:49,699 Ropa Para Ni帽a\n",
      "2023-02-03 01:25:49,699 Item 83 scrapeado con 茅xito\n",
      "2023-02-03 01:25:51,733 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:51,733 Scrapeando item 84\n",
      "2023-02-03 01:25:56,873 Venta camiseta cristal\n",
      "2023-02-03 01:25:56,873 Item 84 scrapeado con 茅xito\n",
      "2023-02-03 01:25:58,908 -------------------------------------------------------------------\n",
      "2023-02-03 01:25:58,908 Scrapeando item 85\n",
      "2023-02-03 01:26:04,088 pack de parejas\n",
      "2023-02-03 01:26:04,088 Item 85 scrapeado con 茅xito\n",
      "2023-02-03 01:26:06,128 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:06,130 Scrapeando item 86\n",
      "2023-02-03 01:26:11,272 Vestidos Importados Remato USA 吼\n",
      "2023-02-03 01:26:11,272 Item 86 scrapeado con 茅xito\n",
      "2023-02-03 01:26:13,306 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:13,306 Scrapeando item 87\n",
      "2023-02-03 01:26:18,470 ｏ POLO APRIL ｏ\n",
      "2023-02-03 01:26:18,470 Item 87 scrapeado con 茅xito\n",
      "2023-02-03 01:26:20,507 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:20,507 Scrapeando item 88\n",
      "2023-02-03 01:26:25,674 Bota de hombre puro cuero premiun pura calidad elegantes exclusivos hechos a mano\n",
      "2023-02-03 01:26:25,674 Item 88 scrapeado con 茅xito\n",
      "2023-02-03 01:26:27,721 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:27,721 Scrapeando item 89\n",
      "2023-02-03 01:26:32,875 Maxi Palazos En Piel De Durazno Y Rid Acanalado\n",
      "2023-02-03 01:26:32,875 Item 89 scrapeado con 茅xito\n",
      "2023-02-03 01:26:34,914 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:34,914 Scrapeando item 90\n",
      "2023-02-03 01:26:40,095 MOCHILA DE MINIOMS PARA NIAS - SET 3 \n",
      "2023-02-03 01:26:40,095 Item 90 scrapeado con 茅xito\n",
      "2023-02-03 01:26:42,129 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:42,129 Scrapeando item 91\n",
      "2023-02-03 01:26:47,280 Lindas Sandalias De Dama Taco 3\n",
      "2023-02-03 01:26:47,280 Item 91 scrapeado con 茅xito\n",
      "2023-02-03 01:26:49,312 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:49,312 Scrapeando item 92\n",
      "2023-02-03 01:26:54,526 Poleras Franela Reativa 20 Al 1x Mayor Y Menor S M L Xl\n",
      "2023-02-03 01:26:54,526 Item 92 scrapeado con 茅xito\n",
      "2023-02-03 01:26:56,564 -------------------------------------------------------------------\n",
      "2023-02-03 01:26:56,564 Scrapeando item 93\n",
      "2023-02-03 01:27:01,778 Vestidos en remate\n",
      "2023-02-03 01:27:01,778 Item 93 scrapeado con 茅xito\n",
      "2023-02-03 01:27:03,816 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:03,816 Scrapeando item 94\n",
      "2023-02-03 01:27:08,967 Hermosos modelos de sandalias de Verano desliza para modelos y colores oferta en 2 Pares\n",
      "2023-02-03 01:27:08,967 Item 94 scrapeado con 茅xito\n",
      "2023-02-03 01:27:11,006 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:11,006 Scrapeando item 95\n",
      "2023-02-03 01:27:16,174 Zapatillas importadas\n",
      "2023-02-03 01:27:16,174 Item 95 scrapeado con 茅xito\n",
      "2023-02-03 01:27:18,213 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:18,213 Scrapeando item 96\n",
      "2023-02-03 01:27:23,384 Sandalias Verano 2023\n",
      "2023-02-03 01:27:23,384 Item 96 scrapeado con 茅xito\n",
      "2023-02-03 01:27:25,422 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:25,424 Scrapeando item 97\n",
      "2023-02-03 01:27:30,601 Hermosas Zapatillas De Dama\n",
      "2023-02-03 01:27:30,601 Item 97 scrapeado con 茅xito\n",
      "2023-02-03 01:27:32,637 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:32,638 Scrapeando item 98\n",
      "2023-02-03 01:27:37,793 Traspaso de tienda en gamarra\n",
      "2023-02-03 01:27:37,793 Item 98 scrapeado con 茅xito\n",
      "2023-02-03 01:27:39,820 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:39,821 Scrapeando item 99\n",
      "2023-02-03 01:27:44,971 Hermoso vestido con copas\n",
      "2023-02-03 01:27:44,971 Item 99 scrapeado con 茅xito\n",
      "2023-02-03 01:27:47,012 -------------------------------------------------------------------\n",
      "2023-02-03 01:27:47,012 Scrapeando item 100\n",
      "2023-02-03 01:27:52,208 Top Lentejuelas\n",
      "2023-02-03 01:27:52,208 Item 100 scrapeado con 茅xito\n",
      "2023-02-03 01:28:00,289 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:00,289 Scrapeando item 101\n",
      "2023-02-03 01:28:05,537 CAMISAS LEVIS MANGA LARGA\n",
      "2023-02-03 01:28:05,537 Item 101 scrapeado con 茅xito\n",
      "2023-02-03 01:28:07,582 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:07,584 Scrapeando item 102\n",
      "2023-02-03 01:28:12,757 Camisas rayadas para caballero manga larga algod贸n tallas S.M.L.XLヰ reactivo\n",
      "2023-02-03 01:28:12,757 Item 102 scrapeado con 茅xito\n",
      "2023-02-03 01:28:14,790 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:14,790 Scrapeando item 103\n",
      "2023-02-03 01:28:19,943 Bellas sandalias nuevo ingreso \n",
      "2023-02-03 01:28:19,943 Item 103 scrapeado con 茅xito\n",
      "2023-02-03 01:28:21,988 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:21,989 Scrapeando item 104\n",
      "2023-02-03 01:28:27,153 POLOS OVERSIZE \"ALL WE NEED IS LOVE\" 25/25\n",
      "2023-02-03 01:28:27,153 Item 104 scrapeado con 茅xito\n",
      "2023-02-03 01:28:29,188 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:29,188 Scrapeando item 105\n",
      "2023-02-03 01:28:34,382 Vestido para d铆a especial\n",
      "2023-02-03 01:28:34,382 Item 105 scrapeado con 茅xito\n",
      "2023-02-03 01:28:36,414 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:36,414 Scrapeando item 106\n",
      "2023-02-03 01:28:41,557 SET LINO COTTON NINOS Y NIAS \n",
      "2023-02-03 01:28:41,557 Item 106 scrapeado con 茅xito\n",
      "2023-02-03 01:28:43,590 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:43,592 Scrapeando item 107\n",
      "2023-02-03 01:28:48,781 Enterizo lino Color azulino Talla S-M\n",
      "2023-02-03 01:28:48,781 Item 107 scrapeado con 茅xito\n",
      "2023-02-03 01:28:50,828 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:50,828 Scrapeando item 108\n",
      "2023-02-03 01:28:55,997 Lindas Sandalias \n",
      "2023-02-03 01:28:55,997 Item 108 scrapeado con 茅xito\n",
      "2023-02-03 01:28:58,036 -------------------------------------------------------------------\n",
      "2023-02-03 01:28:58,036 Scrapeando item 109\n",
      "2023-02-03 01:29:03,185 Casaca De Moto Para Dama\n",
      "2023-02-03 01:29:03,185 Item 109 scrapeado con 茅xito\n",
      "2023-02-03 01:29:05,227 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:05,227 Scrapeando item 110\n",
      "2023-02-03 01:29:10,395 Botines Y Zandalia De .. \"GOTTA\"\n",
      "2023-02-03 01:29:10,395 Item 110 scrapeado con 茅xito\n",
      "2023-02-03 01:29:12,434 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:12,434 Scrapeando item 111\n",
      "2023-02-03 01:29:17,575 Polo rib\n",
      "2023-02-03 01:29:17,575 Item 111 scrapeado con 茅xito\n",
      "2023-02-03 01:29:19,616 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:19,616 Scrapeando item 112\n",
      "2023-02-03 01:29:24,811 Zapato Dauss 40\n",
      "2023-02-03 01:29:24,811 Item 112 scrapeado con 茅xito\n",
      "2023-02-03 01:29:26,839 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:26,839 Scrapeando item 113\n",
      "2023-02-03 01:29:32,046 Polos carnaval de cajamarca\n",
      "2023-02-03 01:29:32,046 Item 113 scrapeado con 茅xito\n",
      "2023-02-03 01:29:34,079 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:34,079 Scrapeando item 114\n",
      "2023-02-03 01:29:39,217 Venta de Ropa Nueva Con Etiqueta\n",
      "2023-02-03 01:29:39,217 Item 114 scrapeado con 茅xito\n",
      "2023-02-03 01:29:41,257 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:41,257 Scrapeando item 115\n",
      "2023-02-03 01:29:46,431 CASACAS IMPERMEABLES PARA HOMBRE\n",
      "2023-02-03 01:29:46,431 Item 115 scrapeado con 茅xito\n",
      "2023-02-03 01:29:48,479 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:48,480 Scrapeando item 116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:29:53,645 Sandalias\n",
      "2023-02-03 01:29:53,645 Item 116 scrapeado con 茅xito\n",
      "2023-02-03 01:29:55,686 -------------------------------------------------------------------\n",
      "2023-02-03 01:29:55,686 Scrapeando item 117\n",
      "2023-02-03 01:30:00,850 Enterizos\n",
      "2023-02-03 01:30:00,850 Item 117 scrapeado con 茅xito\n",
      "2023-02-03 01:30:02,892 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:02,892 Scrapeando item 118\n",
      "2023-02-03 01:30:08,064 Sandalias  TOP TOP\n",
      "2023-02-03 01:30:08,064 Item 118 scrapeado con 茅xito\n",
      "2023-02-03 01:30:10,102 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:10,102 Scrapeando item 119\n",
      "2023-02-03 01:30:15,246 LINDOS BOTINES DE TEMPORADA PARA NIAS MODELOS EXCLUSIVOS\n",
      "2023-02-03 01:30:15,246 Item 119 scrapeado con 茅xito\n",
      "2023-02-03 01:30:17,277 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:17,277 Scrapeando item 120\n",
      "2023-02-03 01:30:22,485 POLOS PERSONALIZADOS - ADULTOS Y NIOS\n",
      "2023-02-03 01:30:22,485 Item 120 scrapeado con 茅xito\n",
      "2023-02-03 01:30:30,594 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:30,594 Scrapeando item 121\n",
      "2023-02-03 01:30:35,743 Vestido Blanco\n",
      "2023-02-03 01:30:35,743 Item 121 scrapeado con 茅xito\n",
      "2023-02-03 01:30:37,793 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:37,794 Scrapeando item 122\n",
      "2023-02-03 01:30:42,953 Polo Katatonia, Polos Metal\n",
      "2023-02-03 01:30:42,953 Item 122 scrapeado con 茅xito\n",
      "2023-02-03 01:30:44,991 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:44,993 Scrapeando item 123\n",
      "2023-02-03 01:30:50,168 Polos Anime Talla S M y L\n",
      "2023-02-03 01:30:50,168 Item 123 scrapeado con 茅xito\n",
      "2023-02-03 01:30:52,212 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:52,212 Scrapeando item 124\n",
      "2023-02-03 01:30:57,364 Jeans Valessa\n",
      "2023-02-03 01:30:57,364 Item 124 scrapeado con 茅xito\n",
      "2023-02-03 01:30:59,405 -------------------------------------------------------------------\n",
      "2023-02-03 01:30:59,406 Scrapeando item 125\n",
      "2023-02-03 01:31:04,586 Tacones Para Dama. \n",
      "2023-02-03 01:31:04,586 Item 125 scrapeado con 茅xito\n",
      "2023-02-03 01:31:06,628 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:06,629 Scrapeando item 126\n",
      "2023-02-03 01:31:11,779 Conjunto para ni帽os de algod贸n PIMA\n",
      "2023-02-03 01:31:11,779 Item 126 scrapeado con 茅xito\n",
      "2023-02-03 01:31:13,819 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:13,819 Scrapeando item 127\n",
      "2023-02-03 01:31:18,993 Venta D BOTAS Industrial\n",
      "2023-02-03 01:31:18,993 Item 127 scrapeado con 茅xito\n",
      "2023-02-03 01:31:21,026 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:21,026 Scrapeando item 128\n",
      "2023-02-03 01:31:26,180 Vendo sandalias y zapatillas para ni帽as desde s/15\n",
      "2023-02-03 01:31:26,180 Item 128 scrapeado con 茅xito\n",
      "2023-02-03 01:31:28,217 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:28,217 Scrapeando item 129\n",
      "2023-02-03 01:31:33,388 SHEIN - TOP CON BOTONES DELANTEROS\n",
      "2023-02-03 01:31:33,388 Item 129 scrapeado con 茅xito\n",
      "2023-02-03 01:31:35,425 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:35,429 Scrapeando item 130\n",
      "2023-02-03 01:31:40,606 Enterizos\n",
      "2023-02-03 01:31:40,606 Item 130 scrapeado con 茅xito\n",
      "2023-02-03 01:31:42,648 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:42,648 Scrapeando item 131\n",
      "2023-02-03 01:31:47,798 Nuevas sandalias super comodas\n",
      "2023-02-03 01:31:47,798 Item 131 scrapeado con 茅xito\n",
      "2023-02-03 01:31:49,843 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:49,843 Scrapeando item 132\n",
      "2023-02-03 01:31:55,024 Variedad de modelos de temporada\n",
      "2023-02-03 01:31:55,024 Item 132 scrapeado con 茅xito\n",
      "2023-02-03 01:31:57,068 -------------------------------------------------------------------\n",
      "2023-02-03 01:31:57,068 Scrapeando item 133\n",
      "2023-02-03 01:32:02,271 CONJUNTO A CUADROS DE HILO \n",
      "2023-02-03 01:32:02,271 Item 133 scrapeado con 茅xito\n",
      "2023-02-03 01:32:04,309 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:04,309 Scrapeando item 134\n",
      "2023-02-03 01:32:09,474 Medias Taloneras 硷\n",
      "2023-02-03 01:32:09,474 Item 134 scrapeado con 茅xito\n",
      "2023-02-03 01:32:11,516 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:11,516 Scrapeando item 135\n",
      "2023-02-03 01:32:16,704 Short Enterizo Fabiana\n",
      "2023-02-03 01:32:16,704 Item 135 scrapeado con 茅xito\n",
      "2023-02-03 01:32:18,739 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:18,740 Scrapeando item 136\n",
      "2023-02-03 01:32:23,893 MOCASINES FORESTA - NUEVO\n",
      "2023-02-03 01:32:23,893 Item 136 scrapeado con 茅xito\n",
      "2023-02-03 01:32:25,931 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:25,931 Scrapeando item 137\n",
      "2023-02-03 01:32:31,102 Bellas Sandalias en biocuero\n",
      "2023-02-03 01:32:31,102 Item 137 scrapeado con 茅xito\n",
      "2023-02-03 01:32:33,139 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:33,139 Scrapeando item 138\n",
      "2023-02-03 01:32:38,314 KIMONOS VERANIEGOS PARA DAMA \n",
      "2023-02-03 01:32:38,314 Item 138 scrapeado con 茅xito\n",
      "2023-02-03 01:32:40,347 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:40,350 Scrapeando item 139\n",
      "2023-02-03 01:32:45,526 Vestido de ni帽a\n",
      "2023-02-03 01:32:45,526 Item 139 scrapeado con 茅xito\n",
      "2023-02-03 01:32:47,571 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:47,571 Scrapeando item 140\n",
      "2023-02-03 01:32:52,721 Casacas para damas\n",
      "2023-02-03 01:32:52,721 Item 140 scrapeado con 茅xito\n",
      "2023-02-03 01:32:54,761 -------------------------------------------------------------------\n",
      "2023-02-03 01:32:54,762 Scrapeando item 141\n",
      "2023-02-03 01:32:59,945 CHALECO DE CUERO\n",
      "2023-02-03 01:32:59,945 Item 141 scrapeado con 茅xito\n",
      "2023-02-03 01:33:08,058 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:08,058 Scrapeando item 142\n",
      "2023-02-03 01:33:13,226 Polos OVERSIZED UNISEX - PROMO 3 X S/100.00\n",
      "2023-02-03 01:33:13,226 Item 142 scrapeado con 茅xito\n",
      "2023-02-03 01:33:15,265 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:15,267 Scrapeando item 143\n",
      "2023-02-03 01:33:20,437 Polos Manga Cero De Algod贸n Para Ni帽as\n",
      "2023-02-03 01:33:20,437 Item 143 scrapeado con 茅xito\n",
      "2023-02-03 01:33:22,473 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:22,473 Scrapeando item 144\n",
      "2023-02-03 01:33:27,631 Hermosas sandalias Trujillanas de yute al por mayor y menor\n",
      "2023-02-03 01:33:27,631 Item 144 scrapeado con 茅xito\n",
      "2023-02-03 01:33:29,680 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:29,680 Scrapeando item 145\n",
      "2023-02-03 01:33:34,836 Vacantes Disponibles\n",
      "2023-02-03 01:33:34,836 Item 145 scrapeado con 茅xito\n",
      "2023-02-03 01:33:36,877 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:36,878 Scrapeando item 146\n",
      "2023-02-03 01:33:42,024 Hermosos Boquita De Pez Taco 7 Y 5 ワ\n",
      "2023-02-03 01:33:42,024 Item 146 scrapeado con 茅xito\n",
      "2023-02-03 01:33:44,064 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:44,064 Scrapeando item 147\n",
      "2023-02-03 01:33:49,227 Blusa Camisero para Dama\n",
      "2023-02-03 01:33:49,227 Item 147 scrapeado con 茅xito\n",
      "2023-02-03 01:33:51,263 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:51,265 Scrapeando item 148\n",
      "2023-02-03 01:33:56,428  Sandalia Kelly\n",
      "2023-02-03 01:33:56,428 Item 148 scrapeado con 茅xito\n",
      "2023-02-03 01:33:58,472 -------------------------------------------------------------------\n",
      "2023-02-03 01:33:58,473 Scrapeando item 149\n",
      "2023-02-03 01:34:03,642 Botas De Bombero Lion T-38.5\n",
      "2023-02-03 01:34:03,642 Item 149 scrapeado con 茅xito\n",
      "2023-02-03 01:34:05,685 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:05,685 Scrapeando item 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:34:10,866 Sandalias temporada 2023\n",
      "Super ofertas ○○\n",
      "2023-02-03 01:34:10,866 Item 150 scrapeado con 茅xito\n",
      "2023-02-03 01:34:12,896 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:12,896 Scrapeando item 151\n",
      "2023-02-03 01:34:18,041 SANDALIAS PARA MUJER ELY\n",
      "2023-02-03 01:34:18,041 Item 151 scrapeado con 茅xito\n",
      "2023-02-03 01:34:20,084 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:20,085 Scrapeando item 152\n",
      "2023-02-03 01:34:25,240 Zapatillas  ofertas \n",
      "2023-02-03 01:34:25,240 Item 152 scrapeado con 茅xito\n",
      "2023-02-03 01:34:27,278 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:27,279 Scrapeando item 153\n",
      "2023-02-03 01:34:32,447 Shein Kids | Vestido | 0-3meses\n",
      "2023-02-03 01:34:32,447 Item 153 scrapeado con 茅xito\n",
      "2023-02-03 01:34:34,478 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:34,479 Scrapeando item 154\n",
      "2023-02-03 01:34:39,636 Botines Tim para caballero \n",
      "2023-02-03 01:34:39,636 Item 154 scrapeado con 茅xito\n",
      "2023-02-03 01:34:41,686 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:41,687 Scrapeando item 155\n",
      "2023-02-03 01:34:46,849 Vestidos de fiesta!\n",
      "2023-02-03 01:34:46,849 Item 155 scrapeado con 茅xito\n",
      "2023-02-03 01:34:48,878 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:48,881 Scrapeando item 156\n",
      "2023-02-03 01:34:54,079 Botines Aster Star Nuevo modelo, tendencia 2023\n",
      "2023-02-03 01:34:54,079 Item 156 scrapeado con 茅xito\n",
      "2023-02-03 01:34:56,121 -------------------------------------------------------------------\n",
      "2023-02-03 01:34:56,122 Scrapeando item 157\n",
      "2023-02-03 01:35:01,301 PANTALON DE MARCA NICA PIEZA TALLA L O 34\n",
      "2023-02-03 01:35:01,301 Item 157 scrapeado con 茅xito\n",
      "2023-02-03 01:35:03,344 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:03,345 Scrapeando item 158\n",
      "2023-02-03 01:35:08,489 Cafarenas al mejor precio\n",
      "2023-02-03 01:35:08,489 Item 158 scrapeado con 茅xito\n",
      "2023-02-03 01:35:10,527 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:10,528 Scrapeando item 159\n",
      "2023-02-03 01:35:15,694 Zapatillas de bebe\n",
      "2023-02-03 01:35:15,694 Item 159 scrapeado con 茅xito\n",
      "2023-02-03 01:35:17,727 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:17,728 Scrapeando item 160\n",
      "2023-02-03 01:35:22,896 Venta de Ropa Por Mayor\n",
      "2023-02-03 01:35:22,896 Item 160 scrapeado con 茅xito\n",
      "2023-02-03 01:35:24,933 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:24,934 Scrapeando item 161\n",
      "2023-02-03 01:35:30,102 Polo Dama\n",
      "2023-02-03 01:35:30,102 Item 161 scrapeado con 茅xito\n",
      "2023-02-03 01:35:32,148 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:32,148 Scrapeando item 162\n",
      "2023-02-03 01:35:37,322 leonisa Camiseta manga corta con mangas tejidas talla M\n",
      "2023-02-03 01:35:37,322 Item 162 scrapeado con 茅xito\n",
      "2023-02-03 01:35:39,353 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:39,354 Scrapeando item 163\n",
      "2023-02-03 01:35:44,513 Faja Reloj de arena 100% Recomendadas No Son Chinas\n",
      "2023-02-03 01:35:44,513 Item 163 scrapeado con 茅xito\n",
      "2023-02-03 01:35:46,554 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:46,555 Scrapeando item 164\n",
      "2023-02-03 01:35:51,695 Zandalia De Bebe\n",
      "2023-02-03 01:35:51,695 Item 164 scrapeado con 茅xito\n",
      "2023-02-03 01:35:53,738 -------------------------------------------------------------------\n",
      "2023-02-03 01:35:53,739 Scrapeando item 165\n",
      "2023-02-03 01:35:58,899 Vestido\n",
      "2023-02-03 01:35:58,899 Item 165 scrapeado con 茅xito\n",
      "2023-02-03 01:36:07,004 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:07,004 Scrapeando item 166\n",
      "2023-02-03 01:36:12,166 Sandalias dama 35 - 39\n",
      "2023-02-03 01:36:12,166 Item 166 scrapeado con 茅xito\n",
      "2023-02-03 01:36:14,203 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:14,204 Scrapeando item 167\n",
      "2023-02-03 01:36:19,357 Camisas manga corta cuello neru / chino\n",
      "2023-02-03 01:36:19,357 Item 167 scrapeado con 茅xito\n",
      "2023-02-03 01:36:21,399 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:21,399 Scrapeando item 168\n",
      "2023-02-03 01:36:26,567 CAMISETA OFICIAL DEL LIVERPOOL TEMPORADA 22/23\n",
      "2023-02-03 01:36:26,567 Item 168 scrapeado con 茅xito\n",
      "2023-02-03 01:36:28,607 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:28,608 Scrapeando item 169\n",
      "2023-02-03 01:36:33,767 Remat贸!!Ropa De Maternidad Por Lote O Separado\n",
      "2023-02-03 01:36:33,767 Item 169 scrapeado con 茅xito\n",
      "2023-02-03 01:36:35,799 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:35,799 Scrapeando item 170\n",
      "2023-02-03 01:36:40,955 Polo FA P\n",
      "2023-02-03 01:36:40,955 Item 170 scrapeado con 茅xito\n",
      "2023-02-03 01:36:42,990 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:42,990 Scrapeando item 171\n",
      "2023-02-03 01:36:48,166 Sandalias para Damas 锔\n",
      "2023-02-03 01:36:48,166 Item 171 scrapeado con 茅xito\n",
      "2023-02-03 01:36:50,203 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:50,204 Scrapeando item 172\n",
      "2023-02-03 01:36:55,361 CAMISON LARGO A RAYAS (SECOND HAND)\n",
      "2023-02-03 01:36:55,361 Item 172 scrapeado con 茅xito\n",
      "2023-02-03 01:36:57,404 -------------------------------------------------------------------\n",
      "2023-02-03 01:36:57,404 Scrapeando item 173\n",
      "2023-02-03 01:37:02,570 Knit de\n",
      "2023-02-03 01:37:02,570 Item 173 scrapeado con 茅xito\n",
      "2023-02-03 01:37:04,612 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:04,612 Scrapeando item 174\n",
      "2023-02-03 01:37:09,794 Botines para Damas \n",
      "2023-02-03 01:37:09,794 Item 174 scrapeado con 茅xito\n",
      "2023-02-03 01:37:11,835 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:11,836 Scrapeando item 175\n",
      "2023-02-03 01:37:16,990 Zapatillas para Damas ぉ\n",
      "2023-02-03 01:37:16,990 Item 175 scrapeado con 茅xito\n",
      "2023-02-03 01:37:19,019 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:19,020 Scrapeando item 176\n",
      "2023-02-03 01:37:24,201 POLOS.OVERSIZE\n",
      "2023-02-03 01:37:24,201 Item 176 scrapeado con 茅xito\n",
      "2023-02-03 01:37:26,235 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:26,235 Scrapeando item 177\n",
      "2023-02-03 01:37:31,407 Polo Overcize\n",
      "2023-02-03 01:37:31,407 Item 177 scrapeado con 茅xito\n",
      "2023-02-03 01:37:33,444 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:33,445 Scrapeando item 178\n",
      "2023-02-03 01:37:38,603 Pantalon Tela Imprtada\n",
      "2023-02-03 01:37:38,603 Item 178 scrapeado con 茅xito\n",
      "2023-02-03 01:37:40,644 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:40,646 Scrapeando item 179\n",
      "2023-02-03 01:37:45,807 Sandalias Kawai Para Ni帽@s\n",
      "2023-02-03 01:37:45,807 Item 179 scrapeado con 茅xito\n",
      "2023-02-03 01:37:47,842 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:47,842 Scrapeando item 180\n",
      "2023-02-03 01:37:53,046 Polos 100%algod贸n OVERSIZE 硷硷硷\n",
      "2023-02-03 01:37:53,046 Item 180 scrapeado con 茅xito\n",
      "2023-02-03 01:37:55,078 -------------------------------------------------------------------\n",
      "2023-02-03 01:37:55,079 Scrapeando item 181\n",
      "2023-02-03 01:38:00,267 CASACAS Y CHOMPAS PARA BEBE  EN BUEN ESTADO\n",
      "2023-02-03 01:38:00,267 Item 181 scrapeado con 茅xito\n",
      "2023-02-03 01:38:02,302 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:02,306 Scrapeando item 182\n",
      "2023-02-03 01:38:07,480 Angela'shop\n",
      "2023-02-03 01:38:07,480 Item 182 scrapeado con 茅xito\n",
      "2023-02-03 01:38:09,527 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:09,528 Scrapeando item 183\n",
      "2023-02-03 01:38:14,705 DIVINAS Y BELLAS TENDENCIA JUVENIL \n",
      "2023-02-03 01:38:14,705 Item 183 scrapeado con 茅xito\n",
      "2023-02-03 01:38:16,741 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:16,743 Scrapeando item 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:38:21,906 Casaca cortaviento mujer - ventas solo x mayor\n",
      "2023-02-03 01:38:21,906 Item 184 scrapeado con 茅xito\n",
      "2023-02-03 01:38:23,941 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:23,942 Scrapeando item 185\n",
      "2023-02-03 01:38:29,100 CONJUNTO SASTRE ENFERMERA UNICA PIEZA TALLA M NUEVO\n",
      "2023-02-03 01:38:29,100 Item 185 scrapeado con 茅xito\n",
      "2023-02-03 01:38:31,148 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:31,149 Scrapeando item 186\n",
      "2023-02-03 01:38:36,335 por  HOY\n",
      "2023-02-03 01:38:36,335 Item 186 scrapeado con 茅xito\n",
      "2023-02-03 01:38:38,370 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:38,371 Scrapeando item 187\n",
      "2023-02-03 01:38:43,537 Parkas importadas var贸n\n",
      "2023-02-03 01:38:43,537 Item 187 scrapeado con 茅xito\n",
      "2023-02-03 01:38:45,571 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:45,572 Scrapeando item 188\n",
      "2023-02-03 01:38:50,729 Conjunto 3 piezas\n",
      "2023-02-03 01:38:50,729 Item 188 scrapeado con 茅xito\n",
      "2023-02-03 01:38:52,772 -------------------------------------------------------------------\n",
      "2023-02-03 01:38:52,773 Scrapeando item 189\n",
      "2023-02-03 01:38:57,948 Uniformes Medicos Polipima\n",
      "2023-02-03 01:38:57,948 Item 189 scrapeado con 茅xito\n",
      "2023-02-03 01:39:06,052 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:06,052 Scrapeando item 190\n",
      "2023-02-03 01:39:11,244 Casaca para ni帽os\n",
      "2023-02-03 01:39:11,244 Item 190 scrapeado con 茅xito\n",
      "2023-02-03 01:39:13,281 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:13,282 Scrapeando item 191\n",
      "2023-02-03 01:39:18,438 Capa Larga \n",
      "2023-02-03 01:39:18,438 Item 191 scrapeado con 茅xito\n",
      "2023-02-03 01:39:20,468 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:20,469 Scrapeando item 192\n",
      "2023-02-03 01:39:25,629 Conjuntos Damas\n",
      "2023-02-03 01:39:25,629 Item 192 scrapeado con 茅xito\n",
      "2023-02-03 01:39:27,678 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:27,679 Scrapeando item 193\n",
      "2023-02-03 01:39:32,839 Shorts para ni帽os\n",
      "2023-02-03 01:39:32,839 Item 193 scrapeado con 茅xito\n",
      "2023-02-03 01:39:34,870 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:34,871 Scrapeando item 194\n",
      "2023-02-03 01:39:40,043 CROP HILO TACTO 硷\n",
      "2023-02-03 01:39:40,043 Item 194 scrapeado con 茅xito\n",
      "2023-02-03 01:39:42,073 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:42,075 Scrapeando item 195\n",
      "2023-02-03 01:39:47,238 POLOS DE HOMBRE, TODAS LAS MARCAS, ALGODN REACTIVO.\n",
      "2023-02-03 01:39:47,238 Item 195 scrapeado con 茅xito\n",
      "2023-02-03 01:39:49,270 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:49,271 Scrapeando item 196\n",
      "2023-02-03 01:39:54,438 Polos de Hombre!!\n",
      "2023-02-03 01:39:54,438 Item 196 scrapeado con 茅xito\n",
      "2023-02-03 01:39:56,473 -------------------------------------------------------------------\n",
      "2023-02-03 01:39:56,474 Scrapeando item 197\n",
      "2023-02-03 01:40:01,639 Conjuntos para ni帽os\n",
      "2023-02-03 01:40:01,639 Item 197 scrapeado con 茅xito\n",
      "2023-02-03 01:40:03,682 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:03,684 Scrapeando item 198\n",
      "2023-02-03 01:40:08,855 Nuevo calzado\n",
      "2023-02-03 01:40:08,855 Item 198 scrapeado con 茅xito\n",
      "2023-02-03 01:40:10,893 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:10,895 Scrapeando item 199\n",
      "2023-02-03 01:40:16,051 Conjuntos\n",
      "2023-02-03 01:40:16,051 Item 199 scrapeado con 茅xito\n",
      "2023-02-03 01:40:18,094 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:18,096 Scrapeando item 200\n",
      "2023-02-03 01:40:23,248 ｐｐｐLLEGARON LOS BUZOS Y UNIFORMES DE TODOS LOS COLEGIOS硷硷\n",
      "2023-02-03 01:40:23,248 Item 200 scrapeado con 茅xito\n",
      "2023-02-03 01:40:25,289 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:25,290 Scrapeando item 201\n",
      "2023-02-03 01:40:30,456 Body Y Enterizos Para Bebe Personalizados Para Cualquier Ocacion\n",
      "2023-02-03 01:40:30,456 Item 201 scrapeado con 茅xito\n",
      "2023-02-03 01:40:32,495 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:32,496 Scrapeando item 202\n",
      "2023-02-03 01:40:37,651 Jeans para ni帽as por mayor y menor  talla 4 hasta 14 hacemos env铆os a provincia\n",
      "2023-02-03 01:40:37,651 Item 202 scrapeado con 茅xito\n",
      "2023-02-03 01:40:39,693 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:39,694 Scrapeando item 203\n",
      "2023-02-03 01:40:44,873 Abrigo rosado para ni帽as de 3 a帽os\n",
      "2023-02-03 01:40:44,873 Item 203 scrapeado con 茅xito\n",
      "2023-02-03 01:40:46,915 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:46,917 Scrapeando item 204\n",
      "2023-02-03 01:40:52,175 SANDALIAS ヰVERANO 2023  ぉぉMODA A TUS PIES わ TENEMOS TODAS LAS TALLAS 35 A LA 39 ぉぉぉ\n",
      "2023-02-03 01:40:52,175 Item 204 scrapeado con 茅xito\n",
      "2023-02-03 01:40:54,213 -------------------------------------------------------------------\n",
      "2023-02-03 01:40:54,214 Scrapeando item 205\n",
      "2023-02-03 01:40:59,423 PARACHUTE PANST\n",
      "Tallas S/M/L\n",
      "2023-02-03 01:40:59,423 Item 205 scrapeado con 茅xito\n",
      "2023-02-03 01:41:01,464 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:01,465 Scrapeando item 206\n",
      "2023-02-03 01:41:06,638 Enterizo\n",
      "2023-02-03 01:41:06,638 Item 206 scrapeado con 茅xito\n",
      "2023-02-03 01:41:08,672 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:08,673 Scrapeando item 207\n",
      "2023-02-03 01:41:13,844 Conjuntos para los.peques \n",
      "2023-02-03 01:41:13,844 Item 207 scrapeado con 茅xito\n",
      "2023-02-03 01:41:15,874 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:15,875 Scrapeando item 208\n",
      "2023-02-03 01:41:21,075 Blusas\n",
      "2023-02-03 01:41:21,075 Item 208 scrapeado con 茅xito\n",
      "2023-02-03 01:41:23,111 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:23,113 Scrapeando item 209\n",
      "2023-02-03 01:41:28,270 2 Pares a S/.65  Gran Liquidaci贸n わ Somos Proveedores Mayoristas  Env铆os a todo El Per煤 ヰぉ\n",
      "2023-02-03 01:41:28,270 Item 209 scrapeado con 茅xito\n",
      "2023-02-03 01:41:30,298 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:30,301 Scrapeando item 210\n",
      "2023-02-03 01:41:35,503 Vacantes disponibles 硷\n",
      "2023-02-03 01:41:35,503 Item 210 scrapeado con 茅xito\n",
      "2023-02-03 01:41:37,537 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:37,538 Scrapeando item 211\n",
      "2023-02-03 01:41:42,712 Conjuntos para ni帽os \n",
      "2023-02-03 01:41:42,712 Item 211 scrapeado con 茅xito\n",
      "2023-02-03 01:41:44,755 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:44,756 Scrapeando item 212\n",
      "2023-02-03 01:41:49,933 PIJAMAS DE VERANO POR MAYOR !!\n",
      "2023-02-03 01:41:49,933 Item 212 scrapeado con 茅xito\n",
      "2023-02-03 01:41:51,969 -------------------------------------------------------------------\n",
      "2023-02-03 01:41:51,971 Scrapeando item 213\n",
      "2023-02-03 01:41:57,162 Remato Sandalias Para Bebe 23.5\n",
      "2023-02-03 01:41:57,162 Item 213 scrapeado con 茅xito\n",
      "2023-02-03 01:42:05,269 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:05,269 Scrapeando item 214\n",
      "2023-02-03 01:42:10,451 Vestidos  Cortas Ideal Para Verano\n",
      "2023-02-03 01:42:10,451 Item 214 scrapeado con 茅xito\n",
      "2023-02-03 01:42:12,489 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:12,490 Scrapeando item 215\n",
      "2023-02-03 01:42:17,663 Closet Sale Dama 烩锔\n",
      "2023-02-03 01:42:17,663 Item 215 scrapeado con 茅xito\n",
      "2023-02-03 01:42:19,697 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:19,698 Scrapeando item 216\n",
      "2023-02-03 01:42:24,892 Coderas MTB O'NEAL\n",
      "2023-02-03 01:42:24,892 Item 216 scrapeado con 茅xito\n",
      "2023-02-03 01:42:26,938 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:26,939 Scrapeando item 217\n",
      "2023-02-03 01:42:32,122 Polo Oversize Cazzu\n",
      "2023-02-03 01:42:32,122 Item 217 scrapeado con 茅xito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:42:34,152 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:34,153 Scrapeando item 218\n",
      "2023-02-03 01:42:39,350 Polos QuikSilver, Billabong, Fox y Volcom \n",
      "2023-02-03 01:42:39,350 Item 218 scrapeado con 茅xito\n",
      "2023-02-03 01:42:41,384 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:41,386 Scrapeando item 219\n",
      "2023-02-03 01:42:46,571 SANDALIAS TACO CUA ぉ\n",
      "2023-02-03 01:42:46,571 Item 219 scrapeado con 茅xito\n",
      "2023-02-03 01:42:48,602 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:48,604 Scrapeando item 220\n",
      "2023-02-03 01:42:53,809 SACO DE MARCA UNICA PIEZA TALLA 46R\n",
      "2023-02-03 01:42:53,809 Item 220 scrapeado con 茅xito\n",
      "2023-02-03 01:42:55,837 -------------------------------------------------------------------\n",
      "2023-02-03 01:42:55,838 Scrapeando item 221\n",
      "2023-02-03 01:43:00,995 Chompa abrigadora\n",
      "2023-02-03 01:43:00,995 Item 221 scrapeado con 茅xito\n",
      "2023-02-03 01:43:03,030 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:03,032 Scrapeando item 222\n",
      "2023-02-03 01:43:08,237 Vestido Romper de conejito\n",
      "2023-02-03 01:43:08,237 Item 222 scrapeado con 茅xito\n",
      "2023-02-03 01:43:10,268 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:10,270 Scrapeando item 223\n",
      "2023-02-03 01:43:15,480 POLO MAGO DE OZ\n",
      "2023-02-03 01:43:15,480 Item 223 scrapeado con 茅xito\n",
      "2023-02-03 01:43:17,523 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:17,524 Scrapeando item 224\n",
      "2023-02-03 01:43:22,712 Medias TALONERAS Еぉ\n",
      "2023-02-03 01:43:22,712 Item 224 scrapeado con 茅xito\n",
      "2023-02-03 01:43:24,745 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:24,748 Scrapeando item 225\n",
      "2023-02-03 01:43:29,918 Zapatillas\n",
      "2023-02-03 01:43:29,918 Item 225 scrapeado con 茅xito\n",
      "2023-02-03 01:43:31,958 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:31,960 Scrapeando item 226\n",
      "2023-02-03 01:43:37,120 FALDA\n",
      "2023-02-03 01:43:37,120 Item 226 scrapeado con 茅xito\n",
      "2023-02-03 01:43:45,207 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:45,207 Scrapeando item 227\n",
      "2023-02-03 01:43:50,379 Leonisa Camiseta cuello redondo manga corta color Verde  talla M y L\n",
      "2023-02-03 01:43:50,379 Item 227 scrapeado con 茅xito\n",
      "2023-02-03 01:43:52,414 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:52,415 Scrapeando item 228\n",
      "2023-02-03 01:43:57,610 CAMISA TALLA M ヰ\n",
      "2023-02-03 01:43:57,610 Item 228 scrapeado con 茅xito\n",
      "2023-02-03 01:43:59,647 -------------------------------------------------------------------\n",
      "2023-02-03 01:43:59,649 Scrapeando item 229\n",
      "2023-02-03 01:44:04,822 Sandal De Dama Latina\n",
      "2023-02-03 01:44:04,822 Item 229 scrapeado con 茅xito\n",
      "2023-02-03 01:44:06,862 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:06,864 Scrapeando item 230\n",
      "2023-02-03 01:44:12,025 Enterizo Oversize\n",
      "2023-02-03 01:44:12,025 Item 230 scrapeado con 茅xito\n",
      "2023-02-03 01:44:14,067 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:14,069 Scrapeando item 231\n",
      "2023-02-03 01:44:19,257 Zapatillas \n",
      "2023-02-03 01:44:19,257 Item 231 scrapeado con 茅xito\n",
      "2023-02-03 01:44:21,296 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:21,297 Scrapeando item 232\n",
      "2023-02-03 01:44:26,462 ZAPATOS MARCA BALLY PARA MUJER\n",
      "2023-02-03 01:44:26,462 Item 232 scrapeado con 茅xito\n",
      "2023-02-03 01:44:28,496 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:28,497 Scrapeando item 233\n",
      "2023-02-03 01:44:33,701 Talla 38\n",
      "2023-02-03 01:44:33,701 Item 233 scrapeado con 茅xito\n",
      "2023-02-03 01:44:35,742 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:35,743 Scrapeando item 234\n",
      "2023-02-03 01:44:40,986 Sandalias de dama\n",
      "2023-02-03 01:44:40,986 Item 234 scrapeado con 茅xito\n",
      "2023-02-03 01:44:43,021 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:43,022 Scrapeando item 235\n",
      "2023-02-03 01:44:48,242 Vestuario De Marinera Casi Nuevo Para Ni帽a De 6 A 9 A帽os\n",
      "2023-02-03 01:44:48,242 Item 235 scrapeado con 茅xito\n",
      "2023-02-03 01:44:50,279 -------------------------------------------------------------------\n",
      "2023-02-03 01:44:50,280 Scrapeando item 236\n",
      "2023-02-03 01:44:55,490 POLO BRILLANTE \n",
      "2023-02-03 01:44:55,490 Item 236 scrapeado con 茅xito\n",
      "2023-02-03 01:45:03,596 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:03,596 Scrapeando item 237\n",
      "2023-02-03 01:45:08,772 Venta o trueque por ropa o zapatillas de ni帽as\n",
      "2023-02-03 01:45:08,772 Item 237 scrapeado con 茅xito\n",
      "2023-02-03 01:45:10,801 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:10,802 Scrapeando item 238\n",
      "2023-02-03 01:45:15,964 Dockers, Pierre cardin, Polo Ralph Lauren, grand slam penguin\n",
      "2023-02-03 01:45:15,964 Item 238 scrapeado con 茅xito\n",
      "2023-02-03 01:45:18,006 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:18,007 Scrapeando item 239\n",
      "2023-02-03 01:45:23,209 Sandalia Dino\n",
      "2023-02-03 01:45:23,209 Item 239 scrapeado con 茅xito\n",
      "2023-02-03 01:45:25,242 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:25,243 Scrapeando item 240\n",
      "2023-02-03 01:45:30,446 Shorts y tops\n",
      "2023-02-03 01:45:30,446 Item 240 scrapeado con 茅xito\n",
      "2023-02-03 01:45:32,484 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:32,485 Scrapeando item 241\n",
      "2023-02-03 01:45:37,640 Sandalias Marquis nuevas de dama\n",
      "2023-02-03 01:45:37,640 Item 241 scrapeado con 茅xito\n",
      "2023-02-03 01:45:39,675 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:39,676 Scrapeando item 242\n",
      "2023-02-03 01:45:44,828 Vestido Licrado 3\n",
      "2023-02-03 01:45:44,828 Item 242 scrapeado con 茅xito\n",
      "2023-02-03 01:45:46,871 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:46,873 Scrapeando item 243\n",
      "2023-02-03 01:45:52,060 ○ Zapatos de Plataforma de Mujer ｏヰ○\n",
      "2023-02-03 01:45:52,060 Item 243 scrapeado con 茅xito\n",
      "2023-02-03 01:45:54,098 -------------------------------------------------------------------\n",
      "2023-02-03 01:45:54,099 Scrapeando item 244\n",
      "2023-02-03 01:45:59,284 Zapatos Y Zapatillas De Ni帽a\n",
      "2023-02-03 01:45:59,284 Item 244 scrapeado con 茅xito\n",
      "2023-02-03 01:46:01,320 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:01,321 Scrapeando item 245\n",
      "2023-02-03 01:46:06,468 CASACAS DE HOMBRE CON PELUCHE \n",
      "2023-02-03 01:46:06,468 Item 245 scrapeado con 茅xito\n",
      "2023-02-03 01:46:08,511 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:08,512 Scrapeando item 246\n",
      "2023-02-03 01:46:13,714 ┌BALERINAS PARA BEBITAS IMPORTADAS ┌\n",
      "2023-02-03 01:46:13,714 Item 246 scrapeado con 茅xito\n",
      "2023-02-03 01:46:15,742 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:15,743 Scrapeando item 247\n",
      "2023-02-03 01:46:20,926 Closet sale\n",
      "2023-02-03 01:46:20,926 Item 247 scrapeado con 茅xito\n",
      "2023-02-03 01:46:22,959 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:22,960 Scrapeando item 248\n",
      "2023-02-03 01:46:28,133 Polo de gatitos kawaii polo de mujer\n",
      "2023-02-03 01:46:28,133 Item 248 scrapeado con 茅xito\n",
      "2023-02-03 01:46:30,168 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:30,169 Scrapeando item 249\n",
      "2023-02-03 01:46:35,347 Sandalias FIORETTA animal print talla 38\n",
      "2023-02-03 01:46:35,347 Item 249 scrapeado con 茅xito\n",
      "2023-02-03 01:46:37,381 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:37,384 Scrapeando item 250\n",
      "2023-02-03 01:46:42,611 CASACA STAR WARS-Kylo Ren, Disney, para ni帽o.\n",
      "2023-02-03 01:46:42,611 Item 250 scrapeado con 茅xito\n",
      "2023-02-03 01:46:44,652 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:44,653 Scrapeando item 251\n",
      "2023-02-03 01:46:49,915 Conjunto Roma\n",
      "2023-02-03 01:46:49,915 Item 251 scrapeado con 茅xito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:46:51,956 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:51,958 Scrapeando item 252\n",
      "2023-02-03 01:46:57,170 Zapatos\n",
      "2023-02-03 01:46:57,170 Item 252 scrapeado con 茅xito\n",
      "2023-02-03 01:46:59,204 -------------------------------------------------------------------\n",
      "2023-02-03 01:46:59,205 Scrapeando item 253\n",
      "2023-02-03 01:47:04,410 MEDIAS ANTIDESLIZANTES, c贸modos para caminar en piso bailar marinera, bailar, danzar, pilates yoga\n",
      "2023-02-03 01:47:04,410 Item 253 scrapeado con 茅xito\n",
      "2023-02-03 01:47:06,450 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:06,452 Scrapeando item 254\n",
      "2023-02-03 01:47:11,609 Conjunto dos piezas\n",
      "2023-02-03 01:47:11,609 Item 254 scrapeado con 茅xito\n",
      "2023-02-03 01:47:19,717 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:19,717 Scrapeando item 255\n",
      "2023-02-03 01:47:24,884 Oferta De Sandiltas Pada Ni帽as\n",
      "2023-02-03 01:47:24,884 Item 255 scrapeado con 茅xito\n",
      "2023-02-03 01:47:26,917 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:26,918 Scrapeando item 256\n",
      "2023-02-03 01:47:32,081 Conjunto Adriana en Rib Grueso\n",
      "2023-02-03 01:47:32,081 Item 256 scrapeado con 茅xito\n",
      "2023-02-03 01:47:34,116 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:34,118 Scrapeando item 257\n",
      "2023-02-03 01:47:39,276 Conjunto  tela loma\n",
      "2023-02-03 01:47:39,276 Item 257 scrapeado con 茅xito\n",
      "2023-02-03 01:47:41,309 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:41,310 Scrapeando item 258\n",
      "2023-02-03 01:47:46,467 Conjunto de ni帽os\n",
      "2023-02-03 01:47:46,467 Item 258 scrapeado con 茅xito\n",
      "2023-02-03 01:47:48,506 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:48,508 Scrapeando item 259\n",
      "2023-02-03 01:47:53,669 Broderi + eng眉es color blanco - Talla Small\n",
      "2023-02-03 01:47:53,669 Item 259 scrapeado con 茅xito\n",
      "2023-02-03 01:47:55,713 -------------------------------------------------------------------\n",
      "2023-02-03 01:47:55,714 Scrapeando item 260\n",
      "2023-02-03 01:48:00,892 GUANTE CORRUGADO\n",
      "2023-02-03 01:48:00,892 Item 260 scrapeado con 茅xito\n",
      "2023-02-03 01:48:02,932 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:02,933 Scrapeando item 261\n",
      "2023-02-03 01:48:08,108 ぉ HERMOSAS SANDALIAS VERANO ぉ\n",
      "2023-02-03 01:48:08,108 Item 261 scrapeado con 茅xito\n",
      "2023-02-03 01:48:10,147 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:10,148 Scrapeando item 262\n",
      "2023-02-03 01:48:15,313 Zapatilla Taco Incorporado\n",
      "2023-02-03 01:48:15,313 Item 262 scrapeado con 茅xito\n",
      "2023-02-03 01:48:17,351 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:17,352 Scrapeando item 263\n",
      "2023-02-03 01:48:22,509 Overol para ni帽a\n",
      "2023-02-03 01:48:22,509 Item 263 scrapeado con 茅xito\n",
      "2023-02-03 01:48:24,539 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:24,540 Scrapeando item 264\n",
      "2023-02-03 01:48:29,710 Ropa importada \n",
      "2023-02-03 01:48:29,710 Item 264 scrapeado con 茅xito\n",
      "2023-02-03 01:48:31,751 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:31,753 Scrapeando item 265\n",
      "2023-02-03 01:48:36,899 Vestido Isabela Encanto Disney - Isabella Madrigal\n",
      "2023-02-03 01:48:36,899 Item 265 scrapeado con 茅xito\n",
      "2023-02-03 01:48:38,941 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:38,942 Scrapeando item 266\n",
      "2023-02-03 01:48:44,115 Cambio x vestido de ni帽a talla 4 color blanco o ivory\n",
      "2023-02-03 01:48:44,115 Item 266 scrapeado con 茅xito\n",
      "2023-02-03 01:48:46,170 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:46,172 Scrapeando item 267\n",
      "2023-02-03 01:48:51,348 CARTUCHERA KAWAI\n",
      "2023-02-03 01:48:51,348 Item 267 scrapeado con 茅xito\n",
      "2023-02-03 01:48:53,391 -------------------------------------------------------------------\n",
      "2023-02-03 01:48:53,392 Scrapeando item 268\n",
      "2023-02-03 01:48:58,553 SANDALIAS Y ZAPATILLAS○\n",
      "硷TENDENCIA 2023 硷\n",
      "2023-02-03 01:48:58,553 Item 268 scrapeado con 茅xito\n",
      "2023-02-03 01:49:00,591 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:00,592 Scrapeando item 269\n",
      "2023-02-03 01:49:05,748 boxers para ninos\n",
      "2023-02-03 01:49:05,748 Item 269 scrapeado con 茅xito\n",
      "2023-02-03 01:49:07,791 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:07,792 Scrapeando item 270\n",
      "2023-02-03 01:49:13,002 POLOS PARA HOMBRE \n",
      "2023-02-03 01:49:13,002 Item 270 scrapeado con 茅xito\n",
      "2023-02-03 01:49:15,035 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:15,037 Scrapeando item 271\n",
      "2023-02-03 01:49:20,242 SHORTS CON BROCHES ヰ\n",
      "2023-02-03 01:49:20,242 Item 271 scrapeado con 茅xito\n",
      "2023-02-03 01:49:22,274 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:22,276 Scrapeando item 272\n",
      "2023-02-03 01:49:27,423 Zapatillas Altas Urbanas\n",
      "2023-02-03 01:49:27,423 Item 272 scrapeado con 茅xito\n",
      "2023-02-03 01:49:29,453 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:29,454 Scrapeando item 273\n",
      "2023-02-03 01:49:34,632 Pantalones Palazos\n",
      "2023-02-03 01:49:34,632 Item 273 scrapeado con 茅xito\n",
      "2023-02-03 01:49:36,674 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:36,675 Scrapeando item 274\n",
      "2023-02-03 01:49:41,838 ZAPATILLAS URBANAS ぁ\n",
      "2023-02-03 01:49:41,838 Item 274 scrapeado con 茅xito\n",
      "2023-02-03 01:49:43,880 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:43,881 Scrapeando item 275\n",
      "2023-02-03 01:49:49,036 Lindas Prendas De Mujer Gt44!\n",
      "2023-02-03 01:49:49,036 Item 275 scrapeado con 茅xito\n",
      "2023-02-03 01:49:51,079 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:51,081 Scrapeando item 276\n",
      "2023-02-03 01:49:56,272 Conjunto GYM ΦFITH\n",
      "2023-02-03 01:49:56,272 Item 276 scrapeado con 茅xito\n",
      "2023-02-03 01:49:58,311 -------------------------------------------------------------------\n",
      "2023-02-03 01:49:58,313 Scrapeando item 277\n",
      "2023-02-03 01:50:03,487 Zapatos Pibes para beb茅 de segunda\n",
      "2023-02-03 01:50:03,487 Item 277 scrapeado con 茅xito\n",
      "2023-02-03 01:50:05,520 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:05,521 Scrapeando item 278\n",
      "2023-02-03 01:50:10,684 Joggers jeans tallas extras grandes xxl xxxl\n",
      "2023-02-03 01:50:10,684 Item 278 scrapeado con 茅xito\n",
      "2023-02-03 01:50:18,787 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:18,787 Scrapeando item 279\n",
      "2023-02-03 01:50:23,948 Top asim茅trico para se帽oritas\n",
      "2023-02-03 01:50:23,948 Item 279 scrapeado con 茅xito\n",
      "2023-02-03 01:50:25,988 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:25,989 Scrapeando item 280\n",
      "2023-02-03 01:50:31,160 OVERSIZE- Polos Para Mujeres Y Caballeros\n",
      "2023-02-03 01:50:31,160 Item 280 scrapeado con 茅xito\n",
      "2023-02-03 01:50:33,195 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:33,197 Scrapeando item 281\n",
      "2023-02-03 01:50:38,361 硷 LTIMAS  VACANTES DISPONIBLES 硷\n",
      "2023-02-03 01:50:38,361 Item 281 scrapeado con 茅xito\n",
      "2023-02-03 01:50:40,403 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:40,404 Scrapeando item 282\n",
      "2023-02-03 01:50:45,565 ZAPATOS ORIGINAL\n",
      "2023-02-03 01:50:45,565 Item 282 scrapeado con 茅xito\n",
      "2023-02-03 01:50:47,607 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:47,608 Scrapeando item 283\n",
      "2023-02-03 01:50:52,775 Camison\n",
      "2023-02-03 01:50:52,775 Item 283 scrapeado con 茅xito\n",
      "2023-02-03 01:50:54,806 -------------------------------------------------------------------\n",
      "2023-02-03 01:50:54,808 Scrapeando item 284\n",
      "2023-02-03 01:50:59,961 PAR DE BOTAS NUEVAS\n",
      "2023-02-03 01:50:59,961 Item 284 scrapeado con 茅xito\n",
      "2023-02-03 01:51:01,996 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:01,996 Scrapeando item 285\n",
      "2023-02-03 01:51:07,157 POLOS PERSONALIZADOS PARA SAN VALENTIN \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:51:07,157 Item 285 scrapeado con 茅xito\n",
      "2023-02-03 01:51:09,187 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:09,188 Scrapeando item 286\n",
      "2023-02-03 01:51:14,351 Vestido rabe de Jordania ( abaya )\n",
      "2023-02-03 01:51:14,351 Item 286 scrapeado con 茅xito\n",
      "2023-02-03 01:51:16,391 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:16,393 Scrapeando item 287\n",
      "2023-02-03 01:51:21,552 Summer\n",
      "2023-02-03 01:51:21,552 Item 287 scrapeado con 茅xito\n",
      "2023-02-03 01:51:23,597 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:23,598 Scrapeando item 288\n",
      "2023-02-03 01:51:28,795 Uniforme Y Buzo\n",
      "2023-02-03 01:51:28,795 Item 288 scrapeado con 茅xito\n",
      "2023-02-03 01:51:30,838 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:30,840 Scrapeando item 289\n",
      "2023-02-03 01:51:36,011 Zapatos de tac贸n \n",
      "2023-02-03 01:51:36,011 Item 289 scrapeado con 茅xito\n",
      "2023-02-03 01:51:38,051 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:38,053 Scrapeando item 290\n",
      "2023-02-03 01:51:43,196 Botas Strecho Con Plataforma, Taco 9\n",
      "2023-02-03 01:51:43,196 Item 290 scrapeado con 茅xito\n",
      "2023-02-03 01:51:45,227 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:45,228 Scrapeando item 291\n",
      "2023-02-03 01:51:50,409 TOPS MANGA LARGA OFERTA\n",
      "2023-02-03 01:51:50,409 Item 291 scrapeado con 茅xito\n",
      "2023-02-03 01:51:52,446 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:52,447 Scrapeando item 292\n",
      "2023-02-03 01:51:57,624 BLAZER CROP COLOR CAMELL OSCURO\n",
      "2023-02-03 01:51:57,624 Item 292 scrapeado con 茅xito\n",
      "2023-02-03 01:51:59,656 -------------------------------------------------------------------\n",
      "2023-02-03 01:51:59,658 Scrapeando item 293\n",
      "2023-02-03 01:52:04,814 DIA DEL AMOR Y LA AMISTAD REGALA ALGO BONITO CON EL DISEO Q MAS TE GUSTE\n",
      "2023-02-03 01:52:04,814 Item 293 scrapeado con 茅xito\n",
      "2023-02-03 01:52:06,865 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:06,866 Scrapeando item 294\n",
      "2023-02-03 01:52:12,068 VESTIDO LINDA \n",
      "2023-02-03 01:52:12,068 Item 294 scrapeado con 茅xito\n",
      "2023-02-03 01:52:14,096 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:14,097 Scrapeando item 295\n",
      "2023-02-03 01:52:19,298 Pantalon de vestir HyM\n",
      "2023-02-03 01:52:19,298 Item 295 scrapeado con 茅xito\n",
      "2023-02-03 01:52:21,330 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:21,331 Scrapeando item 296\n",
      "2023-02-03 01:52:26,487 Enterizo para dama 猴\n",
      "2023-02-03 01:52:26,487 Item 296 scrapeado con 茅xito\n",
      "2023-02-03 01:52:28,527 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:28,529 Scrapeando item 297\n",
      "2023-02-03 01:52:33,694 Legginss\n",
      "2023-02-03 01:52:33,694 Item 297 scrapeado con 茅xito\n",
      "2023-02-03 01:52:35,727 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:35,729 Scrapeando item 298\n",
      "2023-02-03 01:52:40,884 Sandalias de Dama para Verano\n",
      "2023-02-03 01:52:40,884 Item 298 scrapeado con 茅xito\n",
      "2023-02-03 01:52:42,920 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:42,921 Scrapeando item 299\n",
      "2023-02-03 01:52:48,105 Closet sale \n",
      "Shorts talla s \n",
      "Pago contraentrega\n",
      "2023-02-03 01:52:48,105 Item 299 scrapeado con 茅xito\n",
      "2023-02-03 01:52:50,146 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:50,147 Scrapeando item 300\n",
      "2023-02-03 01:52:55,316 Buzos de colegio\n",
      "2023-02-03 01:52:55,316 Item 300 scrapeado con 茅xito\n",
      "2023-02-03 01:52:57,356 -------------------------------------------------------------------\n",
      "2023-02-03 01:52:57,357 Scrapeando item 301\n",
      "2023-02-03 01:53:02,510 Casacas Na煤tica\n",
      "2023-02-03 01:53:02,510 Item 301 scrapeado con 茅xito\n",
      "2023-02-03 01:53:04,553 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:04,555 Scrapeando item 302\n",
      "2023-02-03 01:53:09,703 CASACA DE CUERO ORIGINAL NEGRA HOMBRE (HECHA EN ARGENTINA)\n",
      "2023-02-03 01:53:09,703 Item 302 scrapeado con 茅xito\n",
      "2023-02-03 01:53:17,825 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:17,825 Scrapeando item 303\n",
      "2023-02-03 01:53:22,992  Vestidos tem谩tica sirenita baby\n",
      "2023-02-03 01:53:22,992 Item 303 scrapeado con 茅xito\n",
      "2023-02-03 01:53:25,028 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:25,029 Scrapeando item 304\n",
      "2023-02-03 01:53:30,188 Remate Closet sale \n",
      "2023-02-03 01:53:30,188 Item 304 scrapeado con 茅xito\n",
      "2023-02-03 01:53:32,225 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:32,227 Scrapeando item 305\n",
      "2023-02-03 01:53:37,392 Ropa de\n",
      "Bebe\n",
      "2023-02-03 01:53:37,392 Item 305 scrapeado con 茅xito\n",
      "2023-02-03 01:53:39,433 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:39,434 Scrapeando item 306\n",
      "2023-02-03 01:53:44,606 Ropa de Beb茅s Y Ni帽os c/u\n",
      "2023-02-03 01:53:44,606 Item 306 scrapeado con 茅xito\n",
      "2023-02-03 01:53:46,636 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:46,637 Scrapeando item 307\n",
      "2023-02-03 01:53:51,813 ENTRADAS PARA ROMEO SANTOS\n",
      "2023-02-03 01:53:51,813 Item 307 scrapeado con 茅xito\n",
      "2023-02-03 01:53:53,847 -------------------------------------------------------------------\n",
      "2023-02-03 01:53:53,849 Scrapeando item 308\n",
      "2023-02-03 01:53:59,013 Zapatillas Vans de mujer\n",
      "2023-02-03 01:53:59,013 Item 308 scrapeado con 茅xito\n",
      "2023-02-03 01:54:01,043 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:01,045 Scrapeando item 309\n",
      "2023-02-03 01:54:06,196 SHORTS Y POLOS \n",
      "2023-02-03 01:54:06,196 Item 309 scrapeado con 茅xito\n",
      "2023-02-03 01:54:08,243 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:08,244 Scrapeando item 310\n",
      "2023-02-03 01:54:13,414 Zapatillas Vans  Sk8 hi  \n",
      "2023-02-03 01:54:13,414 Item 310 scrapeado con 茅xito\n",
      "2023-02-03 01:54:15,456 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:15,457 Scrapeando item 311\n",
      "2023-02-03 01:54:20,600 POLOS OVERSIZE DC SHOES | ORIGINALES\n",
      "2023-02-03 01:54:20,600 Item 311 scrapeado con 茅xito\n",
      "2023-02-03 01:54:22,645 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:22,646 Scrapeando item 312\n",
      "2023-02-03 01:54:27,853 Terno \n",
      "2023-02-03 01:54:27,853 Item 312 scrapeado con 茅xito\n",
      "2023-02-03 01:54:29,887 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:29,888 Scrapeando item 313\n",
      "2023-02-03 01:54:35,049 Tacos Mas Secadora\n",
      "2023-02-03 01:54:35,065 Item 313 scrapeado con 茅xito\n",
      "2023-02-03 01:54:37,094 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:37,095 Scrapeando item 314\n",
      "2023-02-03 01:54:42,280 DKNY Zapatillas mujer - Talla 9 - Precio negociable\n",
      "2023-02-03 01:54:42,280 Item 314 scrapeado con 茅xito\n",
      "2023-02-03 01:54:44,319 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:44,320 Scrapeando item 315\n",
      "2023-02-03 01:54:49,505 Seminuevo Uniforme Maria Reyna De La Esperanza\n",
      "2023-02-03 01:54:49,505 Item 315 scrapeado con 茅xito\n",
      "2023-02-03 01:54:51,550 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:51,551 Scrapeando item 316\n",
      "2023-02-03 01:54:56,773 BOTAS OSH KOSH TALLA 24\n",
      "2023-02-03 01:54:56,773 Item 316 scrapeado con 茅xito\n",
      "2023-02-03 01:54:58,816 -------------------------------------------------------------------\n",
      "2023-02-03 01:54:58,817 Scrapeando item 317\n",
      "2023-02-03 01:55:03,976 Blusitas rumberas, talla STD\n",
      "2023-02-03 01:55:03,976 Item 317 scrapeado con 茅xito\n",
      "2023-02-03 01:55:06,012 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:06,014 Scrapeando item 318\n",
      "2023-02-03 01:55:11,253 CASACAS PUFFER CROP DELIVERY GRATIS A SU DOMICILIO \n",
      "2023-02-03 01:55:11,253 Item 318 scrapeado con 茅xito\n",
      "2023-02-03 01:55:13,292 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:13,293 Scrapeando item 319\n",
      "2023-02-03 01:55:18,509 JEANS Y CAMISAS \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 01:55:18,509 Item 319 scrapeado con 茅xito\n",
      "2023-02-03 01:55:20,549 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:20,551 Scrapeando item 320\n",
      "2023-02-03 01:55:25,722 Pantalon Correa Por Mayor Y Unidad\n",
      "2023-02-03 01:55:25,738 Item 320 scrapeado con 茅xito\n",
      "2023-02-03 01:55:27,768 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:27,769 Scrapeando item 321\n",
      "2023-02-03 01:55:32,957 Todo x 48 soles \n",
      "2023-02-03 01:55:32,957 Item 321 scrapeado con 茅xito\n",
      "2023-02-03 01:55:34,992 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:34,993 Scrapeando item 322\n",
      "2023-02-03 01:55:40,209 Ropa de bebe en remate\n",
      "2023-02-03 01:55:40,209 Item 322 scrapeado con 茅xito\n",
      "2023-02-03 01:55:42,246 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:42,247 Scrapeando item 323\n",
      "2023-02-03 01:55:47,493 Remato Ropa De Ni帽a Talla 6 De Segunda\n",
      "2023-02-03 01:55:47,493 Item 323 scrapeado con 茅xito\n",
      "2023-02-03 01:55:49,523 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:49,525 Scrapeando item 324\n",
      "2023-02-03 01:55:54,728 Hermoso polo importado de PUNTA CANA\n",
      "2023-02-03 01:55:54,728 Item 324 scrapeado con 茅xito\n",
      "2023-02-03 01:55:56,766 -------------------------------------------------------------------\n",
      "2023-02-03 01:55:56,767 Scrapeando item 325\n",
      "2023-02-03 01:56:01,969 ENTRADA PARA ROMEO SANTOS\n",
      "2023-02-03 01:56:01,969 Item 325 scrapeado con 茅xito\n",
      "2023-02-03 01:56:04,006 -------------------------------------------------------------------\n",
      "2023-02-03 01:56:04,008 Scrapeando item 326\n",
      "2023-02-03 01:56:09,213 Zandalias para dama modelo 2021.\n",
      "2023-02-03 01:56:09,213 Item 326 scrapeado con 茅xito\n",
      "2023-02-03 01:56:17,360 -------------------------------------------------------------------\n",
      "2023-02-03 01:56:17,360 Scrapeando item 327\n",
      "2023-02-03 01:56:22,534 Hermoso polo \n",
      "2023-02-03 01:56:22,534 Item 327 scrapeado con 茅xito\n",
      "2023-02-03 01:56:24,566 -------------------------------------------------------------------\n",
      "2023-02-03 01:56:24,580 Se hall贸 0 errores\n",
      "2023-02-03 01:56:24,581 Fin de la extraccion\n",
      "2023-02-03 01:56:24,583 Guardando Data\n",
      "2023-02-03 01:56:25,105 Data Guardados Correctamente\n",
      "2023-02-03 01:56:25,106 Guardando Error\n",
      "2023-02-03 01:56:25,108 El archivo de tipo Error no se va a guardar por no tener informaci贸n\n",
      "2023-02-03 01:56:25,109 Guardando tiempos\n",
      "2023-02-03 01:56:25,111 Productos Extra铆dos: 284\n",
      "2023-02-03 01:56:25,112 Hora Fin: 01:56:25\n",
      "2023-02-03 01:56:25,263 Tiempos Guardados Correctamente\n",
      "2023-02-03 01:56:25,263 Programa finalizado\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
