{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import basicConfig, ERROR, getLogger, INFO, log\n",
    "from os import _exit, getenv, makedirs, path\n",
    "from re import findall\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire import webdriver\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, ElementNotInteractableException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    def __init__(self):\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": []\n",
    "        }\n",
    "        \n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "    \n",
    "    def _append_error(self, error, enlace):\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_message = traceback_error._str\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        log(ERROR, error_message)  \n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(error_message)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"delivery_data\": [],\n",
    "            \"delivery_direccion\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": []\n",
    "        }\n",
    "        \n",
    "    def _get_dataset(self):\n",
    "        return self._dataset\n",
    "    \n",
    "    def _append_data(self, item, fecha_extraccion, enlace):\n",
    "        self._dataset[\"titulo_marketplace\"].append(item.get('marketplace_listing_title'))\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get('creation_time'))\n",
    "        self._dataset[\"disponible\"].append(item.get('is_live'))\n",
    "        self._dataset[\"vendido\"].append(item.get('is_sold'))\n",
    "        self._dataset[\"cantidad\"].append(item.get('listing_inventory_type'))\n",
    "        self._dataset[\"name_vendedor\"].append(item.get('story').get('actors')[0].get('name'))\n",
    "        self._dataset[\"tipo_vendedor\"].append(item.get('story').get('actors')[0]['__typename'])\n",
    "        self._dataset[\"id_vendedor\"].append(item.get('story').get('actors')[0]['id'])\n",
    "        self._dataset[\"locacion_id\"].append(item.get('location_vanity_or_id'))\n",
    "        self._dataset[\"latitud\"].append(item.get('location', {}).get('latitude'))\n",
    "        self._dataset[\"longitud\"].append(item.get('location', {}).get('longitude'))\n",
    "        self._dataset[\"precio\"].append(item.get('listing_price', {}).get('amount'))\n",
    "        self._dataset[\"tipo_moneda\"].append(item.get('listing_price', {}).get('currency'))\n",
    "        self._dataset[\"amount_with_concurrency\"].append(item.get('listing_price', {}).get('amount_with_offset_in_currency'))\n",
    "        self._dataset[\"tipo_delivery\"].append(item.get('delivery_types', [None])[0])\n",
    "        self._dataset[\"delivery_data\"].append(item.get(\"delivery_data\", {}).get('carrier'))\n",
    "        self._dataset[\"delivery_direccion\"].append(item.get(\"delivery_data\", {}).get('delivery_address'))\n",
    "        self._dataset[\"descripcion\"].append(item.get('redacted_description', {}).get('text'))\n",
    "        self._dataset[\"fecha_union_vendedor\"].append(item.get('marketplace_listing_seller', {}).get('join_time'))  \n",
    "        data = item.get('location_text', {})\n",
    "        if data:\n",
    "            data = data.get('text')\n",
    "        self._dataset[\"locacion\"].append(data)\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    def __init__(self, start):\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(start))\n",
    "        log(INFO, f\"Hora de inicio: {self._hora_inicio}\")\n",
    "        self._fecha = (datetime.now().date() - timedelta(days=1)).strftime('%d/%m/%Y')\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = None\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._enlace = None\n",
    "        self._observaciones = None\n",
    "        self._errores = None\n",
    "        \n",
    "    def _get_fecha(self):\n",
    "        return self._fecha\n",
    "    \n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "    \n",
    "    def _set_cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "    \n",
    "    def _set_errores(self, errores):\n",
    "        self._errores = errores\n",
    "    \n",
    "    def _set_param_final(self, start):\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        log(INFO, f\"Productos Extraídos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")\n",
    "        total = end - start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = int(self._cantidad /(total / 60))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"Representa a un bot para hacer web scarping en fb marketplace.\n",
    "\n",
    "    Attributes:\n",
    "        driver (Object): Maneja un navegador para hacer web scraping\n",
    "        wait (Object): Maneja el Tiempo de espera durante la ejecución del bot\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, start):\n",
    "        \"\"\"Inicializa un objeto de tipo ScraperFb.\n",
    "\n",
    "        Args:\n",
    "            driver (Object): [Driver]\n",
    "            wait (Object): [Wait]\n",
    "        \"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo(start)\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\" : 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\",prefs)\n",
    "        self.driver = webdriver.Chrome(chrome_options=chrome_options,service=Service(ChromeDriverManager().install()))\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "\n",
    "    def _get_data(self):\n",
    "        return self._data\n",
    "    \n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "        \n",
    "    def iniciar_sesion(self, url):\n",
    "        \"\"\"Inicia sesión en una página web usando un usuario y contraseña\n",
    "\n",
    "        Args:\n",
    "            url (str): [Url]\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesión\")\n",
    "        self.driver.get(url)\n",
    "        self.driver.maximize_window()\n",
    "        username = self.wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self.wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        username.send_keys(getenv('FB_USERNAME'))\n",
    "        password.send_keys(getenv('FB_PASSWORD'))\n",
    "        self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))).click()\n",
    "        log(INFO, \"Inicio de sesión con éxito\")\n",
    "        \n",
    "    def mapear_datos(self, url):\n",
    "        sleep(10)\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self.driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self.driver.switch_to.window(\"newtab\")\n",
    "        self.driver.get(url)\n",
    "        \n",
    "        sleep(8)\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self.driver.find_elements(By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]')\n",
    "        fecha_publicacion = fecha_extraccion = int(datetime.strptime(self._tiempo._get_fecha(),\"%d/%m/%Y\").timestamp())\n",
    "        fecha_flag = fecha_extraccion + 86400\n",
    "        i=0\n",
    "        e=0\n",
    "        del self.driver.requests\n",
    "        \n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "            \n",
    "            try:\n",
    "                enlace = findall(\"(.*)\\/\\?\", ropa[i].find_element(By.XPATH, \".//ancestor::a\").get_attribute('href'))[0]\n",
    "            except NoSuchElementException as error:\n",
    "                enlace = None\n",
    "                self._errores._append_error(error, enlace)\n",
    "            \n",
    "            try:\n",
    "                ropa[i].click()\n",
    "                sleep(5)\n",
    "                for request in self.driver.requests:\n",
    "                    if not request.response or 'graphql' not in request.url:\n",
    "                        continue\n",
    "                    \n",
    "                    body = decode(request.response.body, request.response.headers.get('Content-Encoding', 'identity'))\n",
    "                    decoded_body = body.decode('utf-8')\n",
    "                    json_data = loads(decoded_body)\n",
    "                    \n",
    "                    if 'prefetch_uris_v2' not in json_data['extensions']:\n",
    "                        continue\n",
    "\n",
    "                    fecha_publicacion = json_data['data']['viewer']['marketplace_product_details_page']['target']['creation_time']\n",
    "                    if fecha_publicacion < fecha_flag:\n",
    "                        dato = json_data['data']['viewer']['marketplace_product_details_page'][\"target\"]\n",
    "                        log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                        self._data._append_data(dato, self._tiempo._get_fecha(), enlace)\n",
    "                        log(INFO, f\"Item {i + 1} scrapeado con éxito\")\n",
    "                    break\n",
    "                self.driver.execute_script(\"window.history.go(-1)\");\n",
    "                \n",
    "            except (NoSuchElementException, ElementNotInteractableException, StaleElementReferenceException) as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e=e+1\n",
    "                \n",
    "            except (KeyError, JSONDecodeError) as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e=e+1\n",
    "                self.driver.execute_script(\"window.history.go(-1)\")\n",
    "                \n",
    "            except Exception as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e = e + 1\n",
    "                print(error)\n",
    "                self.guardar_datos(self._errores._get_errores())\n",
    "                _exit(0)\n",
    "            i = i + 1\n",
    "            if i == len(ropa):\n",
    "                self.driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "                sleep(7)\n",
    "                ropa = self.driver.find_elements(By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]')\n",
    "            \n",
    "            del self.driver.requests\n",
    "            log(INFO, \"-------------------------------------------------------------------\")\n",
    "            sleep(3)\n",
    "        self._tiempo._set_errores(e)\n",
    "        log(INFO, f\"Se halló {e} errores\")\n",
    "        log(INFO, \"Fin de la extraccion\")\n",
    "    \n",
    "    def guardar_datos(self, dataset, filetype = \"Data\", folder=\"Data\", filename=\"fb_data\"):\n",
    "        log(INFO, f\"Guardando {filetype}\")\n",
    "        df_fb_mkp_ropa = DataFrame(dataset)\n",
    "        if filetype == \"Data\":\n",
    "            df_fb_mkp_ropa.drop(len(df_fb_mkp_ropa)-1, axis=0, inplace=True)\n",
    "            cantidad = len(df_fb_mkp_ropa)\n",
    "            self._tiempo._set_cantidad(cantidad)\n",
    "        elif filetype == \"Error\":\n",
    "            cantidad = self._tiempo._get_errores()\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        datetime_obj = datetime.strptime(self._tiempo._get_fecha(),\"%d/%m/%Y\")\n",
    "        filepath = folder + \"/\" + datetime_obj.strftime('%d-%m-%Y') + \"/\"\n",
    "        filename = filename + \"_\" + datetime_obj.strftime('%d%m%Y') + \"_\" + str(cantidad) + \".xlsx\"\n",
    "        if not path.exists(filepath):\n",
    "            makedirs(filepath)\n",
    "        df_fb_mkp_ropa.to_excel(filepath + filename, index = False)\n",
    "        log(INFO, f\"{filetype} Guardados Correctamente\")\n",
    "        \n",
    "    def guardar_tiempos(self, filename, sheet_name, start):\n",
    "        log(INFO, \"Guardando tiempos\")\n",
    "        self._tiempo._set_param_final(start)\n",
    "        tiempos = load_workbook(filename)\n",
    "        header_exist = True\n",
    "        if sheet_name not in [ws.title for ws in tiempos.worksheets]:\n",
    "            tiempos.create_sheet(sheet_name)\n",
    "            header_exist = False\n",
    "        worksheet = tiempos[sheet_name]\n",
    "        if not header_exist:\n",
    "            worksheet.append(list(self._tiempo.__dict__.keys()))\n",
    "        worksheet.append(list(self._tiempo.__dict__.values()))\n",
    "        tiempos.save(filename)\n",
    "        tiempos.close()\n",
    "        log(INFO, \"Tiempos Guardados Correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae05c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log():\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    urllibLogger.propagate = False\n",
    "    logger = getLogger('seleniumwire')\n",
    "    logger.setLevel(ERROR)\n",
    "    basicConfig(format='%(asctime)s %(message)s', level=INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Formato para el debugger\n",
    "    config_log()\n",
    "    log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "    \n",
    "    # Cargar variables de entorno\n",
    "    log(INFO, \"Cargando Variables de entorno\")\n",
    "    load_dotenv()\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    # Url base a scrapear\n",
    "    url_base = 'https://www.facebook.com/'\n",
    "    url_ropa = \"https://www.facebook.com/marketplace/category/apparel/?sortBy=creation_time_descend&exact=false\"\n",
    "    \n",
    "    # Parámetros para guardar la data extraída por el scraper\n",
    "    data_filename = \"fb_ropa\"\n",
    "    data_folder = data_type = \"Data\"\n",
    "    \n",
    "    # Parámetros para guardar la medición de la ejecución del scraper\n",
    "    filename_tiempos = 'Tiempos.xlsx'\n",
    "    sheet_tiempos = \"Ropa\"\n",
    "    \n",
    "    # Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "    error_filename = \"fb_error\"\n",
    "    error_folder = error_type = \"Error\"\n",
    "    \n",
    "    scraper = ScraperFb(start)\n",
    "    scraper.iniciar_sesion(url_base)\n",
    "    scraper.mapear_datos(url_ropa)\n",
    "    \n",
    "    # Guardando la data extraída por el scraper\n",
    "    scraper.guardar_datos(scraper._get_data()._get_dataset(), data_type, data_folder, data_filename)\n",
    "    \n",
    "    # Guardando los errores extraídos por el scraper\n",
    "    scraper.guardar_datos(scraper._get_errores()._get_errores(), error_type, error_folder, error_filename)\n",
    "    \n",
    "    # Guardando los tiempos durante la ejecución del scraper\n",
    "    scraper.guardar_tiempos(filename_tiempos, sheet_tiempos, start)\n",
    "    \n",
    "    log(INFO, \"Programa ejecutado satisfactoriamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a430409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 07:03:40,399 Configurando Formato Básico del Debugger\n",
      "2023-01-10 07:03:40,399 Cargando Variables de entorno\n",
      "2023-01-10 07:03:40,414 Inicializando scraper\n",
      "2023-01-10 07:03:40,414 Hora de inicio: 07:03:40\n",
      "2023-01-10 07:03:40,414 ====== WebDriver manager ======\n",
      "2023-01-10 07:03:41,211 Get LATEST chromedriver version for google-chrome 108.0.5359\n",
      "2023-01-10 07:03:42,711 Driver [C:\\Users\\param\\.wdm\\drivers\\chromedriver\\win32\\108.0.5359\\chromedriver.exe] found in cache\n",
      "2023-01-10 07:03:44,145 Iniciando sesión\n",
      "2023-01-10 07:03:49,436 Inicio de sesión con éxito\n",
      "2023-01-10 07:03:59,441 Accediendo a la URL\n",
      "2023-01-10 07:04:22,864 Mapeando Publicaciones\n",
      "2023-01-10 07:04:23,129 Scrapeando item 1\n",
      "2023-01-10 07:04:28,309 Zapatos plataforma\n",
      "2023-01-10 07:04:28,309 Item 1 scrapeado con éxito\n",
      "2023-01-10 07:04:28,399 -------------------------------------------------------------------\n",
      "2023-01-10 07:04:31,409 Scrapeando item 2\n",
      "2023-01-10 07:04:36,611 SPORT NUEVOS BOTINES 100% CUERO DAMAS CX\n",
      "2023-01-10 07:04:36,611 Item 2 scrapeado con éxito\n",
      "2023-01-10 07:04:36,682 -------------------------------------------------------------------\n",
      "2023-01-10 07:04:39,688 Scrapeando item 3\n",
      "2023-01-10 07:04:44,863 Tops básicos\n",
      "2023-01-10 07:04:44,878 Item 3 scrapeado con éxito\n",
      "2023-01-10 07:04:44,927 -------------------------------------------------------------------\n",
      "2023-01-10 07:04:47,932 Scrapeando item 4\n",
      "2023-01-10 07:04:53,076 Chompa Sweater Tommy Hilfiger S\n",
      "2023-01-10 07:04:53,076 Item 4 scrapeado con éxito\n",
      "2023-01-10 07:04:53,124 -------------------------------------------------------------------\n",
      "2023-01-10 07:04:56,136 Scrapeando item 5\n",
      "2023-01-10 07:05:01,334 Emilia121405\n",
      "2023-01-10 07:05:01,334 Item 5 scrapeado con éxito\n",
      "2023-01-10 07:05:01,373 Se halló 0 errores\n",
      "2023-01-10 07:05:01,375 Fin de la extraccion\n",
      "2023-01-10 07:05:01,376 Guardando Data\n",
      "2023-01-10 07:05:01,482 Data Guardados Correctamente\n",
      "2023-01-10 07:05:01,482 Guardando Error\n",
      "2023-01-10 07:05:01,529 Error Guardados Correctamente\n",
      "2023-01-10 07:05:01,529 Guardando tiempos\n",
      "2023-01-10 07:05:01,529 Productos Extraídos: 4\n",
      "2023-01-10 07:05:01,529 Hora Fin: 07:05:01\n",
      "2023-01-10 07:05:01,794 Tiempos Guardados Correctamente\n",
      "2023-01-10 07:05:01,794 Programa ejecutado satisfactoriamente\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f449ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-10 06:54:41,952 Configurando Formato Básico del Debugger\n",
      "2023-01-10 06:54:41,952 Cargando Variables de entorno\n",
      "2023-01-10 06:54:41,952 Inicializando scraper\n",
      "2023-01-10 06:54:41,952 Hora de inicio: 06:54:41\n",
      "2023-01-10 06:54:41,952 ====== WebDriver manager ======\n",
      "2023-01-10 06:54:42,749 Get LATEST chromedriver version for google-chrome 108.0.5359\n",
      "2023-01-10 06:54:44,062 Driver [C:\\Users\\param\\.wdm\\drivers\\chromedriver\\win32\\108.0.5359\\chromedriver.exe] found in cache\n",
      "2023-01-10 06:54:45,587 Iniciando sesión\n",
      "2023-01-10 06:54:51,139 Inicio de sesión con éxito\n"
     ]
    }
   ],
   "source": [
    "config_log()\n",
    "log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "    \n",
    "# Cargar variables de entorno\n",
    "log(INFO, \"Cargando Variables de entorno\")\n",
    "load_dotenv()\n",
    "    \n",
    "start = time()\n",
    "    \n",
    "# Url base a scrapear\n",
    "url_base = 'https://www.facebook.com/'\n",
    "url_ropa = \"https://www.facebook.com/marketplace/category/apparel/?sortBy=creation_time_descend&exact=false\"\n",
    "    \n",
    "# Parámetros para guardar la data extraída por el scraper\n",
    "data_filename = \"fb_ropa\"\n",
    "data_folder = data_type = \"Data\"\n",
    "    \n",
    "# Parámetros para guardar la medición de la ejecución del scraper\n",
    "filename_tiempos = 'Tiempos.xlsx'\n",
    "sheet_tiempos = \"Ropa\"\n",
    "    \n",
    "# Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "error_filename = \"fb_error\"\n",
    "error_folder = error_type = \"Error\"\n",
    "    \n",
    "scraper = ScraperFb(start)\n",
    "scraper.iniciar_sesion(url_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a114fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "scraper.driver.switch_to.window(\"newtab\")\n",
    "scraper.driver.get(url_ropa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90966599",
   "metadata": {},
   "outputs": [],
   "source": [
    "ropa = scraper.driver.find_elements(By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bc2a022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.facebook.com/marketplace/item/2692264587577387'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(\"(.*)\\/\\?\", ropa[1].find_element(By.XPATH, \".//ancestor::a\").get_attribute('href'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b903a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.facebook.com/marketplace/item/1201396634148786'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(\"(.*)\\/\\?\", ropa[2].find_element(By.XPATH, \".//ancestor::a\").get_attribute('href'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "539054b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.facebook.com/marketplace/item/675734347578870'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findall(\"(.*)\\/\\?\", ropa[3].find_element(By.XPATH, \".//ancestor::a\").get_attribute('href'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547891e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
