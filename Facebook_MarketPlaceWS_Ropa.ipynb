{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import basicConfig, CRITICAL, ERROR, getLogger, INFO, log, StreamHandler\n",
    "from os import getenv, makedirs, path\n",
    "from re import findall\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire import webdriver\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    \"\"\"\n",
    "    Representa a los errores ocurridos durante la ejecución de un scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    errores : dict\n",
    "        Conjunto de datos que contiene toda información de los errores ocurridos durante la ejecución del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_error(error, enlace):\n",
    "        Agrega la información de un error al diccionario de datos errores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para el objeto Errores\n",
    "        \"\"\"\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def agregar_error(self, error, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la información de un error al diccionario de datos errores\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        error: Exception\n",
    "            Objeto de tipo excepción ocurrida durante la ejecución del scraper\n",
    "        enlace: str\n",
    "            Enlace de la publicación de la página facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(ERROR, error)\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(traceback_error._str)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Representa al conjunto de datos generado por el scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : dict\n",
    "        Conjunto de datos que contiene toda información extraída de una categoría de la página de facebook marketplace\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_data():\n",
    "        Agrega la información de una publicación al diccionario de datos dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para el objeto Dataset\n",
    "        \"\"\"\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos dataset\"\"\"\n",
    "        return self._dataset\n",
    "\n",
    "    def agregar_data(self, item, fecha_extraccion, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la información de una publicación al dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item: dict\n",
    "            Conjunto de datos que contiene toda la información de una publicación\n",
    "        fecha_extraccion: datetime\n",
    "            Fecha actual en la que se creó una publicación\n",
    "        enlace: str\n",
    "            Enlace de la publicación de la página facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._dataset[\"titulo_marketplace\"].append(\n",
    "            item.get(\"marketplace_listing_title\")\n",
    "        )\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get(\"creation_time\"))\n",
    "        self._dataset[\"disponible\"].append(item.get(\"is_live\"))\n",
    "        self._dataset[\"vendido\"].append(item.get(\"is_sold\"))\n",
    "        self._dataset[\"cantidad\"].append(item.get(\"listing_inventory_type\"))\n",
    "        self._dataset[\"name_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0].get(\"name\")\n",
    "        )\n",
    "        self._dataset[\"tipo_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0][\"__typename\"]\n",
    "        )\n",
    "        self._dataset[\"id_vendedor\"].append(item.get(\"story\").get(\"actors\")[0][\"id\"])\n",
    "        self._dataset[\"locacion_id\"].append(item.get(\"location_vanity_or_id\"))\n",
    "        self._dataset[\"latitud\"].append(item.get(\"location\", {}).get(\"latitude\"))\n",
    "        self._dataset[\"longitud\"].append(item.get(\"location\", {}).get(\"longitude\"))\n",
    "        self._dataset[\"precio\"].append(item.get(\"listing_price\", {}).get(\"amount\"))\n",
    "        self._dataset[\"tipo_moneda\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"currency\")\n",
    "        )\n",
    "        self._dataset[\"amount_with_concurrency\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"amount_with_offset_in_currency\")\n",
    "        )\n",
    "        self._dataset[\"tipo_delivery\"].append(item.get(\"delivery_types\", [None])[0])\n",
    "        self._dataset[\"descripcion\"].append(\n",
    "            item.get(\"redacted_description\", {}).get(\"text\")\n",
    "        )\n",
    "        self._dataset[\"fecha_union_vendedor\"].append(\n",
    "            item.get(\"marketplace_listing_seller\", {}).get(\"join_time\")\n",
    "        )\n",
    "        data = item.get(\"location_text\", {})\n",
    "        if data:\n",
    "            data = data.get(\"text\")\n",
    "        self._dataset[\"locacion\"].append(data)\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    \"\"\"\n",
    "    Representa el tiempo que se demora el scraper en extraer la información\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    start: float\n",
    "        Hora actual en segundos\n",
    "    hora_inicio : str\n",
    "        Hora de inicio de la ejecución del scraper en formato %H:%M:%S\n",
    "    fecha : str\n",
    "        Fecha de las publicaciones a extraer en formato %d/%m/%Y\n",
    "    hora_fin : str\n",
    "        Hora de término de la ejecución del scraper en formato %H:%M:%S\n",
    "    cantidad : int\n",
    "        Cantidad de publicaciones extraídas de la página de facebook marketplace\n",
    "    tiempo : str\n",
    "        Tiempo de ejecución del scraper en formato %d days, %H:%M:%S\n",
    "    productos_por_min : float\n",
    "        Cantidad de publicaciones que puede extraer el scraper en un minuto\n",
    "    num_error : int\n",
    "        Cantidad de errores ocurridos durante la ejecución del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    set_param_final():\n",
    "        Establece los parámetros finales cuando se termina de ejecutar el scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para el objeto Tiempo\n",
    "        \"\"\"\n",
    "        self._start = time()\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(self._start))\n",
    "        log(INFO, f\"Hora de inicio: {self._hora_inicio}\")\n",
    "        self._fecha = (datetime.now().date() - timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = None\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._num_error = None\n",
    "\n",
    "    @property\n",
    "    def cantidad(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo cantidad\"\"\"\n",
    "        return self._cantidad\n",
    "\n",
    "    @property\n",
    "    def fecha(self):\n",
    "        \"\"\"Retorna el valor actual del atributo fecha\"\"\"\n",
    "        return self._fecha\n",
    "\n",
    "    @property\n",
    "    def num_error(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo num_error\"\"\"\n",
    "        return self._num_error\n",
    "\n",
    "    @cantidad.setter\n",
    "    def cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "\n",
    "    @num_error.setter\n",
    "    def num_error(self, num_error):\n",
    "        self._num_error = num_error\n",
    "\n",
    "    def set_param_final(self):\n",
    "        \"\"\"\n",
    "        Establece parametros finales para medir el tiempo de ejecución del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        log(INFO, f\"Productos Extraídos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")\n",
    "        total = end - self._start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = round(self._cantidad / (total / 60), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"\n",
    "    Representa a un bot para hacer web scraping en fb marketplace\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tiempo : Tiempo\n",
    "        Objeto de la clase Tiempo que maneja información del tiempo de ejecución del scraper\n",
    "    driver: webdriver.Chrome\n",
    "        Objeto de la clase webdriver que maneja un navegador para hacer web scraping\n",
    "    wait : WebDriverWait\n",
    "        Objeto de la clase WebDriverWait que maneja el Tiempo de espera durante la ejecución del scraper\n",
    "    errores : Errores\n",
    "        Objeto de la clase Errores que maneja información de los errores ocurridos durante la ejecución del scraper\n",
    "    data : Dataset\n",
    "        Objeto de la clase Dataset que maneja información de las publicaciones extraídas por el scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    iniciar_sesion():\n",
    "        Iniciar sesión en facebook usando un usuario y contraseña\n",
    "    mapear_datos(url):\n",
    "        Mapea y extrae los datos de las publicaciones de una categoría\n",
    "    guardar_datos(dataset, filetype, folder, filename):\n",
    "        Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "    guardar_tiempos(filename, sheet_name):\n",
    "        Guarda la información del tiempo de ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"Representa a un bot para hacer web scarping en fb marketplace.\n",
    "\n",
    "    Attributes:\n",
    "        driver (Object): Maneja un navegador para hacer web scraping\n",
    "        wait (Object): Maneja el Tiempo de espera durante la ejecución del bot\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para el objeto ScraperFb\n",
    "        \"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo()\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        self._driver = webdriver.Chrome(\n",
    "            chrome_options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retorna el valor actual del atributo data\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del atributo errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def iniciar_sesion(self):\n",
    "        \"\"\"\n",
    "        Inicia sesión en una página web usando un usuario y contraseña\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesión\")\n",
    "        self._driver.get(\"https://www.facebook.com/\")\n",
    "        self._driver.maximize_window()\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        username.send_keys(getenv(\"FB_USERNAME\"))\n",
    "        password.send_keys(getenv(\"FB_PASSWORD\"))\n",
    "        self._wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))\n",
    "        ).click()\n",
    "        log(INFO, \"Inicio de sesión con éxito\")\n",
    "\n",
    "    def mapear_datos(self, url):\n",
    "        \"\"\"\n",
    "        Mapea y extrae los datos de las publicaciones de una categoría\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: str\n",
    "            Link de la página de una categoría en facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        sleep(10)\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self._driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self._driver.switch_to.window(\"newtab\")\n",
    "        self._driver.get(url)\n",
    "\n",
    "        sleep(8)\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self._driver.find_elements(\n",
    "            By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "        )\n",
    "        fecha_publicacion = fecha_extraccion = int(\n",
    "            datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\").timestamp()\n",
    "        )\n",
    "        fecha_flag = fecha_extraccion + 86400\n",
    "        i = 0\n",
    "        e = 0\n",
    "        del self._driver.requests\n",
    "\n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "\n",
    "            try:\n",
    "                try:\n",
    "                    enlace = findall(\n",
    "                        \"(.*)\\/\\?\",\n",
    "                        ropa[i]\n",
    "                        .find_element(By.XPATH, \".//ancestor::a\")\n",
    "                        .get_attribute(\"href\"),\n",
    "                    )[0]\n",
    "                except NoSuchElementException as error:\n",
    "                    enlace = None\n",
    "                    self._errores.agregar_error(error, enlace)\n",
    "                ropa[i].click()\n",
    "                sleep(5)\n",
    "                for request in self._driver.requests:\n",
    "                    if not request.response or \"graphql\" not in request.url:\n",
    "                        continue\n",
    "\n",
    "                    body = decode(\n",
    "                        request.response.body,\n",
    "                        request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "                    )\n",
    "                    decoded_body = body.decode(\"utf-8\")\n",
    "                    json_data = loads(decoded_body)\n",
    "\n",
    "                    if \"prefetch_uris_v2\" not in json_data[\"extensions\"]:\n",
    "                        continue\n",
    "\n",
    "                    fecha_publicacion = json_data[\"data\"][\"viewer\"][\n",
    "                        \"marketplace_product_details_page\"\n",
    "                    ][\"target\"][\"creation_time\"]\n",
    "                    if fecha_publicacion < fecha_flag:\n",
    "                        dato = json_data[\"data\"][\"viewer\"][\n",
    "                            \"marketplace_product_details_page\"\n",
    "                        ][\"target\"]\n",
    "                        log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                        self._data.agregar_data(dato, self._tiempo.fecha, enlace)\n",
    "                        log(INFO, f\"Item {i + 1} scrapeado con éxito\")\n",
    "                    break\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except (\n",
    "                NoSuchElementException,\n",
    "                ElementNotInteractableException,\n",
    "                StaleElementReferenceException,\n",
    "            ) as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e = e + 1\n",
    "\n",
    "            except (KeyError, JSONDecodeError) as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e = e + 1\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except Exception as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e = e + 1\n",
    "                log(CRITICAL, \"Se detuvo inesperadamente el programa\")\n",
    "                log(CRITICAL, f\"Causa:\\n{error}\")\n",
    "                break\n",
    "            i = i + 1\n",
    "            if i == len(ropa):\n",
    "                self._driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight)\"\n",
    "                )\n",
    "                sleep(7)\n",
    "                ropa = self._driver.find_elements(\n",
    "                    By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "                )\n",
    "            del self._driver.requests\n",
    "            log(\n",
    "                INFO,\n",
    "                \"-------------------------------------------------------------------\",\n",
    "            )\n",
    "            sleep(3)\n",
    "        self._tiempo.num_error = e\n",
    "        log(INFO, f\"Se halló {e} errores\")\n",
    "        log(INFO, \"Fin de la extraccion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log():\n",
    "    \"\"\"\n",
    "    Función que configura los logs para rastrear al programa\n",
    "         Parameter:\n",
    "                 None\n",
    "\n",
    "        Returns:\n",
    "               None\n",
    "    \"\"\"\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    logger = getLogger(\"seleniumwire\")\n",
    "    logger.setLevel(ERROR)\n",
    "    basicConfig(\n",
    "        format=\"%(asctime)s %(message)s\", level=INFO, handlers=[StreamHandler()]\n",
    "    )\n",
    "\n",
    "\n",
    "def validar_parametros(parametros):\n",
    "    \"\"\"\n",
    "    Función que valida si los parámetros a usar están definidos\n",
    "         Parameter:\n",
    "                 parametros (list): Lista de parámetros\n",
    "\n",
    "        Returns:\n",
    "               None\n",
    "    \"\"\"\n",
    "    for parametro in parametros:\n",
    "        if not parametro:\n",
    "            log(ERROR, \"Parámetros incorrectos\")\n",
    "            return False\n",
    "    log(INFO, \"Parámetros válidos\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Formato para el debugger\n",
    "    config_log()\n",
    "    log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "\n",
    "    # Cargar variables de entorno\n",
    "    log(INFO, \"Cargando Variables de entorno\")\n",
    "    load_dotenv()\n",
    "\n",
    "    # Url de la categoría a scrapear\n",
    "    url_ropa = getenv(\"URL_CATEGORY\")\n",
    "\n",
    "    # Parámetros para guardar la data extraída por el scraper\n",
    "    data_filename = getenv(\"DATA_FILENAME\")\n",
    "    data_folder = getenv(\"DATA_FOLDER\")\n",
    "\n",
    "    # Parámetros para guardar la medición de la ejecución del scraper\n",
    "    filename_tiempos = getenv(\"FILENAME_TIEMPOS\")\n",
    "    sheet_tiempos = getenv(\"SHEET_TIEMPOS\")\n",
    "\n",
    "    # Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "    error_filename = getenv(\"ERROR_FILENAME\")\n",
    "    error_folder = getenv(\"ERROR_FOLDER\")\n",
    "\n",
    "    # Validar parámetros\n",
    "    if not validar_parametros(\n",
    "        [\n",
    "            url_ropa,\n",
    "            data_filename,\n",
    "            data_folder,\n",
    "            filename_tiempos,\n",
    "            sheet_tiempos,\n",
    "            error_filename,\n",
    "            error_folder,\n",
    "        ]\n",
    "    ):\n",
    "        return\n",
    "\n",
    "    # Inicializar scrapper\n",
    "    scraper = ScraperFb()\n",
    "\n",
    "    # Iniciar sesión\n",
    "    scraper.iniciar_sesion()\n",
    "\n",
    "    # Extracción de datos\n",
    "    scraper.mapear_datos(url_ropa)\n",
    "\n",
    "    # Guardando la data extraída por el scraper\n",
    "    scraper.guardar_datos(scraper.data.dataset, \"Data\", data_folder, data_filename)\n",
    "\n",
    "    # Guardando los errores extraídos por el scraper\n",
    "    scraper.guardar_datos(\n",
    "        scraper.errores.errores, \"Error\", error_folder, error_filename\n",
    "    )\n",
    "\n",
    "    # Guardando los tiempos durante la ejecución del scraper\n",
    "    scraper.guardar_tiempos(filename_tiempos, sheet_tiempos)\n",
    "    log(INFO, \"Programa ejecutado satisfactoriamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a430409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 14:25:01,089 Configurando Formato Básico del Debugger\n",
      "2023-01-11 14:25:01,089 Cargando Variables de entorno\n",
      "2023-01-11 14:25:01,089 Parámetros válidos\n",
      "2023-01-11 14:25:01,089 Inicializando scraper\n",
      "2023-01-11 14:25:01,089 Hora de inicio: 14:25:01\n",
      "2023-01-11 14:25:01,089 ====== WebDriver manager ======\n",
      "2023-01-11 14:25:01,886 Get LATEST chromedriver version for google-chrome 108.0.5359\n",
      "2023-01-11 14:25:03,199 Driver [C:\\Users\\param\\.wdm\\drivers\\chromedriver\\win32\\108.0.5359\\chromedriver.exe] found in cache\n",
      "2023-01-11 14:25:04,893 Iniciando sesión\n",
      "2023-01-11 14:25:10,396 Inicio de sesión con éxito\n",
      "2023-01-11 14:25:20,397 Accediendo a la URL\n",
      "2023-01-11 14:25:40,187 Mapeando Publicaciones\n",
      "2023-01-11 14:25:40,453 Scrapeando item 1\n",
      "2023-01-11 14:25:45,635 Vestido Catania con lazo/ S y M\n",
      "2023-01-11 14:25:45,635 Item 1 scrapeado con éxito\n",
      "2023-01-11 14:25:45,725 -------------------------------------------------------------------\n",
      "2023-01-11 14:25:48,732 Scrapeando item 2\n",
      "2023-01-11 14:25:53,919 Camisa de hombre\n",
      "2023-01-11 14:25:53,919 Item 2 scrapeado con éxito\n",
      "2023-01-11 14:25:53,967 -------------------------------------------------------------------\n",
      "2023-01-11 14:25:56,971 Scrapeando item 3\n",
      "2023-01-11 14:26:02,170 Zapatillas para niña\n",
      "2023-01-11 14:26:02,170 Item 3 scrapeado con éxito\n",
      "2023-01-11 14:26:02,199 Se halló 0 errores\n",
      "2023-01-11 14:26:02,200 Fin de la extraccion\n",
      "2023-01-11 14:26:02,201 Guardando Data\n",
      "2023-01-11 14:26:02,246 Data Guardados Correctamente\n",
      "2023-01-11 14:26:02,246 Guardando Error\n",
      "2023-01-11 14:26:02,262 El archivo de tipo Error no se va a guardar por no tener información\n",
      "2023-01-11 14:26:02,262 Guardando tiempos\n",
      "2023-01-11 14:26:02,262 Productos Extraídos: 2\n",
      "2023-01-11 14:26:02,262 Hora Fin: 14:26:02\n",
      "2023-01-11 14:26:02,387 Tiempos Guardados Correctamente\n",
      "2023-01-11 14:26:02,387 Programa ejecutado satisfactoriamente\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
