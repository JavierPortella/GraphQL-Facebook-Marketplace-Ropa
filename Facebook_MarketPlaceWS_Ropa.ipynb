{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import (\n",
    "    basicConfig,\n",
    "    CRITICAL,\n",
    "    ERROR,\n",
    "    FileHandler,\n",
    "    getLogger,\n",
    "    INFO,\n",
    "    log,\n",
    "    shutdown,\n",
    "    StreamHandler\n",
    ")\n",
    "from os import getenv, makedirs, path\n",
    "from re import findall\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire import webdriver\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    \"\"\"\n",
    "    Representa a los errores ocurridos durante la ejecución de un scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    errores : dict\n",
    "        Conjunto de datos que contiene toda información de los errores ocurridos durante la ejecución del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_error(error, enlace):\n",
    "        Agrega la información de un error al diccionario de datos errores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Errores\n",
    "        \"\"\"\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def agregar_error(self, error, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la información de un error al diccionario de datos errores\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        error: Exception\n",
    "            Error ocurrido durante la ejecución del scraper\n",
    "        enlace: str\n",
    "            Enlace de la publicación de la página facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(ERROR, f\"Error:\\n{error}\")\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(traceback_error._str)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Representa al conjunto de datos generado por el scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    dataset : dict\n",
    "        Conjunto de datos que contiene toda información extraída de una categoría de la página de facebook marketplace\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    agregar_data():\n",
    "        Agrega la información de una publicación al diccionario de datos dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Dataset\n",
    "        \"\"\"\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": [],\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        \"\"\"Retorna el valor actual del diccionario de datos dataset\"\"\"\n",
    "        return self._dataset\n",
    "\n",
    "    def agregar_data(self, item, fecha_extraccion, enlace):\n",
    "        \"\"\"\n",
    "        Agrega la información de una publicación al dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        item: dict\n",
    "            Conjunto de datos que contiene toda la información de una publicación\n",
    "        fecha_extraccion: str\n",
    "            Fecha correspondiente a la extracción de todas las publicaciones\n",
    "        enlace: str\n",
    "            Enlace de la publicación\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self._dataset[\"titulo_marketplace\"].append(\n",
    "            item.get(\"marketplace_listing_title\")\n",
    "        )\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get(\"creation_time\"))\n",
    "        self._dataset[\"disponible\"].append(item.get(\"is_live\"))\n",
    "        self._dataset[\"vendido\"].append(item.get(\"is_sold\"))\n",
    "        self._dataset[\"cantidad\"].append(item.get(\"listing_inventory_type\"))\n",
    "        self._dataset[\"name_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0].get(\"name\")\n",
    "        )\n",
    "        self._dataset[\"tipo_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0][\"__typename\"]\n",
    "        )\n",
    "        self._dataset[\"id_vendedor\"].append(item.get(\"story\").get(\"actors\")[0][\"id\"])\n",
    "        self._dataset[\"locacion_id\"].append(item.get(\"location_vanity_or_id\"))\n",
    "        self._dataset[\"latitud\"].append(item.get(\"location\", {}).get(\"latitude\"))\n",
    "        self._dataset[\"longitud\"].append(item.get(\"location\", {}).get(\"longitude\"))\n",
    "        self._dataset[\"precio\"].append(item.get(\"listing_price\", {}).get(\"amount\"))\n",
    "        self._dataset[\"tipo_moneda\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"currency\")\n",
    "        )\n",
    "        self._dataset[\"amount_with_concurrency\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"amount_with_offset_in_currency\")\n",
    "        )\n",
    "        self._dataset[\"tipo_delivery\"].append(item.get(\"delivery_types\", [None])[0])\n",
    "        self._dataset[\"descripcion\"].append(\n",
    "            item.get(\"redacted_description\", {}).get(\"text\")\n",
    "        )\n",
    "        self._dataset[\"fecha_union_vendedor\"].append(\n",
    "            item.get(\"marketplace_listing_seller\", {}).get(\"join_time\")\n",
    "        )\n",
    "        data = item.get(\"location_text\", {})\n",
    "        if data:\n",
    "            data = data.get(\"text\")\n",
    "        self._dataset[\"locacion\"].append(data)\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    \"\"\"\n",
    "    Representa al tiempo de ejecución del scraper\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    start : float\n",
    "        Hora actual en segundos\n",
    "    hora_inicio : str\n",
    "        Hora de inicio de la ejecución del scraper en formato %H:%M:%S\n",
    "    fecha : str\n",
    "        Fecha de las publicaciones a extraer en formato %d/%m/%Y\n",
    "    hora_fin : str\n",
    "        Hora de término de la ejecución del scraper en formato %H:%M:%S\n",
    "    cantidad : int\n",
    "        Cantidad de publicaciones extraídas de la página de facebook marketplace\n",
    "    cantidad_real: int\n",
    "        Cantidad de publicaciones analizadas de la página de facebook marketplace\n",
    "    tiempo : str\n",
    "        Tiempo de ejecución del scraper en formato %d days, %H:%M:%S\n",
    "    productos_por_min : float\n",
    "        Cantidad de publicaciones que puede extraer el scraper en un minuto\n",
    "    productos_por_min_real : float\n",
    "        Cantidad publicaciones que puede analizar el scraper en un minuto\n",
    "    num_error : int\n",
    "        Cantidad de errores ocurridos durante la ejecución del scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    set_param_final():\n",
    "        Establece los parámetros finales cuando se termina de ejecutar el scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fecha_actual):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase Tiempo\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fecha_actual: str\n",
    "            Fecha en la que se ejecuta el scraper\n",
    "        \"\"\"\n",
    "        self._start = time()\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(self._start))\n",
    "        log(INFO, f\"Hora de inicio: {self._hora_inicio}\")\n",
    "        self._fecha = fecha_actual.strftime(\"%d/%m/%Y\")\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = None\n",
    "        self._cantidad_real = None\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._productos_por_min_real = None\n",
    "        self._num_error = None\n",
    "\n",
    "    @property\n",
    "    def cantidad(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo cantidad\"\"\"\n",
    "        return self._cantidad\n",
    "\n",
    "    @property\n",
    "    def cantidad_real(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo cantidad_real\"\"\"\n",
    "        return self._cantidad_real\n",
    "\n",
    "    @property\n",
    "    def fecha(self):\n",
    "        \"\"\"Retorna el valor actual del atributo fecha\"\"\"\n",
    "        return self._fecha\n",
    "\n",
    "    @property\n",
    "    def num_error(self):\n",
    "        \"\"\"Retorna el valor actual o asigna un nuevo valor del atributo num_error\"\"\"\n",
    "        return self._num_error\n",
    "\n",
    "    @cantidad.setter\n",
    "    def cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "\n",
    "    @cantidad_real.setter\n",
    "    def cantidad_real(self, cantidad_real):\n",
    "        self._cantidad_real = cantidad_real\n",
    "\n",
    "    @num_error.setter\n",
    "    def num_error(self, num_error):\n",
    "        self._num_error = num_error\n",
    "\n",
    "    def set_param_final(self):\n",
    "        \"\"\"\n",
    "        Establece parametros finales para medir el tiempo de ejecución del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        log(INFO, f\"Productos Extraídos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")\n",
    "        total = end - self._start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = round(self._cantidad / (total / 60), 2)\n",
    "        self._productos_por_min_real = round(self._cantidad_real / (total / 60), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"\n",
    "    Representa a un bot para hacer web scraping en fb marketplace\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tiempo : Tiempo\n",
    "        Objeto de la clase Tiempo que maneja información del tiempo de ejecución del scraper\n",
    "    driver: webdriver.Chrome\n",
    "        Objeto de la clase webdriver que maneja un navegador para hacer web scraping\n",
    "    wait : WebDriverWait\n",
    "        Objeto de la clase WebDriverWait que maneja el Tiempo de espera durante la ejecución del scraper\n",
    "    errores : Errores\n",
    "        Objeto de la clase Errores que maneja información de los errores ocurridos durante la ejecución del scraper\n",
    "    data : Dataset\n",
    "        Objeto de la clase Dataset que maneja información de las publicaciones extraídas por el scraper\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    iniciar_sesion():\n",
    "        Inicia sesión en la página web de facebook usando un usuario y contraseña\n",
    "    obtener_publicaciones(selector, xpath):\n",
    "        Retorna una lista de publicaciones visibles en facebook marketplace\n",
    "    mapear_datos(url):\n",
    "        Mapea y extrae los datos de las publicaciones de una categoría\n",
    "    guardar_datos(filetype, folder, filename):\n",
    "        Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "    guardar_tiempos(filename, sheet_name):\n",
    "        Guarda la información del tiempo de ejecución del scraper\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fecha_actual):\n",
    "        \"\"\"\n",
    "        Genera todos los atributos para una instancia de la clase ScraperFb\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fecha_actual: str\n",
    "            Fecha en la que se ejecuta el scraper\n",
    "        \"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo(fecha_actual)\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        self._driver = webdriver.Chrome(\n",
    "            chrome_options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self._wait = WebDriverWait(self._driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        \"\"\"Retorna el valor actual del atributo data\"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def errores(self):\n",
    "        \"\"\"Retorna el valor actual del atributo errores\"\"\"\n",
    "        return self._errores\n",
    "\n",
    "    def iniciar_sesion(self):\n",
    "        \"\"\"\n",
    "        Inicia sesión en la página web de facebook usando un usuario y contraseña\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesión\")\n",
    "        # Ingresando al página de facebook\n",
    "        self._driver.get(\"https://www.facebook.com/\")\n",
    "        # Maximizando el explorador\n",
    "        self._driver.maximize_window()\n",
    "        # Localizando los campos de usuario y contraseña\n",
    "        username = self._wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self._wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        # Limpiando el contenido que existe en los campos de usuario y contraseña\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        # Mandando valores a los campos de usuario y contraseña\n",
    "        username.send_keys(getenv(\"FB_USERNAME\"))\n",
    "        password.send_keys(getenv(\"FB_PASSWORD\"))\n",
    "        # Dar click en el botón de iniciar sesión\n",
    "        self._wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))\n",
    "        ).click()\n",
    "        sleep(10)\n",
    "        log(INFO, \"Inicio de sesión con éxito\")\n",
    "\n",
    "    def obtener_publicaciones(self, selector, xpath):\n",
    "        \"\"\"\n",
    "        Retornar una lista de publicaciones visibles con respecto a una categoría en facebook marketplace\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        selector: str\n",
    "            Selector a ser usado para localizar las publicaciones\n",
    "        xpath: str\n",
    "            Ruta de las publicaciones a ser usado por el selector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "        \"\"\"\n",
    "        return self._driver.find_elements(selector, xpath)\n",
    "\n",
    "    def mapear_datos(self, url):\n",
    "        \"\"\"\n",
    "        Mapea y extrae los datos de las publicaciones de una categoría\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        url: str\n",
    "            Link de la página de una categoría en facebook marketplace\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self._driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self._driver.switch_to.window(\"newtab\")\n",
    "        self._driver.get(url)\n",
    "        sleep(8)\n",
    "\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self.obtener_publicaciones(\n",
    "            By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "        )\n",
    "\n",
    "        log(INFO, \"Creando variables\")\n",
    "        # Enteros que hacen referencia a la fecha en que se postea una publicación y en la que se extrae la información\n",
    "        fecha_publicacion = fecha_extraccion = int(\n",
    "            datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\").timestamp()\n",
    "        )\n",
    "        # Entero que hace referencia al día siguiente de la fecha en la que se extrae la información\n",
    "        fecha_flag = fecha_extraccion + 86400\n",
    "        # Cuenta la cantidad de publicaciones que mapea el scraper\n",
    "        i = 0\n",
    "        # Cuenta la cantidad de errores ocurridos durante la ejecución del mapeo del scraper\n",
    "        e = 0\n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            try:\n",
    "                log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "                # Eliminar de la memoria requests innecesarios\n",
    "                del self._driver.requests\n",
    "                # Link de la publicación de facebook\n",
    "                enlace = findall(\n",
    "                    \"(.*)\\/\\?\",\n",
    "                    ropa[i]\n",
    "                    .find_element(By.XPATH, \".//ancestor::a\")\n",
    "                    .get_attribute(\"href\"),\n",
    "                )[0]\n",
    "                # Dar click a la publicación de facebook\n",
    "                ropa[i].click()\n",
    "                sleep(5)\n",
    "\n",
    "                for request in self._driver.requests:\n",
    "                    # Validar si la api es de graphql\n",
    "                    if not request.response or \"graphql\" not in request.url:\n",
    "                        continue\n",
    "                    # Obtener la respuesta de la api en bytes\n",
    "                    body = decode(\n",
    "                        request.response.body,\n",
    "                        request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "                    )\n",
    "                    # Decodificar la respuesta a utf-8\n",
    "                    decoded_body = body.decode(\"utf-8\")\n",
    "\n",
    "                    # Validar si la respuesta decodificada es la deseada\n",
    "                    if decoded_body.find('\"extensions\":{\"prefetch_uris_v2\"') == -1:\n",
    "                        continue\n",
    "\n",
    "                    # Convertir al formato json la respuesta decodificada anteriormente\n",
    "                    json_data = loads(decoded_body)\n",
    "                    # Extraer la fecha de publicación\n",
    "                    fecha_publicacion = json_data[\"data\"][\"viewer\"][\n",
    "                        \"marketplace_product_details_page\"\n",
    "                    ][\"target\"][\"creation_time\"]\n",
    "\n",
    "                    # Validar si la fecha de publicación corresponda a la deseada\n",
    "                    if fecha_publicacion < fecha_flag:\n",
    "                        # Diccionario que contiene toda la información de la publicación\n",
    "                        dato = json_data[\"data\"][\"viewer\"][\n",
    "                            \"marketplace_product_details_page\"\n",
    "                        ][\"target\"]\n",
    "                        log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                        self._data.agregar_data(dato, self._tiempo.fecha, enlace)\n",
    "                        log(INFO, f\"Item {i + 1} scrapeado con éxito\")\n",
    "\n",
    "                    break\n",
    "                # Regresar al inicio donde se encuentran todas las publicaciones de facebook\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except (\n",
    "                NoSuchElementException,\n",
    "                ElementNotInteractableException,\n",
    "                StaleElementReferenceException,\n",
    "            ) as error:\n",
    "                enlace = None\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "\n",
    "            except (KeyError, JSONDecodeError) as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "                self._driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except Exception as error:\n",
    "                self._errores.agregar_error(error, enlace)\n",
    "                e += 1\n",
    "                i += 1\n",
    "                log(CRITICAL, \"Se detuvo inesperadamente el programa\")\n",
    "                log(CRITICAL, f\"Causa:\\n{error}\")\n",
    "                break\n",
    "\n",
    "            finally:\n",
    "                i += 1\n",
    "\n",
    "                # Verificar si se ha mapeado todas las publicaciones visibles\n",
    "                if i == len(ropa):\n",
    "                    # Hacer uso del scroll para obtener más publicaciones\n",
    "                    self._driver.execute_script(\n",
    "                        \"window.scrollTo(0, document.body.scrollHeight)\"\n",
    "                    )\n",
    "                    sleep(6)\n",
    "                    # Mapear las nuevas publicaciones\n",
    "                    ropa = self.obtener_publicaciones(\n",
    "                        By.XPATH,\n",
    "                        '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]',\n",
    "                    )\n",
    "                sleep(2)\n",
    "                log(\n",
    "                    INFO,\n",
    "                    \"-------------------------------------------------------------------\",\n",
    "                )\n",
    "\n",
    "        del self._driver.requests\n",
    "        # Guardar algunos datos del tiempo de ejecución del scraper\n",
    "        self._tiempo.cantidad_real = i - e\n",
    "        self._tiempo.num_error = e\n",
    "        log(INFO, f\"Se halló {e} errores\")\n",
    "        log(INFO, \"Fin de la extraccion\")\n",
    "\n",
    "    def guardar_datos(\n",
    "        self,\n",
    "        filetype=\"Data\",\n",
    "        folder=\"Data//datos_obtenidos\",\n",
    "        filename=\"fb_data\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Guarda los datos o errores obtenidos durante la ejecución del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filetype: str\n",
    "            Indica si la información son datos de las publicaciones o errores. Se acepta Data y Error\n",
    "        folder: str\n",
    "            Ruta del archivo\n",
    "        filename: str\n",
    "            Nombre del archivo\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, f\"Guardando {filetype}\")\n",
    "        # Comprobando si el valor ingresado para la variable filetype es correcto\n",
    "        if filetype == \"Data\":\n",
    "            # Registrando toda la información de las publicaciones extraídas por el scraper\n",
    "            dataset = self._data.dataset\n",
    "        elif filetype == \"Error\":\n",
    "            # Registrando toda la información de los errores ocurridos durante la ejecución del scraper\n",
    "            dataset = self._errores.errores\n",
    "        else:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no está admitido. Solo se aceptan los valores Data y Error\",\n",
    "            )\n",
    "            log(\n",
    "                ERROR,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no ser de tipo Data o Error\",\n",
    "            )\n",
    "            return\n",
    "        # Crear un dataframe\n",
    "        df_fb_mkp_ropa = DataFrame(dataset)\n",
    "\n",
    "        # Comprobando que el dataset contenga información\n",
    "        if len(df_fb_mkp_ropa) == 0:\n",
    "            log(\n",
    "                INFO,\n",
    "                f\"El archivo de tipo {filetype} no se va a guardar por no tener información\",\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Ejecutando diferentes acciones de acuerdo al tipo de información que se va a guardar\n",
    "        if filetype == \"Data\":\n",
    "            # Eliminando la última publicación, porque su fecha de creación es de otro día\n",
    "            df_fb_mkp_ropa.drop(len(df_fb_mkp_ropa) - 1, axis=0, inplace=True)\n",
    "            # Registrando la cantidad de información que contiene el dataset\n",
    "            cantidad = len(df_fb_mkp_ropa)\n",
    "            self._tiempo.cantidad = cantidad\n",
    "        else:\n",
    "            # Registrando la cantidad de errores ocurridos durante la ejecución del scraper\n",
    "            cantidad = self._tiempo.num_error\n",
    "\n",
    "        datetime_obj = datetime.strptime(self._tiempo.fecha, \"%d/%m/%Y\")\n",
    "        # Generando la ruta donde se va a guardar la información\n",
    "        filepath = path.join(folder, datetime_obj.strftime(\"%d-%m-%Y\"))\n",
    "        # Generando el nombre del archivo que va a contener la información\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + datetime_obj.strftime(\"%d%m%Y\")\n",
    "            + \"_\"\n",
    "            + str(cantidad)\n",
    "            + \".xlsx\"\n",
    "        )\n",
    "        # Verificando si la ruta donde se va a guardar la información existe\n",
    "        if not path.exists(filepath):\n",
    "            # Creando la ruta donde se va a guardar la información\n",
    "            makedirs(filepath)\n",
    "        # Guardando la información en un archivo de tipo excel\n",
    "        df_fb_mkp_ropa.to_excel(path.join(filepath, filename), index=False)\n",
    "        log(INFO, f\"{filetype} Guardados Correctamente\")\n",
    "\n",
    "    def guardar_tiempos(self, filename, sheet_name):\n",
    "        \"\"\"\n",
    "        Guarda la información del tiempo de ejecución del scraper\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename: str\n",
    "            Nombre del archivo\n",
    "        sheet_name: str\n",
    "            Nombre de la hoja de cálculo\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        log(INFO, \"Guardando tiempos\")\n",
    "        # Guardando los parametros finales del tiempo de ejecución del scraper\n",
    "        self._tiempo.set_param_final()\n",
    "        # Variable que indica si el encabezados existe o no en el archivo de excel\n",
    "        header_exist = True\n",
    "        # Verificando si el archivo existe o no\n",
    "        if path.isfile(filename):\n",
    "            # Leendo el archivo\n",
    "            tiempos = load_workbook(filename)\n",
    "        else:\n",
    "            # Creando un archivo de tipo workbook\n",
    "            tiempos = Workbook()\n",
    "\n",
    "        # Comprobando si ya existe un sheet con el nombre indicado en la variable sheet_name\n",
    "        if sheet_name not in [ws.title for ws in tiempos.worksheets]:\n",
    "            # Creando un nuevo sheet\n",
    "            tiempos.create_sheet(sheet_name)\n",
    "            # Especificar que no existen encabezados en el nuevo sheet\n",
    "            header_exist = False\n",
    "        # Seleccionar el sheet deseado donde se va a guardar la información\n",
    "        worksheet = tiempos[sheet_name]\n",
    "\n",
    "        # Comprobando si el encabezados existe o no\n",
    "        if not header_exist:\n",
    "            # Reordenar la lista que contiene los encabezados a ser insertados\n",
    "            keys = cambiar_posiciones(list(self._tiempo.__dict__.keys())[1:], 0, 1)\n",
    "            # Insertando los encabezados al sheet\n",
    "            worksheet.append(keys)\n",
    "        # Reordenar la lista que contiene los valores a ser insertados\n",
    "        values = cambiar_posiciones(list(self._tiempo.__dict__.values())[1:], 0, 1)\n",
    "        # Insertando la información del tiempo al sheet\n",
    "        worksheet.append(values)\n",
    "        # Guardar la información en un archivo excel\n",
    "        tiempos.save(filename)\n",
    "        # Cerrar el archivo excel\n",
    "        tiempos.close()\n",
    "        log(INFO, \"Tiempos Guardados Correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "767432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log(\n",
    "    log_folder, log_filename, log_file_mode, log_file_encoding, fecha_actual\n",
    "):\n",
    "    \"\"\"\n",
    "    Función que configura los logs para rastrear al programa\n",
    "        Parameter:\n",
    "                log_folder (str): Carpeta donde se va a generar el archivo log\n",
    "                log_filename (str): Nombre del archivo log a ser generado\n",
    "                log_file_mode (str): Modo de guardado del archivo\n",
    "                log_file_encoding (str): Codificación usada para el archivo\n",
    "                fecha_actual (datetime): Fecha actual de la creación del archivo log\n",
    "        Returns:\n",
    "                None\n",
    "    \"\"\"\n",
    "    # Mostrar solo los errores de los registros que maneja selenium\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja urllib\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    # Mostrar solo los errores de los registros que maneja seleniumwire\n",
    "    logger = getLogger(\"seleniumwire\")\n",
    "    logger.setLevel(ERROR)\n",
    "    # Generando la ruta donde se va a guardar los registros de ejecución\n",
    "    log_path = path.join(log_folder, fecha_actual.strftime(\"%d-%m-%Y\"))\n",
    "    # Generando el nombre del archivo que va a contener los registros de ejecución\n",
    "    log_filename = log_filename + \"_\" + fecha_actual.strftime(\"%d%m%Y\") + \".log\"\n",
    "    # Verificando si la ruta donde se va a guardar los registros de ejecución existe\n",
    "    if not path.exists(log_path):\n",
    "        # Creando la ruta donde se va a guardar los registros de ejecución\n",
    "        makedirs(log_path)\n",
    "    # Configuración básica de los logs que maneja este programa\n",
    "    basicConfig(\n",
    "        format=\"%(asctime)s %(message)s\",\n",
    "        level=INFO,\n",
    "        handlers=[\n",
    "            StreamHandler(),\n",
    "            FileHandler(\n",
    "                path.join(log_path, log_filename), log_file_mode, log_file_encoding\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def validar_parametros(parametros):\n",
    "    \"\"\"\n",
    "    Función que valida si los parámetros a usar están definidos\n",
    "         Parameter:\n",
    "                 parametros (list): Lista de parámetros\n",
    "\n",
    "        Returns:\n",
    "               None\n",
    "    \"\"\"\n",
    "    for parametro in parametros:\n",
    "        # Verifica que el parámetro haya sido definido\n",
    "        if not parametro:\n",
    "            log(ERROR, \"Parámetros incorrectos\")\n",
    "            # Retorna false si algunos de los parámetros no fue definido\n",
    "            return False\n",
    "    log(INFO, \"Parámetros válidos\")\n",
    "    # Retorna verdadero si todos los parámetros fueron definidos\n",
    "    return True\n",
    "\n",
    "\n",
    "def cambiar_posiciones(lista, index1, index2):\n",
    "    \"\"\"\n",
    "    Función que intercambia las posiciones de 2 elementos de un arreglo\n",
    "         Parameter:\n",
    "                 lista (list): Lista no vacía de elementos\n",
    "                 index1 (int): Posición del primer elemento\n",
    "                 index2 (int): Posición del segundo elemento\n",
    "\n",
    "        Returns:\n",
    "               list\n",
    "    \"\"\"\n",
    "    # Comprobar si la lista contiene valores\n",
    "    if len(lista) > 0:\n",
    "        # Intercambio de posiciones\n",
    "        aux = lista[index2]\n",
    "        lista[index2] = lista[index1]\n",
    "        lista[index1] = aux\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Formato para el debugger\n",
    "        fecha_actual = datetime.now().date() - timedelta(days=1)\n",
    "        config_log(\"Log\", \"fb_ropa_log\", \"w\", \"utf-8\", fecha_actual)\n",
    "        log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "\n",
    "        # Cargar variables de entorno\n",
    "        log(INFO, \"Cargando Variables de entorno\")\n",
    "        load_dotenv()\n",
    "\n",
    "        # Url de la categoría a scrapear\n",
    "        url_ropa = getenv(\"URL_CATEGORY\")\n",
    "\n",
    "        # Parámetros para guardar la data extraída por el scraper\n",
    "        data_filename = getenv(\"DATA_FILENAME\")\n",
    "        data_folder = getenv(\"DATA_FOLDER\")\n",
    "\n",
    "        # Parámetros para guardar la medición de la ejecución del scraper\n",
    "        filename_tiempos = getenv(\"FILENAME_TIEMPOS\")\n",
    "        sheet_tiempos = getenv(\"SHEET_TIEMPOS\")\n",
    "\n",
    "        # Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "        error_filename = getenv(\"ERROR_FILENAME\")\n",
    "        error_folder = getenv(\"ERROR_FOLDER\")\n",
    "\n",
    "        # Validar parámetros\n",
    "        if not validar_parametros(\n",
    "            [\n",
    "                url_ropa,\n",
    "                data_filename,\n",
    "                data_folder,\n",
    "                filename_tiempos,\n",
    "                sheet_tiempos,\n",
    "                error_filename,\n",
    "                error_folder,\n",
    "            ]\n",
    "        ):\n",
    "            return\n",
    "\n",
    "        # Inicializar scrapper\n",
    "        scraper = ScraperFb(fecha_actual)\n",
    "\n",
    "        # Iniciar sesión\n",
    "        scraper.iniciar_sesion()\n",
    "\n",
    "        # Extracción de datos\n",
    "        scraper.mapear_datos(url_ropa)\n",
    "\n",
    "        # Guardando la data extraída por el scraper\n",
    "        scraper.guardar_datos(\"Data\", data_folder, data_filename)\n",
    "\n",
    "        # Guardando los errores extraídos por el scraper\n",
    "        scraper.guardar_datos(\"Error\", error_folder, error_filename)\n",
    "\n",
    "        # Guardando los tiempos durante la ejecución del scraper\n",
    "        scraper.guardar_tiempos(filename_tiempos, sheet_tiempos)\n",
    "        log(INFO, \"Programa finalizado\")\n",
    "\n",
    "    except Exception as error:\n",
    "        log(ERROR, f\"Error: {error}\")\n",
    "        log(INFO, \"Programa ejecutado con fallos\")\n",
    "\n",
    "    finally:\n",
    "        # Liberar el archivo log\n",
    "        shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a430409",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
