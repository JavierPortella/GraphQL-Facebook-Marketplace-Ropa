{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9825e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from json import loads, JSONDecodeError\n",
    "from logging import basicConfig, CRITICAL, ERROR, getLogger, INFO, log\n",
    "from os import getenv, makedirs, path\n",
    "from re import findall\n",
    "from time import localtime, sleep, strftime, time\n",
    "from traceback import TracebackException\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook, Workbook\n",
    "from pandas import DataFrame\n",
    "from seleniumwire import webdriver\n",
    "from seleniumwire.utils import decode\n",
    "from selenium.common.exceptions import (\n",
    "    NoSuchElementException,\n",
    "    StaleElementReferenceException,\n",
    "    ElementNotInteractableException,\n",
    ")\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER as seleniumLogger\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from urllib3.connectionpool import log as urllibLogger\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9318c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Errores:\n",
    "    def __init__(self):\n",
    "        self._errores = {\n",
    "            \"Clase\": [],\n",
    "            \"Mensaje\": [],\n",
    "            \"Linea de Error\": [],\n",
    "            \"Codigo Error\": [],\n",
    "            \"Publicacion\": [],\n",
    "        }\n",
    "\n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "\n",
    "    def _append_error(self, error, enlace):\n",
    "        traceback_error = TracebackException.from_exception(error)\n",
    "        error_message = traceback_error._str\n",
    "        error_stack = traceback_error.stack[0]\n",
    "        log(ERROR, error_message)\n",
    "        self._errores[\"Clase\"].append(traceback_error.exc_type)\n",
    "        self._errores[\"Mensaje\"].append(error_message)\n",
    "        self._errores[\"Linea de Error\"].append(error_stack.lineno)\n",
    "        self._errores[\"Codigo Error\"].append(error_stack.line)\n",
    "        self._errores[\"Publicacion\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self):\n",
    "        self._dataset = {\n",
    "            \"Fecha Extraccion\": [],\n",
    "            \"titulo_marketplace\": [],\n",
    "            \"tiempo_creacion\": [],\n",
    "            \"tipo_delivery\": [],\n",
    "            \"descripcion\": [],\n",
    "            \"disponible\": [],\n",
    "            \"vendido\": [],\n",
    "            \"fecha_union_vendedor\": [],\n",
    "            \"cantidad\": [],\n",
    "            \"precio\": [],\n",
    "            \"tipo_moneda\": [],\n",
    "            \"amount_with_concurrency\": [],\n",
    "            \"latitud\": [],\n",
    "            \"longitud\": [],\n",
    "            \"locacion\": [],\n",
    "            \"locacion_id\": [],\n",
    "            \"name_vendedor\": [],\n",
    "            \"tipo_vendedor\": [],\n",
    "            \"id_vendedor\": [],\n",
    "            \"enlace\": [],\n",
    "        }\n",
    "\n",
    "    def _get_dataset(self):\n",
    "        return self._dataset\n",
    "\n",
    "    def _append_data(self, item, fecha_extraccion, enlace):\n",
    "        self._dataset[\"titulo_marketplace\"].append(\n",
    "            item.get(\"marketplace_listing_title\")\n",
    "        )\n",
    "        self._dataset[\"tiempo_creacion\"].append(item.get(\"creation_time\"))\n",
    "        self._dataset[\"disponible\"].append(item.get(\"is_live\"))\n",
    "        self._dataset[\"vendido\"].append(item.get(\"is_sold\"))\n",
    "        self._dataset[\"cantidad\"].append(item.get(\"listing_inventory_type\"))\n",
    "        self._dataset[\"name_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0].get(\"name\")\n",
    "        )\n",
    "        self._dataset[\"tipo_vendedor\"].append(\n",
    "            item.get(\"story\").get(\"actors\")[0][\"__typename\"]\n",
    "        )\n",
    "        self._dataset[\"id_vendedor\"].append(item.get(\"story\").get(\"actors\")[0][\"id\"])\n",
    "        self._dataset[\"locacion_id\"].append(item.get(\"location_vanity_or_id\"))\n",
    "        self._dataset[\"latitud\"].append(item.get(\"location\", {}).get(\"latitude\"))\n",
    "        self._dataset[\"longitud\"].append(item.get(\"location\", {}).get(\"longitude\"))\n",
    "        self._dataset[\"precio\"].append(item.get(\"listing_price\", {}).get(\"amount\"))\n",
    "        self._dataset[\"tipo_moneda\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"currency\")\n",
    "        )\n",
    "        self._dataset[\"amount_with_concurrency\"].append(\n",
    "            item.get(\"listing_price\", {}).get(\"amount_with_offset_in_currency\")\n",
    "        )\n",
    "        self._dataset[\"tipo_delivery\"].append(item.get(\"delivery_types\", [None])[0])\n",
    "        self._dataset[\"descripcion\"].append(\n",
    "            item.get(\"redacted_description\", {}).get(\"text\")\n",
    "        )\n",
    "        self._dataset[\"fecha_union_vendedor\"].append(\n",
    "            item.get(\"marketplace_listing_seller\", {}).get(\"join_time\")\n",
    "        )\n",
    "        data = item.get(\"location_text\", {})\n",
    "        if data:\n",
    "            data = data.get(\"text\")\n",
    "        self._dataset[\"locacion\"].append(data)\n",
    "        self._dataset[\"Fecha Extraccion\"].append(fecha_extraccion)\n",
    "        self._dataset[\"enlace\"].append(enlace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5abe3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tiempo:\n",
    "    def __init__(self, start):\n",
    "        self._hora_inicio = strftime(\"%H:%M:%S\", localtime(start))\n",
    "        log(INFO, f\"Hora de inicio: {self._hora_inicio}\")\n",
    "        self._fecha = (datetime.now().date() - timedelta(days=1)).strftime(\"%d/%m/%Y\")\n",
    "        self._hora_fin = None\n",
    "        self._cantidad = None\n",
    "        self._tiempo = None\n",
    "        self._productos_por_min = None\n",
    "        self._enlace = None\n",
    "        self._errores = None\n",
    "\n",
    "    def _get_fecha(self):\n",
    "        return self._fecha\n",
    "\n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "\n",
    "    def _set_cantidad(self, cantidad):\n",
    "        self._cantidad = cantidad\n",
    "\n",
    "    def _set_errores(self, errores):\n",
    "        self._errores = errores\n",
    "\n",
    "    def _set_param_final(self, start):\n",
    "        end = time()\n",
    "        self._hora_fin = strftime(\"%H:%M:%S\", localtime(end))\n",
    "        log(INFO, f\"Productos Extraídos: {self._cantidad}\")\n",
    "        log(INFO, f\"Hora Fin: {self._hora_fin}\")\n",
    "        total = end - start\n",
    "        self._tiempo = str(timedelta(seconds=total)).split(\".\")[0]\n",
    "        self._productos_por_min = int(round(self._cantidad / (total / 60), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e788fe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScraperFb:\n",
    "    \"\"\"Representa a un bot para hacer web scarping en fb marketplace.\n",
    "\n",
    "    Attributes:\n",
    "        driver (Object): Maneja un navegador para hacer web scraping\n",
    "        wait (Object): Maneja el Tiempo de espera durante la ejecución del bot\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, start):\n",
    "        \"\"\"Inicializa un objeto de tipo ScraperFb.\n",
    "\n",
    "        Args:\n",
    "            driver (Object): [Driver]\n",
    "            wait (Object): [Wait]\n",
    "        \"\"\"\n",
    "        log(INFO, \"Inicializando scraper\")\n",
    "        self._tiempo = Tiempo(start)\n",
    "        chrome_options = webdriver.ChromeOptions()\n",
    "        prefs = {\"profile.default_content_setting_values.notifications\": 2}\n",
    "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "        self.driver = webdriver.Chrome(\n",
    "            chrome_options=chrome_options,\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "        )\n",
    "        self.wait = WebDriverWait(self.driver, 10)\n",
    "        self._errores = Errores()\n",
    "        self._data = Dataset()\n",
    "\n",
    "    def _get_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def _get_errores(self):\n",
    "        return self._errores\n",
    "\n",
    "    def iniciar_sesion(self, url):\n",
    "        \"\"\"Inicia sesión en una página web usando un usuario y contraseña\n",
    "\n",
    "        Args:\n",
    "            url (str): [Url]\n",
    "        \"\"\"\n",
    "        log(INFO, \"Iniciando sesión\")\n",
    "        self.driver.get(url)\n",
    "        self.driver.maximize_window()\n",
    "        username = self.wait.until(EC.presence_of_element_located((By.ID, \"email\")))\n",
    "        password = self.wait.until(EC.presence_of_element_located((By.ID, \"pass\")))\n",
    "        username.clear()\n",
    "        password.clear()\n",
    "        username.send_keys(getenv(\"FB_USERNAME\"))\n",
    "        password.send_keys(getenv(\"FB_PASSWORD\"))\n",
    "        self.wait.until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[name='login']\"))\n",
    "        ).click()\n",
    "        log(INFO, \"Inicio de sesión con éxito\")\n",
    "\n",
    "    def mapear_datos(self, url):\n",
    "        sleep(10)\n",
    "        log(INFO, \"Accediendo a la URL\")\n",
    "        self.driver.execute_script(\"window.open('about:blank', 'newtab');\")\n",
    "        self.driver.switch_to.window(\"newtab\")\n",
    "        self.driver.get(url)\n",
    "\n",
    "        sleep(8)\n",
    "        log(INFO, \"Mapeando Publicaciones\")\n",
    "        ropa = self.driver.find_elements(\n",
    "            By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "        )\n",
    "        fecha_publicacion = fecha_extraccion = int(\n",
    "            datetime.strptime(self._tiempo._get_fecha(), \"%d/%m/%Y\").timestamp()\n",
    "        )\n",
    "        fecha_flag = fecha_extraccion + 86400\n",
    "        i = 0\n",
    "        e = 0\n",
    "        del self.driver.requests\n",
    "\n",
    "        while fecha_publicacion >= fecha_extraccion:\n",
    "            log(INFO, f\"Scrapeando item {i + 1}\")\n",
    "\n",
    "            try:\n",
    "                try:\n",
    "                    enlace = findall(\n",
    "                        \"(.*)\\/\\?\",\n",
    "                        ropa[i]\n",
    "                        .find_element(By.XPATH, \".//ancestor::a\")\n",
    "                        .get_attribute(\"href\"),\n",
    "                    )[0]\n",
    "                except NoSuchElementException as error:\n",
    "                    enlace = None\n",
    "                    self._errores._append_error(error, enlace)\n",
    "                ropa[i].click()\n",
    "                sleep(5)\n",
    "                for request in self.driver.requests:\n",
    "                    if not request.response or \"graphql\" not in request.url:\n",
    "                        continue\n",
    "\n",
    "                    body = decode(\n",
    "                        request.response.body,\n",
    "                        request.response.headers.get(\"Content-Encoding\", \"identity\"),\n",
    "                    )\n",
    "                    decoded_body = body.decode(\"utf-8\")\n",
    "                    json_data = loads(decoded_body)\n",
    "\n",
    "                    if \"prefetch_uris_v2\" not in json_data[\"extensions\"]:\n",
    "                        continue\n",
    "\n",
    "                    fecha_publicacion = json_data[\"data\"][\"viewer\"][\n",
    "                        \"marketplace_product_details_page\"\n",
    "                    ][\"target\"][\"creation_time\"]\n",
    "                    if fecha_publicacion < fecha_flag:\n",
    "                        dato = json_data[\"data\"][\"viewer\"][\n",
    "                            \"marketplace_product_details_page\"\n",
    "                        ][\"target\"]\n",
    "                        log(INFO, f\"{dato['marketplace_listing_title']}\")\n",
    "                        self._data._append_data(dato, self._tiempo._get_fecha(), enlace)\n",
    "                        log(INFO, f\"Item {i + 1} scrapeado con éxito\")\n",
    "                    break\n",
    "                self.driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except (\n",
    "                NoSuchElementException,\n",
    "                ElementNotInteractableException,\n",
    "                StaleElementReferenceException,\n",
    "            ) as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e = e + 1\n",
    "\n",
    "            except (KeyError, JSONDecodeError) as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e = e + 1\n",
    "                self.driver.execute_script(\"window.history.go(-1)\")\n",
    "\n",
    "            except Exception as error:\n",
    "                self._errores._append_error(error, enlace)\n",
    "                e = e + 1\n",
    "                log(CRITICAL, \"Se detuvo inesperadamente el programa\")\n",
    "                log(CRITICAL, f\"Causa:\\n{error}\")\n",
    "                break\n",
    "            i = i + 1\n",
    "            if i == len(ropa):\n",
    "                self.driver.execute_script(\n",
    "                    \"window.scrollTo(0, document.body.scrollHeight)\"\n",
    "                )\n",
    "                sleep(7)\n",
    "                ropa = self.driver.find_elements(\n",
    "                    By.XPATH, '//*[@class=\"xt7dq6l xl1xv1r x6ikm8r x10wlt62 xh8yej3\"]'\n",
    "                )\n",
    "\n",
    "            del self.driver.requests\n",
    "            log(\n",
    "                INFO,\n",
    "                \"-------------------------------------------------------------------\",\n",
    "            )\n",
    "            sleep(3)\n",
    "        self._tiempo._set_errores(e)\n",
    "        log(INFO, f\"Se halló {e} errores\")\n",
    "        log(INFO, \"Fin de la extraccion\")\n",
    "\n",
    "    def guardar_datos(\n",
    "        self, dataset, filetype=\"Data\", folder=\"Data\", filename=\"fb_data\"\n",
    "    ):\n",
    "        log(INFO, f\"Guardando {filetype}\")\n",
    "        df_fb_mkp_ropa = DataFrame(dataset)\n",
    "        if filetype == \"Data\":\n",
    "            df_fb_mkp_ropa.drop(len(df_fb_mkp_ropa) - 1, axis=0, inplace=True)\n",
    "            cantidad = len(df_fb_mkp_ropa)\n",
    "            self._tiempo._set_cantidad(cantidad)\n",
    "        elif filetype == \"Error\":\n",
    "            cantidad = self._tiempo._get_errores()\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        datetime_obj = datetime.strptime(self._tiempo._get_fecha(), \"%d/%m/%Y\")\n",
    "        filepath = folder + \"/\" + datetime_obj.strftime(\"%d-%m-%Y\") + \"/\"\n",
    "        filename = (\n",
    "            filename\n",
    "            + \"_\"\n",
    "            + datetime_obj.strftime(\"%d%m%Y\")\n",
    "            + \"_\"\n",
    "            + str(cantidad)\n",
    "            + \".xlsx\"\n",
    "        )\n",
    "        if not path.exists(filepath):\n",
    "            makedirs(filepath)\n",
    "        df_fb_mkp_ropa.to_excel(filepath + filename, index=False)\n",
    "        log(INFO, f\"{filetype} Guardados Correctamente\")\n",
    "\n",
    "    def guardar_tiempos(self, filename, sheet_name, start):\n",
    "        log(INFO, \"Guardando tiempos\")\n",
    "        self._tiempo._set_param_final(start)\n",
    "        header_exist = True\n",
    "        if path.isfile(filename):\n",
    "            tiempos = load_workbook(filename)\n",
    "            if sheet_name not in [ws.title for ws in tiempos.worksheets]:\n",
    "                tiempos.create_sheet(sheet_name)\n",
    "                header_exist = False\n",
    "        else:\n",
    "            tiempos = Workbook()\n",
    "            tiempos.create_sheet(sheet_name)\n",
    "            header_exist = False\n",
    "        worksheet = tiempos[sheet_name]\n",
    "        if not header_exist:\n",
    "            worksheet.append(list(self._tiempo.__dict__.keys()))\n",
    "        worksheet.append(list(self._tiempo.__dict__.values()))\n",
    "        tiempos.save(filename)\n",
    "        tiempos.close()\n",
    "        log(INFO, \"Tiempos Guardados Correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767432d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_log():\n",
    "    seleniumLogger.setLevel(ERROR)\n",
    "    urllibLogger.setLevel(ERROR)\n",
    "    urllibLogger.propagate = False\n",
    "    logger = getLogger(\"seleniumwire\")\n",
    "    logger.setLevel(ERROR)\n",
    "    basicConfig(format=\"%(asctime)s %(message)s\", level=INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6838bca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Formato para el debugger\n",
    "    log(INFO, \"Configurando Formato Básico del Debugger\")\n",
    "    config_log()\n",
    "\n",
    "    # Cargar variables de entorno\n",
    "    log(INFO, \"Cargando Variables de entorno\")\n",
    "    load_dotenv()\n",
    "\n",
    "    start = time()\n",
    "\n",
    "    # Url base a scrapear\n",
    "    url_base = getenv(\"URL_BASE\")\n",
    "    url_ropa = getenv(\"URL_CATEGORY\")\n",
    "\n",
    "    # Parámetros para guardar la data extraída por el scraper\n",
    "    data_filename = getenv(\"DATA_FILENAME\")\n",
    "    data_folder = getenv(\"DATA_FOLDER\")\n",
    "\n",
    "    # Parámetros para guardar la medición de la ejecución del scraper\n",
    "    filename_tiempos = getenv(\"FILENAME_TIEMPOS\")\n",
    "    sheet_tiempos = getenv(\"SHEET_TIEMPOS\")\n",
    "\n",
    "    # Parámetros para guardar los errores durante la ejecución por el scraper\n",
    "    error_filename = getenv(\"ERROR_FILENAME\")\n",
    "    error_folder = getenv(\"ERROR_FOLDER\")\n",
    "\n",
    "    scraper = ScraperFb(start)\n",
    "    scraper.iniciar_sesion(url_base)\n",
    "    scraper.mapear_datos(url_ropa)\n",
    "\n",
    "    # Guardando la data extraída por el scraper\n",
    "    scraper.guardar_datos(\n",
    "        scraper._get_data()._get_dataset(), \"Data\", data_folder, data_filename\n",
    "    )\n",
    "\n",
    "    # Guardando los errores extraídos por el scraper\n",
    "    scraper.guardar_datos(\n",
    "        scraper._get_errores()._get_errores(), \"Error\", error_folder, error_filename\n",
    "    )\n",
    "\n",
    "    # Guardando los tiempos durante la ejecución del scraper\n",
    "    scraper.guardar_tiempos(filename_tiempos, sheet_tiempos, start)\n",
    "\n",
    "    log(INFO, \"Programa ejecutado satisfactoriamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a430409",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
